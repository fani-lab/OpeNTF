## Transformer Configuration

## Inspired by the Google setup
## from https://opennmt.net/OpenNMT-py/FAQ.html#how-do-i-train-the-transformer-model

## Where the samples will be written
save_data: ../output/nmt/run/transformer

## Where the vocab(s) will be written
src_vocab: ../output/nmt/run/transformer.vocab.src
tgt_vocab: ../output/nmt/run/transformer.vocab.tgt

# Prevent overwriting existing files in the folder
overwrite: True

# Corpus opts:
data:
    corpus_1:
        path_src: ../output/nmt/src-train.txt
        path_tgt: ../output/nmt/tgt-train.txt
        weight: 1
    valid:
        path_src: ../output/nmt/src-val.txt
        path_tgt: ../output/nmt/tgt-val.txt

# Vocabulary files that were just created
src_vocab: ../output/nmt/run/transformer.vocab.src
tgt_vocab: ../output/nmt/run/transformer.vocab.tgt


# --------------------------------------------------------------------------


# General opts
save_model: ../output/nmt/run/transformer_model
save_checkpoint_steps: 10000
valid_steps: 10000
train_steps: 200000


# Batching
bucket_size: 262144
world_size: 1
gpu_ranks: [0]
num_workers: 4
batch_type: "tokens"
batch_size: 32 # previously 4096
valid_batch_size: 2048
accum_count: [4]
accum_steps: [0]


# Optimization
model_dtype: "fp16"
optim: adam
learning_rate: 2
warmup_steps: 8000
decay_method: "noam"
adam_beta2: 0.998
max_grad_norm: 0
label_smoothing: 0.1
param_init: 0
param_init_glorot: true
normalization: "tokens"


# Model hyperparameters
encoder_type: transformer
decoder_type: transformer
position_encoding: true
enc_layers: 6
dec_layers: 6
heads: 8
hidden_size: 512
word_vec_size: 512
transformer_ff: 2048
dropout_steps: [0]
dropout: [0.1]
attention_dropout: [0.1]


# I forgot to comment these out for model1 runs but commented out for model2 runs
layers: 6  # Transformers often use 6 or more layers effectively
learning_rate_decay: 0.95  # Slower decay
start_decay_steps: 100  # Delay decay start
decay_steps: 100  # Decay less frequently
batch_size: 16  # Increased batch size for better gradient estimates
