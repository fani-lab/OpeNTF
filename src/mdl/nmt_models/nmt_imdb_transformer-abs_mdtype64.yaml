## Transformer with Attention Configuration for imdb

save_data: ../output/nmt/run/transformer
save_model: ../output/nmt/run/transformer_model
src_vocab: ../output/nmt/run/transformer.vocab.src
tgt_vocab: ../output/nmt/run/transformer.vocab.tgt
overwrite: True
data:
  corpus_1:
    path_src: ../output/nmt/src-train.txt
    path_tgt: ../output/nmt/tgt-train.txt
    weight: 1
  valid:
    path_src: ../output/nmt/src-val.txt
    path_tgt: ../output/nmt/tgt-val.txt

# -------------------------------------------
# Configuration for training
# -------------------------------------------

# Common hyperparameters
# General opts
keep_checkpoint: -1
seed: 0
train_steps: 400 # sample size of 32059
save_checkpoint_steps: 40
valid_steps: 40
warmup_steps: 40
report_every: 40

# Batching
world_size: 2
gpu_ranks: [0, 1]
num_workers: 8
# batch_type: "tokens"
batch_size: 80
bucket_size: 128
valid_batch_size: 256
accum_count: [1]
accum_steps: [0]

# Optimization
model_dtype: "fp64"
optim: adam
weight_decay: 0.0001
learning_rate: 0.0013
# decay_method: "noam"
# adam_beta1: 0.9
adam_beta2: 0.98
learning_rate_decay: 0.90
start_decay_steps: 50

max_grad_norm: 5
label_smoothing: 0.05
param_init: 0
param_init_glorot: true
# normalization: "tokens"

beam_size: 5
length_penalty: 1.0

# --------------------------------------------

# Architecture specific hyperparameters
encoder_type: transformer
decoder_type: transformer
position_encoding: true
hidden_size: 256
word_vec_size: 256
transformer_ff: 512

dropout: [0.1]
attention_dropout: [0.1]

max_relative_positions: 16
enc_layers: 4
dec_layers: 4
heads: 4
