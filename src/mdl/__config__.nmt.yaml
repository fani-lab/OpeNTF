data:
  corpus_1:
    path_src: ${path_src} #../output/{domain}/{dataset}/nmt.{settings}/f{foldidx}/src-train.txt
    path_tgt: ${path_tgt} #../output/{domain}/{dataset}/nmt.{settings}/f{foldidx}/tgt-train.txt
    transforms: []
  valid:
    path_src: ${path_src} #../output/{domain}/{dataset}/nmt.{settings}/f{foldidx}/src-val.txt
    path_tgt: ${path_tgt} #../output/{domain}/{dataset}/nmt.{settings}/f{foldidx}/tgt-val.txt
src_vocab: ${src_vocab} #../output/{domain}/{dataset}/nmt.{settings}/f{foldidx}/vocab.src
tgt_vocab: ${tgt_vocab} #../output/{domain}/{dataset}/nmt.{settings}/f{foldidx}/vocab.tgt
n_sample: -1
overwrite: True #True raise warning, False >> raise exception
vocab_sample_limit: -1
#src_embeddings: ../output/nmt/run/example.vocab.src.emb
#tgt_embeddings: ../output/nmt/run/example.vocab.tgt.emb

save_data: ${save_data} #../output/{domain}/{dataset}/nmt.{settings}/f{foldidx}/
save_model: ${save_model} #../output/{domain}/{dataset}/nmt.{settings}/f{foldidx}/model_step_*.pt
seed: ${seed}

early_stopping: ${es}
early_stopping_criteria: accuracy
dropout: 0.2
word_vec_size: 128 # same as embedding dimension _{d}

# Batching
world_size: ${acceleration} # 1 train on cpu or a single GPU or 2 on two gpus [x, y]
gpu_ranks: ${acceleration} # for cpu -> [], for gpus -> the indexes [0, 1]
num_workers: 0 # keep 0 as for >0 UnboundLocalError: local variable 'item' referenced before assignment at ./onmt/inputters/dynamic_iterator.py", line 91, in __iter__
batch_size: ${batch_size}
batch_type: 'sents' #fixed number of sentence (team: (skill,member)) pairs per batch ('tokens' is not a good choice for here)
bucket_size: ${train_size} # first buckets are to be filled, even if multiple reads if train is small, batches are coming from the buckets. So, it's better to make it equal to train_size
#valid_batch_size: defaults to ${batch_size}

# # Where to save the checkpoints and common hyperparameters
keep_checkpoint: -1
train_steps: ${e} # default behaviour based on roughly train_steps = (total_training_examples) / batch_size) * num_epochs
save_checkpoint_steps: ${spe} #but here is per batch
valid_steps: 1
report_every: 1

# Optimization
model_dtype: "fp16"
optim: adam
weight_decay: 0.0001
learning_rate: ${lr} #0.001
adam_beta1: 0.9
adam_beta2: 0.98

#start_decay_steps: 50
#decay_steps: 50

# copy_attn: 'true'
# global_attention: mlp
# reuse_copy_attn: 'true'
# bridge: 'true'

encoder_type: transformer #cnn,rnn,transformer
decoder_type: transformer #could be same or different from encoder_type

## ConvS2S with Attention Configuration Template
cnn_size: 128 # same as hidden size
cnn_kernel_width: 3
layers: 4

## RNN with Attention Configuration Template
rnn_type: LSTM
rnn_size: 128
input_feed: 1
enc_layers: 4
dec_layers: 4

## Transformer with Attention Configuration Template
warmup_steps: 501 # transformer
position_encoding: False # for now, we don't want purticular order in input skills and output members. But we can experiment on True
hidden_size: 128
transformer_ff: 512
attention_dropout: 0.2
#enc_layers: 8
#dec_layers: 8
heads: 8



# Inference-only params here (harmless during training)
beam_size: 10
n_best: 1
min_length: 2
max_length: 100