#defaults:
#  - mdl/emb/config

hydra:
  output_subdir: null
  run:
    dir: . #unix or macos
    #dir: /dev/null # windows
seed: 0
acceleration: 'cpu' #cpu for all cores minus one, cpu:3 for 3 cores, 'cuda' or 'cuda:0' for the only available gpu, 'cuda:3' for gpu3 >> TODO: multiple gpus
cmd:
  #- prep
  #- train
  #- test
  #- eval
  #- plot
  #- agg
  #- fair
data: #required for 'prep' step
  hf: True
  domain:
    cmn.publication.Publication
    #cmn.movie.Movie
    #cmn.repository.Repository
    #cmn.patent.Patent
  source:
    ../data/dblp/toy.dblp.v12.json
    #../data/imdb/toy.title.basics.tsv
    #../data/gith/toy.repos.csv
    #../data/uspt/toy.patent.tsv
  #these are folders or will be created as folders
  output:
    ../output/dblp/toy.dblp.v12.json
    #../output/imdb/toy.title.basics.tsv
    #../output/gith/toy.repos.csv
    #../output/uspt/toy.patent.tsv

  filter:
    min_nteam: 10 # as this may yield empty teams, min_team_size should be set to at least 1
    min_team_size: 2 #no filter -> no key 'filter' or ~data.filter in the command line
  location: 'venue' #should be one of 'city', 'state', 'country', represents the *location of team members*, or 'venue' represents the location of team members as replica of the location of team like in dblp
  bucket_size: 1000
  acceleration: 'cpu' # ${acceleration}
  embedding:
    class_method: #directs to the correct part of the embedding config file. If not set, skipped.
      #mdl.emb.d2v.D2v_d2v   #Doc2Vec https://radimrehurek.com/gensim/auto_examples/tutorials/run_doc2vec_lee.html
      #mdl.emb.gnn.Gnn_n2v   #Node2Vec https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.Node2Vec.html
      #mdl.emb.gnn.Gnn_m2v   #Metapath2Vec https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.MetaPath2Vec.html
      #mdl.emb.gnn.Gnn_gcn   #GraphConvolutionNetwork https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GCNConv.html
      #mdl.emb.gnn.Gnn_gs    #GraphSAGE https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.SAGEConv.html
      #mdl.emb.gnn.Gnn_gat   #GraphAttentionNetwork https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GATConv.html
      #mdl.emb.gnn.Gnn_gatv2 #GraphAttentionNetwork https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GATv2Conv.html
      #mdl.emb.gnn.Gnn_gin   #GraphIsomorphicNetwork
    config: ./mdl/emb/__config__.yaml

models:
  instances:
    - mdl.rnd.Rnd
    - mdl.fnn.Fnn #input skills can be multihot (default), embeddings by d2v, n2v, m2v, ... see above data.embedding.class_method
    - mdl.bnn.Bnn #thanks to bayesian-torch package at https://github.com/IntelLabs/bayesian-torch
    - mdl.nmt.Nmt
    #- mdl.rrn.Rrn     #external baseline >> isn't it temporal?
    ## NOTE: temporal baselines should not be mixed with non-temporal ones as the training/test splits are temporal based on train.step_ahead
    #- mdl.tnmt.tNmt
    #- mdl.tntf.tNtf_mdl.rnd.Rnd   #underlying base model could be fnn and bnn. future: tgnn?. train.step_ahead must be set.
    #- mdl.caser.Caser #external baseline

  config: ./mdl/__config__.yaml

train:
  nfolds: 3 #ends with {nfolds} models each with nfolds-1 train splits and one fold valid split, nfolds results, averaged on a single test split
  train_test_ratio: 0.85 #the nfolds will be on the trainig part.
  merge_teams_w_same_skills: False # for explanation >> https://github.com/fani-lab/OpeNTF/issues/156
  step_ahead: #NOTE: setting this tag trigger temporal train/test splits for tNtf: streaming/temporal training. for now, it means that whatever are in the last [step_ahead] time interval will be the test set!
  save_per_epoch: 10 # if 0, no per epoch save, if nonzero x, every x epoch. Also, due to early stopping the number of files could be less than the max training epoch

test:
  on_train: False # for over vs. under-fitting check. # random baseline (mdl.rnd.Rnd()) cannot join this.
  per_epoch: ${train.save_per_epoch} # the test results of each epoch on the test set >> for per-epoch performance. Similar to valid losses during training but here we can have eval metrics
  topK: 1000 #as the *.pred files becomes huge for test set (|test| by |experts|), or when on_tain=True, we can only keep the topk probs per test instance, zero the rest, and save sparse matrix

eval:
  topK: ${test.topK}
  topk: '2,5,10'
  metrics:
    trec: [P_topk, recall_topk, ndcg_cut_topk, map_cut_topk, success_topk] # see trec_eval for a complete list or trec_* metrics https://github.com/terrierteam/pytrec_eval/blob/master/tests/pytrec_eval_tests.py
    other: [skill_coverage_topk, aucroc] #aucroc+ includes the tpr-fpr curve points and may not be a good one as the number of classes are number of experts; very large
  on_train: ${test.on_train}
  per_epoch: ${test.per_epoch}
  per_instance: True # needed for paired significance tests

fair: #post-hoc reranks of the prediction list
  np_ratio: #desired ratio of non-popular experts after reranking; if None, based on distribution in dataset; default: None; Eg. 0.5
  fairness: det_greedy #reranking algorithm from {det_greedy, det_cons, det_relaxed}; required; Eg. det_cons
  k_max: #cutoff for the reranking algorithms; default: None
  attribute: #the set of our sensitive attributes')
