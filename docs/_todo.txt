#TODO: leaderboard src/web #192 integration

./ipynb >> extension.ipynb

#TODO: +han, +lant(not pyg friendly)

#TODO: nmt does not work with our embeddings of any kind. Can we inject our embedding for it?

#TODO: - bundle recommenders

#TODO: - cl
    {emb}_{fnn,bnn}_cl_{train/test} >> future


#TODO: temporal/sequence
    tNtf_{emb}_{fnn,bnn}_{a1}_{train,test}
    a1: adding one 1 to the input (time as aspect/vector for time)
    dt2v_emb: adding the year to the doc2vec training (temporal dense skill vecs in input)

    # output_ = f'{output}/{year_idx[-self.step_ahead - 1][1]}'  # this folder will be created by the last model training

    #if 'test' in cmd:# todo: the prediction of each step ahead should be seperate
    #   # for i, v in enumerate(year_idx[-self.step_ahead:]):  # the last years are for test.
    #   #     tsplits['test'] = np.arange(year_idx[i][0], year_idx[i + 1][0] if i < len(year_idx) else len(indexes['i2t']))
    #   self.model.test(output_, splits, indexes, vecs, settings, on_train_valid_set, per_epoch)
    #
    #         # todo: the evaluation of each step ahead should be seperate
    #   if 'eval' in cmd: self.model.evaluate(output_, splits, vecs, on_train_valid_set, per_instance, per_epoch)
    #   if 'plot' in cmd: self.model.plot_roc(output_, splits, on_train_valid_set)
            # if m_name.endswith('a1'): vecs_['skill'] = lil_matrix(scipy.sparse.hstack((vecs_['skill'], lil_matrix(np.ones((vecs_['skill'].shape[0], 1))))))

    user-item recommender baselines:
        # # temporal recommender systems
        # if 'caser' in model_list: models['caser'] = Caser(settings['model']['step_ahead'])
        - caser
        - sasrec
        - bert4rec


-- end2end
- mlp-based predictor after gnn layers
- hetero version of gnns using HeteroConv that wraps gnn methods per node type
- inductive

============
d2v
-- strict splits of train/valid/test. I think even if it sees everything, does not produce good embeddings
why not d2v when we have team, skill, and member vectors?
-- skill-member as docs during the train, then skills as docs during test to find the closest experts vectors as members :DD
-- may not worth it as the seq2seq is more powerful (document completion using pre-words of skills with members)

for earlystopping of d2v or strict splits
def evaluate_model(model, val_corpus):
    sims = []
    for doc in val_corpus:
        inferred_vec = model.infer_vector(doc.words)
        most_similar = model.dv.most_similar([inferred_vec], topn=1)
        top_score = most_similar[0][1]
        sims.append(top_score)
    return sum(sims) / len(sims) >> a doc, when in test, should return the same doc if in train, or the most similar one (textoverlap?)

=========

update/check cmn.team.Team.get_stats()
update/check cmn.patent.Patent.get_stats()
=======================================================

-- contextual embeddings (transfer or preinit e2e)>> llm-based >> sentencetranformer
--- we can train it from scratch using our own customize skill,member vocabs
--- uses gpu
--- we can also pretrain on skill and members real names (to capture natural meaning of words and people names!)

new change to the skill subset >> skill list for each team, ordered skills
- dblp based on fos score
- gith based on line numbers of the langs
- imdb based on the genre, subgenre, ... the actual order in the dataset

lazy load update -- when doing the filtering, we read from scratch. We can load the teams.pkl and filter, and update it. >> messy and confusing, not worth it
- embeddings for experts. in future may want to do skill-experts in input ...)
--------------------------------------------------------