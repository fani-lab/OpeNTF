#TODO: update to readme for cmn layer and main readme
./readme.md >> new models class hierarchy and dataset hierarchy

./data/{dataset}/readme.md
    github >> explain how it is collected and where to find/dl it

./output/readme.md
./ipynb >> extension.ipynb

#TODO: +han, +lant(not pyg friendly)

#TODO: nmt does not work with our embeddings of any kind. Can we inject our embedding for it?
- bundle recommenders

temporal/sequence
    tNtf_{emb}_{fnn,bnn}_{train,test}
    user-item recommender baselines:
        - rrn?
        - caser
        - sasrec
        - bert4rec

{emb}_{fnn,bnn}_cl_{train/test} >> future

-- end2end
- mlp-based predictor after gnn layers
- hetero version of gnns using HeteroConv that wraps gnn methods per node type
- inductive

============
d2v
-- strict splits of train/valid/test. I think even if it sees everything, does not produce good embeddings
why not d2v when we have team, skill, and member vectors?
-- skill-member as docs during the train, then skills as docs during test to find the closest experts vectors as members :DD
-- may not worth it as the seq2seq is more powerful (document completion using pre-words of skills with members)

for earlystopping of d2v or strict splits
def evaluate_model(model, val_corpus):
    sims = []
    for doc in val_corpus:
        inferred_vec = model.infer_vector(doc.words)
        most_similar = model.dv.most_similar([inferred_vec], topn=1)
        top_score = most_similar[0][1]
        sims.append(top_score)
    return sum(sims) / len(sims) >> a doc, when in test, should return the same doc if in train, or the most similar one (textoverlap?)

=========

update/check cmn.team.Team.get_stats()
update/check cmn.patent.Patent.get_stats()
=======================================================

-- contextual embeddings (transfer or preinit e2e)>> llm-based >> sentencetranformer
--- we can train it from scratch using our own customize skill,member vocabs
--- uses gpu
--- we can also pretrain on skill and members real names (to capture natural meaning of words and people names!)

new change to the skill subset >> skill list for each team, ordered skills
- dblp based on fos score
- gith based on line numbers of the langs
- imdb based on the genre, subgenre, ... the actual order in the dataset

lazy load update -- when doing the filtering, we read from scratch. We can load the teams.pkl and filter, and update it. >> messy and confusing, not worth it
- embeddings for experts. in future may want to do skill-experts in input ...)
--------------------------------------------------------