Let's start the transfer part for now. I'll complete the tasks when I come to e2e

- earlystoppings
- strict splits, transductive, inductive
- hetero version of gnns

Notes:

Our graph are mainly hetero, but gnn methods are mainly designed for homo graph. So, we create hetero, make it to_homo(), ...
For strict hetero, we will use HeteroConv that wraps gnn methods per node type
Indeed, it is worth study a mixture of gnn methods for node types :D

to do:

update to readme for cmn layer and main readme 


d2v
-- strict splits of train/valid/test. I think even if it sees everything, does not produce good embeddings
-- skill-member as docs during the train, then skills as docs during test to find the closest experts vectors as members :DD
-- may not worth it as the seq2seq is more powerful (document completion using pre-words of skills with members)

for earlystopping of d2v or strict splits
def evaluate_model(model, val_corpus):
    sims = []
    for doc in val_corpus:
        inferred_vec = model.infer_vector(doc.words)
        most_similar = model.dv.most_similar([inferred_vec], topn=1)
        top_score = most_similar[0][1]
        sims.append(top_score)
    return sum(sims) / len(sims) >> a doc, when in test, should return the same doc if in train, or the most similar one (textoverlap?)


-- gnn-transferred >> for now, entire graph is seen. But
Strict split way: create a training graph using training teams. The test teams should then added to infer the node embeddings.
luckily the nodes are already seen (skills, experts) (team nodes are always unseen), but if not seen, either remove the node or infer based on surroundings.


node2vec, metapath2vec
-- early stopping
-- transductive vs. inductive >> train, valid, test
-- yet to be run on entire datasets >> also check to engage cuda

homo versions: gcn, gs, gat, gatv2, gin
-- early stopping
-- transductive vs. inductive >> train, valid, test
-- yet to be run on entire datasets >> also check to engage cuda

=========
hetero versions of gcn, gs, gat, gatv2, gin
han >> pure hetero like m2v

lant >> not pyg friendly
gine >> edge-based. not relevant for now as we don't have edge attributes

-- end2end
make a connection between train/vali/test splits of teams and train/valid/test edge lists

node2vec >> we don't have it! but we can, right?
metapath2vec >> we didnot have it! but we can, right?
why not d2v when we have team, skill, and member vectors?

update/check cmn.team.Team.get_stats()
update/check cmn.patent.Patent.get_stats()
make cmn.team.Team.get_stats() gpu-friendly

=======================================================

-- contextual embeddings (transfer or preinit e2e)>> llm-based >> sentencetranformer
--- we can train it from scratch using our own customize skill,member vocabs
--- uses gpu
--- we can also pretrain on skill and members real names (to capture natural meaning of words and people names!)

new change to the skill subset >> skill list for each team, ordered skills
- dblp based on fos score
- gith based on line numbers of the langs
- imdb based on the genre, subgenre, ... the actual order in the dataset

lazy load update -- when doing the filtering, we read from scratch. We can load the teams.pkl and filter, and update it. >> messy and confusing, not worth it
- embeddings for experts. in future may want to do skill-experts in input ...)
--------------------------------------------------------