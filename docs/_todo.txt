#TODO: the e2e needs data.embedding.class_method but should not re-train >> When gtf runs for test, no t2v.learn()

multilayer neural classifiers

    - rrn >> isn't it temporal?
    - bundle recommenders

    temporal/sequence
        tNtf_{emb}_{fnn,bnn}_{train,test}
        user-item recommender baselines:
            - rrn?
            - caser
            - sasrec
            - bert4rec

    {emb}_{fnn,bnn}_cl_{train/test} >> future

-- end2end
- inductive
- hetero version of gnns
Our graph are mainly hetero, but gnn methods are mainly designed for homo graph. So, we create hetero, make it to_homo(), ...
For strict hetero, we will use HeteroConv that wraps gnn methods per node type
Indeed, it is worth study a mixture of gnn methods for node types :D

update to readme for cmn layer and main readme

d2v
-- strict splits of train/valid/test. I think even if it sees everything, does not produce good embeddings
-- skill-member as docs during the train, then skills as docs during test to find the closest experts vectors as members :DD
-- may not worth it as the seq2seq is more powerful (document completion using pre-words of skills with members)

for earlystopping of d2v or strict splits
def evaluate_model(model, val_corpus):
    sims = []
    for doc in val_corpus:
        inferred_vec = model.infer_vector(doc.words)
        most_similar = model.dv.most_similar([inferred_vec], topn=1)
        top_score = most_similar[0][1]
        sims.append(top_score)
    return sum(sims) / len(sims) >> a doc, when in test, should return the same doc if in train, or the most similar one (textoverlap?)

=========
hetero versions of gcn, gs, gat, gatv2, gin
han >> pure hetero like m2v

lant >> not pyg friendly
gine >> edge-based. not relevant for now as we don't have edge attributes

node2vec >> we don't have it! but we can, right?
metapath2vec >> we didnot have it! but we can, right?
why not d2v when we have team, skill, and member vectors?

update/check cmn.team.Team.get_stats()
update/check cmn.patent.Patent.get_stats()
make cmn.team.Team.get_stats() gpu-friendly

=======================================================

-- contextual embeddings (transfer or preinit e2e)>> llm-based >> sentencetranformer
--- we can train it from scratch using our own customize skill,member vocabs
--- uses gpu
--- we can also pretrain on skill and members real names (to capture natural meaning of words and people names!)

new change to the skill subset >> skill list for each team, ordered skills
- dblp based on fos score
- gith based on line numbers of the langs
- imdb based on the genre, subgenre, ... the actual order in the dataset

lazy load update -- when doing the filtering, we read from scratch. We can load the teams.pkl and filter, and update it. >> messy and confusing, not worth it
- embeddings for experts. in future may want to do skill-experts in input ...)
--------------------------------------------------------