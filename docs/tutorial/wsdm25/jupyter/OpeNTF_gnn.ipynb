{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `OpeNTF` with `GNN`:\n",
    "`OpeNTF` previously used traditional embedding methods (non-graph based) to provide skill embeddings as an input alternative to the one-hot encoded skills. With the advent of `GNN` methods, we now have graph-based **learned** skill embeddings usable as a form of more meaningful input. The gnns are now able to capture the synergistic collaborative ties within our transformed graph data to provide us with significant embeddings, resulting in even better recommendation of experts  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Graph Neural Networks**\n",
    "\n",
    "The basic breakthrough for graph neural networks is the message passing mechanism. Here we can see how each node representation gets updated to a newer one `(k-th)` with the aggregation and combination of the previous `((k - 1)-th)` neighboring node representations. The learned representations after gnn training result as a collection of node embeddings, usable in downstream tasks.\n",
    "\n",
    "<p align=\"center\"><img src='gnn_embedding-min.png' width=\"500\" ></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Graph Structures**\n",
    "\n",
    "In OpeNTF applied with gnn, we aimed to cover as many variations possible in graph structures, while maintaining the boundaries of the given datasets. Currently we implement 3 different graph structures, adding to previous state-of-the-art works. We have `skill-expert` (s-e), `skill-team-expert` (s-t-e) and `skill-team-expert-location` (s-t-e-l) as various graph structures. Each of these variations represent the node types underlying their structures. \n",
    "\n",
    "<p align=\"center\"><img src='graph_structures.png' width=\"500\" ></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### **Neighborhood Sampling**\n",
    "\n",
    "The graph formed from our existing large scale dataset do not entirely get accomodated into a single GNN model. Therefore, we employ `mini-batching` strategy to extract smaller subgraphs as batches forming the entire graph to the model to train it for a link prediction task. The subgraphs are sampled based on surrounding neighborhood of every starting edge selected. The neighbors are selected from the nodes occupied by the edges that are selected in each hop. While sampling the neighborhood from a `seed edge`, we describe `k-hop` as the `k step` distanced node from a particular source node.  \n",
    "\n",
    "In the link prediction task, the number of elements in one batch is calculated based on the number of sampled `links` or `edges`. Hence, a batch size of 128 would regard as 128 edges collected from a defined `k-hop` neighborhood. \n",
    "\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <img src=\"Subgraph_1-min.png\" width=\"500\">\n",
    "</div>\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <img src=\"Subgraph_2-min.png\" width=\"500\">\n",
    "    <img src=\"Subgraph_3-min.png\" width=\"500\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## **Transfer Learning with GNN**\n",
    "\n",
    "Although we can successfully predict efficient teams with neural models trained on sparse matrices, our experiments found that using transfer learning with skill embeddings is more effective for team recommendation. The GNN approach involves learning vector representations through message-passing techniques.\n",
    "\n",
    "<p align=\"center\"><img src='gnn_pipeline.jpg' width=\"1000\" ></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "<font color=\"green\">This notebook should be run in the root folder of the `gnn` branch of the OpeNTF project. It is already included in the desired location. The `main` branch is yet to be merged with the gnn features, hence the special instructions. </font>\n",
    "\n",
    "We need to have ``Python >= 3.10`` and install the required packages listed in [``requirements.txt``](requirements.txt):\n",
    "\n",
    "Before installing any required packages, we can install `jupyter` and `ipykernel`\n",
    "in order to be able to start the jupyter notebook inside our required virtual environment (created using `virtualenv` or `venv`)\n",
    "We need to perform the below steps first, in order to integrate our virtualenv `kernel` with the jupyter instance\n",
    "(We need to activate the virtualenv first)\n",
    "```\n",
    "pip install jupyter\n",
    "pip install ipython\n",
    "pip install ipykernel\n",
    "\n",
    "# For example : name_of_the_env = \"opentf\"\n",
    "ipython kernel install --user --name=<name_of_the_env>\n",
    "\n",
    "# Start jupyter notebook for test run\n",
    "jupyter notebook\n",
    "\n",
    "# After starting the jupyter notebook, we can select our desired opentf kernel from any notebook\n",
    "```\n",
    "\n",
    "Using git, clone the codebase and using ``pip`` install the required packages:\n",
    "```\n",
    "git clone --recursive https://github.com/Fani-Lab/opentf\n",
    "git checkout gnn\n",
    "cd opentf\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "For installation of specific version of a python package due to, e.g., ``CUDA`` versions compatibility, one can edit [``requirements.txt``](requirements.txt) to include the appropriate `CUDA` versions (example : replace all instances of `torch-2.5.0+cpu` with `torch-2.5.0+cu124` for `CUDA 12.4`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quickstart on `OpeNTF` with `GNN`\n",
    "\n",
    "The entire codebase has two distinct pipelines:\n",
    "\n",
    "1. ``./src/mdl/team2vec/main.py`` handling the embedding generation step in case of dense vector input for the neural team formation\n",
    "2. ``./src/main.py`` handling the main pipeline of the neural team formation\n",
    "\n",
    "The embedding generation pipeline consists of the models``d2v (Doc2Vec), m2v (Metapath2Vec), gs (GraphSAGE), gat (GraphAttention), gatv2 (GraphAttentionV2),\n",
    "han (Heterogeneous Attention Network), gin (Graph Isomorphism Network) and gine (GIN-Edge feature enhanced).``\n",
    "This pipeline accepts the following required arguments:\n",
    "1) ``-teamsvecs``: The path to the teamsvecs.pkl and indexes.pkl files; e.g., ``-teamsvecs ../data/preprocessed/dblp/toy.dblp.v12.json/``\n",
    "2) ``-model``: The embedding model; e.g., ``-model d2v, m2v, gs ...``\n",
    "\n",
    "To generate GNN based embeddings, it is recommended to include additional arguments as follows:  \n",
    "\n",
    "1) ``--agg``: The aggregation method used for the graph data; e.g : ``mean, none, max, min ...``\n",
    "2) ``--d``: Embedding dimension; e.g : ``4, 8, 16, 32 ...``\n",
    "3) ``--e``: Train epochs ; e.g : ``5, 20, 100 ...``\n",
    "\n",
    "The neural network pipeline accepts three required list of values:\n",
    "1) ``-data``: list of path to the raw datafiles, e.g., ``-data ./../data/raw/dblp/dblp.v12.json``, or the main file of a dataset, e.g., ``-data ./../data/raw/imdb/title.basics.tsv``\n",
    "2) ``-domain``: list of domains of the raw data files that could be ``dblp``, ``imdb``, or `uspt`; e.g., ``-domain dblp imdb``.\n",
    "3) ``-model``: list of baseline models that could be ``fnn``, ``bnn``; e.g., ``-model fnn bnn``.\n",
    "\n",
    "If the input type is a dense vector from GNN methods, an additional list of arguments are needed as follows:\n",
    "1) ``--emb_model``: The embedding model; e.g., ``--emb_model gs gat gatv2 han ...``\n",
    "2)  ``--emb_graph_type`` The collaboration graph type used for embedding generation e.g., ``sm or stm``\n",
    "\n",
    "\n",
    "Here is a brief explanation of the models:\n",
    "- ``fnn``, ``bnn``, ``fnn_emb``, ``bnn_emb``: follows the standard machine learning training procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\Documents\\Pycharm_Projects\\OpeNTF\\src\\mdl\\team2vec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\Documents\\Pycharm_Projects\\venvs\\tmp_opentf\\Lib\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd src/mdl/team2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/c/Users/Owner/Documents/Pycharm_Projects/OpeNTF/src/mdl/team2vec\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping for graph type : sm, \n",
      "Loading the data file ./../../../data/preprocessed/dblp/toy.dblp.v12.json//gnn/stm.undir.mean.data.pkl ...\n",
      "mini batch loader for mode train\n",
      "mini batch loader for mode val\n",
      "mini batch loader for mode train\n",
      "mini batch loader for mode val\n",
      "Device: 'cpu'\n",
      "Encoder(\n",
      "  (model): GraphModule(\n",
      "    (conv1): ModuleDict(\n",
      "      (skill__to__team): GATConv((-1, -1), 8, heads=2)\n",
      "      (member__to__team): GATConv((-1, -1), 8, heads=2)\n",
      "      (team__rev_to__skill): GATConv((-1, -1), 8, heads=2)\n",
      "      (team__rev_to__member): GATConv((-1, -1), 8, heads=2)\n",
      "    )\n",
      "    (conv2): ModuleDict(\n",
      "      (skill__to__team): GATConv((-1, -1), 8, heads=2)\n",
      "      (member__to__team): GATConv((-1, -1), 8, heads=2)\n",
      "      (team__rev_to__skill): GATConv((-1, -1), 8, heads=2)\n",
      "      (team__rev_to__member): GATConv((-1, -1), 8, heads=2)\n",
      "    )\n",
      "    (conv3): ModuleDict(\n",
      "      (skill__to__team): GATConv((-1, -1), 8, heads=2)\n",
      "      (member__to__team): GATConv((-1, -1), 8, heads=2)\n",
      "      (team__rev_to__skill): GATConv((-1, -1), 8, heads=2)\n",
      "      (team__rev_to__member): GATConv((-1, -1), 8, heads=2)\n",
      "    )\n",
      "    (conv4): ModuleDict(\n",
      "      (skill__to__team): GATConv((-1, -1), 8, heads=1)\n",
      "      (member__to__team): GATConv((-1, -1), 8, heads=1)\n",
      "      (team__rev_to__skill): GATConv((-1, -1), 8, heads=1)\n",
      "      (team__rev_to__member): GATConv((-1, -1), 8, heads=1)\n",
      "    )\n",
      "  )\n",
      "  (decoder): Decoder()\n",
      ")\n",
      "\n",
      "Device = cpu\n",
      "epoch 001 : batching for train_loader for seed_edge_type : ('skill', 'to', 'team')\n",
      "epoch 001 : batching for train_loader for seed_edge_type : ('member', 'to', 'team')\n",
      "\n",
      "Val loss : 0.772776\n",
      "Val AUC: 0.491716\n",
      "\n",
      "Validation loss decreased (inf --> 0.772776)\n",
      "epoch 002 : batching for train_loader for seed_edge_type : ('skill', 'to', 'team')\n",
      "epoch 002 : batching for train_loader for seed_edge_type : ('member', 'to', 'team')\n",
      "\n",
      "Val loss : 0.645855\n",
      "Val AUC: 0.608876\n",
      "\n",
      "Validation loss decreased (0.772776 --> 0.645855)\n",
      "epoch 003 : batching for train_loader for seed_edge_type : ('skill', 'to', 'team')\n",
      "epoch 003 : batching for train_loader for seed_edge_type : ('member', 'to', 'team')\n",
      "\n",
      "Val loss : 0.610519\n",
      "Val AUC: 0.448521\n",
      "\n",
      "Validation loss decreased (0.645855 --> 0.610519)\n",
      "epoch 004 : batching for train_loader for seed_edge_type : ('skill', 'to', 'team')\n",
      "epoch 004 : batching for train_loader for seed_edge_type : ('member', 'to', 'team')\n",
      "\n",
      "Val loss : 0.548397\n",
      "Val AUC: 0.499408\n",
      "\n",
      "Validation loss decreased (0.610519 --> 0.548397)\n",
      "epoch 005 : batching for train_loader for seed_edge_type : ('skill', 'to', 'team')\n",
      "epoch 005 : batching for train_loader for seed_edge_type : ('member', 'to', 'team')\n",
      "\n",
      "Val loss : 0.516998\n",
      "Val AUC: 0.461538\n",
      "\n",
      "Validation loss decreased (0.548397 --> 0.516998)\n",
      "\n",
      "saving figure as : ./../../../data/preprocessed/dblp/toy.dblp.v12.json//emb//gat.stm.undir.mean.e5.ns5.b128.d8.png\n",
      "\n",
      "\n",
      "saving figure as : ./../../../data/preprocessed/dblp/toy.dblp.v12.json//emb//gat.stm.undir.mean.e5.ns5.b128.d8.val_auc_per_epoch.png\n",
      "\n",
      "\n",
      "it took 0.052883040904998777 mins || 0.000881384015083313 hours to train the model\n",
      "\n",
      "\n",
      "Val loss : 0.500488\n",
      "Val AUC: 0.542012\n",
      "\n",
      "-------------- ending eval --------------\n",
      "\n",
      "saved embedding as : ./../../../data/preprocessed/dblp/toy.dblp.v12.json//emb//gat.stm.undir.mean.e5.ns5.b128.d8.emb.pt ..............\n",
      "\n",
      "Skipping for graph type : stml, \n"
     ]
    }
   ],
   "source": [
    "!python -u main.py -teamsvecs ./../../../data/preprocessed/dblp/toy.dblp.v12.json/ -model gnn.gat --agg mean --e 5 --d 8 --ns 5 --graph_type stm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\Documents\\Pycharm_Projects\\OpeNTF\\src\n",
      "/c/Users/Owner/Documents/Pycharm_Projects/OpeNTF/src\n"
     ]
    }
   ],
   "source": [
    "%cd ../../\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sparse matrices from ./../data/preprocessed/dblp/toy.dblp.v12.json/teamsvecs.pkl ...\n",
      "Loading indexes pickle from ./../data/preprocessed/dblp/toy.dblp.v12.json/indexes.pkl ...\n",
      "It took 0.0 seconds to load from the pickles.\n",
      "It took 0.002470731735229492 seconds to load the sparse matrices.\n",
      "loaded expert-skill co-occurrence matrix es_vecs\n",
      "Running for (dataset, model): (dblp, fnn) ... \n",
      "\n",
      ".............. starting learn .................\n",
      "\n",
      "Fold 0/2, Epoch 0/24, Minibatch 0/0, Phase train, Running Loss train 65.11357116699219, Time 0.03322553634643555, Overall 2.863619804382324 \n",
      "Fold 0/2, Epoch 0/24, Running Loss train 3.830210068646599, Time 0.03322553634643555, Overall 2.863619804382324 \n",
      "Fold 0/2, Epoch 0/24, Minibatch 0/0, Phase valid, Running Loss valid 29.27532386779785, Time 0.0432736873626709, Overall 2.8736679553985596 \n",
      "Fold 0/2, Epoch 0/24, Running Loss valid 3.2528137630886502, Time 0.0432736873626709, Overall 2.8736679553985596 \n",
      "Fold 0/2, Epoch 1/24, Minibatch 0/0, Phase train, Running Loss train 60.65380859375, Time 0.05932354927062988, Overall 2.8897178173065186 \n",
      "Fold 0/2, Epoch 1/24, Running Loss train 3.56787109375, Time 0.05932354927062988, Overall 2.8897178173065186 \n",
      "Fold 0/2, Epoch 1/24, Minibatch 0/0, Phase valid, Running Loss valid 30.647815704345703, Time 0.06523418426513672, Overall 2.8956284523010254 \n",
      "Fold 0/2, Epoch 1/24, Running Loss valid 3.4053128560384116, Time 0.06523418426513672, Overall 2.8956284523010254 \n",
      "EarlyStopping counter: 1 out of 5\n",
      "Fold 0/2, Epoch 2/24, Minibatch 0/0, Phase train, Running Loss train 56.354183197021484, Time 0.08339190483093262, Overall 2.9137861728668213 \n",
      "Fold 0/2, Epoch 2/24, Running Loss train 3.31495195276597, Time 0.08339190483093262, Overall 2.9137861728668213 \n",
      "Fold 0/2, Epoch 2/24, Minibatch 0/0, Phase valid, Running Loss valid 30.502113342285156, Time 0.09145164489746094, Overall 2.9218459129333496 \n",
      "Fold 0/2, Epoch 2/24, Running Loss valid 3.389123704698351, Time 0.09145164489746094, Overall 2.9218459129333496 \n",
      "EarlyStopping counter: 2 out of 5\n",
      "Fold 0/2, Epoch 3/24, Minibatch 0/0, Phase train, Running Loss train 58.54697799682617, Time 0.10250020027160645, Overall 2.932894468307495 \n",
      "Fold 0/2, Epoch 3/24, Running Loss train 3.4439398821662452, Time 0.10250020027160645, Overall 2.932894468307495 \n",
      "Fold 0/2, Epoch 3/24, Minibatch 0/0, Phase valid, Running Loss valid 29.370317459106445, Time 0.1081690788269043, Overall 2.938563346862793 \n",
      "Fold 0/2, Epoch 3/24, Running Loss valid 3.263368606567383, Time 0.1081690788269043, Overall 2.938563346862793 \n",
      "EarlyStopping counter: 3 out of 5\n",
      "Fold 0/2, Epoch 4/24, Minibatch 0/0, Phase train, Running Loss train 63.74433517456055, Time 0.12379646301269531, Overall 2.954190731048584 \n",
      "Fold 0/2, Epoch 4/24, Running Loss train 3.7496667749741497, Time 0.12379646301269531, Overall 2.954190731048584 \n",
      "Fold 0/2, Epoch 4/24, Minibatch 0/0, Phase valid, Running Loss valid 35.07053756713867, Time 0.12661433219909668, Overall 2.9570086002349854 \n",
      "Fold 0/2, Epoch 4/24, Running Loss valid 3.8967263963487415, Time 0.12661433219909668, Overall 2.9570086002349854 \n",
      "EarlyStopping counter: 4 out of 5\n",
      "Fold 0/2, Epoch 5/24, Minibatch 0/0, Phase train, Running Loss train 57.39683151245117, Time 0.1368389129638672, Overall 2.967233180999756 \n",
      "Fold 0/2, Epoch 5/24, Running Loss train 3.3762842066147747, Time 0.1368389129638672, Overall 2.967233180999756 \n",
      "Fold 0/2, Epoch 5/24, Minibatch 0/0, Phase valid, Running Loss valid 34.91938781738281, Time 0.14705967903137207, Overall 2.9774539470672607 \n",
      "Fold 0/2, Epoch 5/24, Running Loss valid 3.8799319797092013, Time 0.14705967903137207, Overall 2.9774539470672607 \n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early Stopping Triggered at epoch: 5\n",
      "Fold 1/2, Epoch 0/24, Minibatch 0/0, Phase train, Running Loss train 50.4671630859375, Time 0.011385440826416016, Overall 2.9888393878936768 \n",
      "Fold 1/2, Epoch 0/24, Running Loss train 2.9686566521139706, Time 0.011385440826416016, Overall 2.9888393878936768 \n",
      "Fold 1/2, Epoch 0/24, Minibatch 0/0, Phase valid, Running Loss valid 26.31653594970703, Time 0.020749330520629883, Overall 2.9982032775878906 \n",
      "Fold 1/2, Epoch 0/24, Running Loss valid 2.9240595499674478, Time 0.020749330520629883, Overall 2.9982032775878906 \n",
      "Fold 1/2, Epoch 1/24, Minibatch 0/0, Phase train, Running Loss train 61.07807540893555, Time 0.03136086463928223, Overall 3.008814811706543 \n",
      "Fold 1/2, Epoch 1/24, Running Loss train 3.592827965231503, Time 0.03136086463928223, Overall 3.008814811706543 \n",
      "Fold 1/2, Epoch 1/24, Minibatch 0/0, Phase valid, Running Loss valid 25.05188751220703, Time 0.041571855545043945, Overall 3.0190258026123047 \n",
      "Fold 1/2, Epoch 1/24, Running Loss valid 2.7835430569118924, Time 0.041571855545043945, Overall 3.0190258026123047 \n",
      "Fold 1/2, Epoch 2/24, Minibatch 0/0, Phase train, Running Loss train 54.176998138427734, Time 0.059981346130371094, Overall 3.037435293197632 \n",
      "Fold 1/2, Epoch 2/24, Running Loss train 3.1868822434369255, Time 0.059981346130371094, Overall 3.037435293197632 \n",
      "Fold 1/2, Epoch 2/24, Minibatch 0/0, Phase valid, Running Loss valid 25.165163040161133, Time 0.059981346130371094, Overall 3.037435293197632 \n",
      "Fold 1/2, Epoch 2/24, Running Loss valid 2.7961292266845703, Time 0.06796979904174805, Overall 3.045423746109009 \n",
      "EarlyStopping counter: 1 out of 5\n",
      "Fold 1/2, Epoch 3/24, Minibatch 0/0, Phase train, Running Loss train 64.68170166015625, Time 0.0787043571472168, Overall 3.0561583042144775 \n",
      "Fold 1/2, Epoch 3/24, Running Loss train 3.804805980009191, Time 0.0787043571472168, Overall 3.0561583042144775 \n",
      "Fold 1/2, Epoch 3/24, Minibatch 0/0, Phase valid, Running Loss valid 24.351518630981445, Time 0.08578085899353027, Overall 3.063234806060791 \n",
      "Fold 1/2, Epoch 3/24, Running Loss valid 2.7057242923312717, Time 0.08578085899353027, Overall 3.063234806060791 \n",
      "Fold 1/2, Epoch 4/24, Minibatch 0/0, Phase train, Running Loss train 59.532955169677734, Time 0.09955906867980957, Overall 3.0770130157470703 \n",
      "Fold 1/2, Epoch 4/24, Running Loss train 3.5019385393928077, Time 0.09955906867980957, Overall 3.0770130157470703 \n",
      "Fold 1/2, Epoch 4/24, Minibatch 0/0, Phase valid, Running Loss valid 24.95684242248535, Time 0.10916614532470703, Overall 3.0866200923919678 \n",
      "Fold 1/2, Epoch 4/24, Running Loss valid 2.7729824913872614, Time 0.10916614532470703, Overall 3.0866200923919678 \n",
      "EarlyStopping counter: 1 out of 5\n",
      "Fold 1/2, Epoch 5/24, Minibatch 0/0, Phase train, Running Loss train 60.684104919433594, Time 0.12008070945739746, Overall 3.097534656524658 \n",
      "Fold 1/2, Epoch 5/24, Running Loss train 3.569653230554917, Time 0.12008070945739746, Overall 3.097534656524658 \n",
      "Fold 1/2, Epoch 5/24, Minibatch 0/0, Phase valid, Running Loss valid 26.791095733642578, Time 0.12008070945739746, Overall 3.097534656524658 \n",
      "Fold 1/2, Epoch 5/24, Running Loss valid 2.9767884148491754, Time 0.12008070945739746, Overall 3.097534656524658 \n",
      "EarlyStopping counter: 2 out of 5\n",
      "Fold 1/2, Epoch 6/24, Minibatch 0/0, Phase train, Running Loss train 53.70836639404297, Time 0.14064717292785645, Overall 3.118101119995117 \n",
      "Fold 1/2, Epoch 6/24, Running Loss train 3.1593156702378216, Time 0.14064717292785645, Overall 3.118101119995117 \n",
      "Fold 1/2, Epoch 6/24, Minibatch 0/0, Phase valid, Running Loss valid 24.992084503173828, Time 0.14064717292785645, Overall 3.118101119995117 \n",
      "Fold 1/2, Epoch 6/24, Running Loss valid 2.7768982781304254, Time 0.14820218086242676, Overall 3.1256561279296875 \n",
      "EarlyStopping counter: 3 out of 5\n",
      "Fold 1/2, Epoch 7/24, Minibatch 0/0, Phase train, Running Loss train 65.23921966552734, Time 0.16184067726135254, Overall 3.1392946243286133 \n",
      "Fold 1/2, Epoch 7/24, Running Loss train 3.837601156795726, Time 0.16184067726135254, Overall 3.1392946243286133 \n",
      "Fold 1/2, Epoch 7/24, Minibatch 0/0, Phase valid, Running Loss valid 27.554105758666992, Time 0.16622281074523926, Overall 3.1436767578125 \n",
      "Fold 1/2, Epoch 7/24, Running Loss valid 3.0615673065185547, Time 0.16622281074523926, Overall 3.1436767578125 \n",
      "EarlyStopping counter: 4 out of 5\n",
      "Fold 1/2, Epoch 8/24, Minibatch 0/0, Phase train, Running Loss train 54.793331146240234, Time 0.18266987800598145, Overall 3.160123825073242 \n",
      "Fold 1/2, Epoch 8/24, Running Loss train 3.2231371262494255, Time 0.18266987800598145, Overall 3.160123825073242 \n",
      "Fold 1/2, Epoch 8/24, Minibatch 0/0, Phase valid, Running Loss valid 30.626035690307617, Time 0.1900951862335205, Overall 3.1675491333007812 \n",
      "Fold 1/2, Epoch 8/24, Running Loss valid 3.4028928544786243, Time 0.19216489791870117, Overall 3.169618844985962 \n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early Stopping Triggered at epoch: 8\n",
      "Fold 2/2, Epoch 0/24, Minibatch 0/0, Phase train, Running Loss train 61.94446563720703, Time 0.012833833694458008, Overall 3.183655261993408 \n",
      "Fold 2/2, Epoch 0/24, Running Loss train 3.441359202067057, Time 0.012833833694458008, Overall 3.183655261993408 \n",
      "Fold 2/2, Epoch 0/24, Minibatch 0/0, Phase valid, Running Loss valid 33.1606559753418, Time 0.021173954010009766, Overall 3.19199538230896 \n",
      "Fold 2/2, Epoch 0/24, Running Loss valid 4.145081996917725, Time 0.021173954010009766, Overall 3.19199538230896 \n",
      "Fold 2/2, Epoch 1/24, Minibatch 0/0, Phase train, Running Loss train 61.42078399658203, Time 0.034293174743652344, Overall 3.2051146030426025 \n",
      "Fold 2/2, Epoch 1/24, Running Loss train 3.4122657775878906, Time 0.034293174743652344, Overall 3.2051146030426025 \n",
      "Fold 2/2, Epoch 1/24, Minibatch 0/0, Phase valid, Running Loss valid 34.1731071472168, Time 0.04395723342895508, Overall 3.2147786617279053 \n",
      "Fold 2/2, Epoch 1/24, Running Loss valid 4.2716383934021, Time 0.04501605033874512, Overall 3.2158374786376953 \n",
      "EarlyStopping counter: 1 out of 5\n",
      "Fold 2/2, Epoch 2/24, Minibatch 0/0, Phase train, Running Loss train 55.99708938598633, Time 0.05119919776916504, Overall 3.2220206260681152 \n",
      "Fold 2/2, Epoch 2/24, Running Loss train 3.110949410332574, Time 0.05119919776916504, Overall 3.2220206260681152 \n",
      "Fold 2/2, Epoch 2/24, Minibatch 0/0, Phase valid, Running Loss valid 28.15422821044922, Time 0.059386253356933594, Overall 3.230207681655884 \n",
      "Fold 2/2, Epoch 2/24, Running Loss valid 3.5192785263061523, Time 0.059386253356933594, Overall 3.230207681655884 \n",
      "Fold 2/2, Epoch 3/24, Minibatch 0/0, Phase train, Running Loss train 56.34727096557617, Time 0.07626891136169434, Overall 3.2470903396606445 \n",
      "Fold 2/2, Epoch 3/24, Running Loss train 3.1304039425320096, Time 0.07626891136169434, Overall 3.2470903396606445 \n",
      "Fold 2/2, Epoch 3/24, Minibatch 0/0, Phase valid, Running Loss valid 31.518003463745117, Time 0.08641672134399414, Overall 3.2572381496429443 \n",
      "Fold 2/2, Epoch 3/24, Running Loss valid 3.9397504329681396, Time 0.08641672134399414, Overall 3.2572381496429443 \n",
      "EarlyStopping counter: 1 out of 5\n",
      "Fold 2/2, Epoch 4/24, Minibatch 0/0, Phase train, Running Loss train 58.459747314453125, Time 0.09867262840270996, Overall 3.26949405670166 \n",
      "Fold 2/2, Epoch 4/24, Running Loss train 3.24776373969184, Time 0.09967970848083496, Overall 3.270501136779785 \n",
      "Fold 2/2, Epoch 4/24, Minibatch 0/0, Phase valid, Running Loss valid 32.78290557861328, Time 0.10074162483215332, Overall 3.2715630531311035 \n",
      "Fold 2/2, Epoch 4/24, Running Loss valid 4.09786319732666, Time 0.10074162483215332, Overall 3.2715630531311035 \n",
      "EarlyStopping counter: 2 out of 5\n",
      "Fold 2/2, Epoch 5/24, Minibatch 0/0, Phase train, Running Loss train 60.466224670410156, Time 0.11774754524230957, Overall 3.2885689735412598 \n",
      "Fold 2/2, Epoch 5/24, Running Loss train 3.3592347039116754, Time 0.11774754524230957, Overall 3.2885689735412598 \n",
      "Fold 2/2, Epoch 5/24, Minibatch 0/0, Phase valid, Running Loss valid 33.2980842590332, Time 0.12505483627319336, Overall 3.2958762645721436 \n",
      "Fold 2/2, Epoch 5/24, Running Loss valid 4.16226053237915, Time 0.12505483627319336, Overall 3.2958762645721436 \n",
      "EarlyStopping counter: 3 out of 5\n",
      "Fold 2/2, Epoch 6/24, Minibatch 0/0, Phase train, Running Loss train 54.81303787231445, Time 0.1399996280670166, Overall 3.310821056365967 \n",
      "Fold 2/2, Epoch 6/24, Running Loss train 3.0451687706841364, Time 0.1399996280670166, Overall 3.310821056365967 \n",
      "Fold 2/2, Epoch 6/24, Minibatch 0/0, Phase valid, Running Loss valid 32.196929931640625, Time 0.14185714721679688, Overall 3.312678575515747 \n",
      "Fold 2/2, Epoch 6/24, Running Loss valid 4.024616241455078, Time 0.14185714721679688, Overall 3.312678575515747 \n",
      "EarlyStopping counter: 4 out of 5\n",
      "Fold 2/2, Epoch 7/24, Minibatch 0/0, Phase train, Running Loss train 59.711509704589844, Time 0.15899181365966797, Overall 3.329813241958618 \n",
      "Fold 2/2, Epoch 7/24, Running Loss train 3.3173060946994357, Time 0.15899181365966797, Overall 3.329813241958618 \n",
      "Fold 2/2, Epoch 7/24, Minibatch 0/0, Phase valid, Running Loss valid 28.357357025146484, Time 0.1667766571044922, Overall 3.3375980854034424 \n",
      "Fold 2/2, Epoch 7/24, Running Loss valid 3.5446696281433105, Time 0.1677868366241455, Overall 3.3386082649230957 \n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early Stopping Triggered at epoch: 7\n",
      "It took 3.3386082649230957 to train the model.\n",
      "\n",
      ".............. ending learn .................\n",
      "\n",
      "\n",
      ".............. starting test .................\n",
      "\n",
      "\n",
      ".............. ending test .................\n",
      "\n",
      "\n",
      ".............. starting eval .................\n",
      "\n",
      "Calculating Skill Coverage for 5 predictions\n",
      "Converting the prediction probabilities to 1-hot predictions\n",
      "-------- Converting for k = 2\n",
      "-------- Converted for k = 2\n",
      "\n",
      "-------- Converting for k = 5\n",
      "-------- Converted for k = 5\n",
      "\n",
      "-------- Converting for k = 10\n",
      "-------- Converted for k = 10\n",
      "\n",
      "---- Calculating skc for k = 2\n",
      "---- Calculated skc for k = 2\n",
      "---- Calculating skc for k = 5\n",
      "---- Calculated skc for k = 5\n",
      "---- Calculating skc for k = 10\n",
      "---- Calculated skc for k = 10\n",
      "\n",
      "Calculating roc_auc_score ...\n",
      "Calculating roc_curve ...\n",
      "\n",
      "Building pytrec_eval input for 5 instances ...\n",
      "Evaluating {'ndcg_cut_2,5,10', 'P_2,5,10', 'recall_2,5,10', 'map_cut_2,5,10'} ...\n",
      "Averaging ...\n",
      "\n",
      "Saving file per fold as : f0.test.pred.eval.mean.csv\n",
      "Calculating Skill Coverage for 5 predictions\n",
      "Converting the prediction probabilities to 1-hot predictions\n",
      "-------- Converting for k = 2\n",
      "-------- Converted for k = 2\n",
      "\n",
      "-------- Converting for k = 5\n",
      "-------- Converted for k = 5\n",
      "\n",
      "-------- Converting for k = 10\n",
      "-------- Converted for k = 10\n",
      "\n",
      "---- Calculating skc for k = 2\n",
      "---- Calculated skc for k = 2\n",
      "---- Calculating skc for k = 5\n",
      "---- Calculated skc for k = 5\n",
      "---- Calculating skc for k = 10\n",
      "---- Calculated skc for k = 10\n",
      "\n",
      "Calculating roc_auc_score ...\n",
      "Calculating roc_curve ...\n",
      "\n",
      "Building pytrec_eval input for 5 instances ...\n",
      "Evaluating {'ndcg_cut_2,5,10', 'P_2,5,10', 'recall_2,5,10', 'map_cut_2,5,10'} ...\n",
      "Averaging ...\n",
      "\n",
      "Saving file per fold as : f1.test.pred.eval.mean.csv\n",
      "Calculating Skill Coverage for 5 predictions\n",
      "Converting the prediction probabilities to 1-hot predictions\n",
      "-------- Converting for k = 2\n",
      "-------- Converted for k = 2\n",
      "\n",
      "-------- Converting for k = 5\n",
      "-------- Converted for k = 5\n",
      "\n",
      "-------- Converting for k = 10\n",
      "-------- Converted for k = 10\n",
      "\n",
      "---- Calculating skc for k = 2\n",
      "---- Calculated skc for k = 2\n",
      "---- Calculating skc for k = 5\n",
      "---- Calculated skc for k = 5\n",
      "---- Calculating skc for k = 10\n",
      "---- Calculated skc for k = 10\n",
      "\n",
      "Calculating roc_auc_score ...\n",
      "Calculating roc_curve ...\n",
      "\n",
      "Building pytrec_eval input for 5 instances ...\n",
      "Evaluating {'ndcg_cut_2,5,10', 'P_2,5,10', 'recall_2,5,10', 'map_cut_2,5,10'} ...\n",
      "Averaging ...\n",
      "\n",
      "Saving file per fold as : f2.test.pred.eval.mean.csv\n",
      "Saving mean evaluation file over nfolds as : test.pred.eval.mean.csv\n",
      "\n",
      ".............. ending eval .................\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\Documents\\Pycharm_Projects\\OpeNTF\\src\\mdl\\fnn.py:315: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  if per_epoch: modelfiles += [f'{model_path}/{_}' for _ in os.listdir(model_path) if re.match(f'state_dict_model.f{foldidx}.e\\d+.pt', _)]\n",
      "C:\\Users\\Owner\\Documents\\Pycharm_Projects\\OpeNTF\\src\\mdl\\ntf.py:31: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  predfiles = [f'{model_path}/{_}' for _ in os.listdir(model_path) if re.match('state_dict_model.f\\d+.pt', _)]\n",
      "C:\\Users\\Owner\\Documents\\Pycharm_Projects\\OpeNTF\\src\\mdl\\ntf.py:32: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  if per_epoch: predfiles += [f'{model_path}/{_}' for _ in os.listdir(model_path) if re.match('state_dict_model.f\\d+.e\\d+', _)]\n",
      "C:\\Users\\Owner\\Documents\\Pycharm_Projects\\OpeNTF\\src\\mdl\\bnn_old.py:245: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  if per_epoch: modelfiles += [f'{model_path}/{_}' for _ in os.listdir(model_path) if re.match(f'state_dict_model.f{foldidx}.e\\d+.pt', _)]\n",
      "C:\\Users\\Owner\\Documents\\Pycharm_Projects\\OpeNTF\\src\\mdl\\bnn.py:227: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  re.match(f'state_dict_model.f{foldidx}.e\\d+.pt', _)]\n",
      "C:\\Users\\Owner\\Documents\\Pycharm_Projects\\OpeNTF\\src\\main.py:177: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  emb_skill = torch.load(emb_filepath, map_location=torch.device('cpu'))['skill'].detach().numpy()\n",
      "C:\\Users\\Owner\\Documents\\Pycharm_Projects\\venvs\\tmp_opentf\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Owner\\Documents\\Pycharm_Projects\\OpeNTF\\src\\mdl\\fnn.py:319: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(modelfile))\n",
      "C:\\Users\\Owner\\Documents\\Pycharm_Projects\\OpeNTF\\src\\mdl\\ntf.py:42: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  Y_ = torch.load(f'{model_path}/f{foldidx}.{pred_set}.{epoch}pred')\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\n",
      "100%|##########| 5/5 [00:00<?, ?it/s]\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\n",
      "100%|##########| 5/5 [00:00<?, ?it/s]\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\n",
      "100%|##########| 5/5 [00:00<?, ?it/s]\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\n",
      "100%|##########| 5/5 [00:00<00:00, 832.93it/s]\n",
      "C:\\Users\\Owner\\Documents\\Pycharm_Projects\\OpeNTF\\src\\mdl\\ntf.py:42: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  Y_ = torch.load(f'{model_path}/f{foldidx}.{pred_set}.{epoch}pred')\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\n",
      "100%|##########| 5/5 [00:00<?, ?it/s]\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\n",
      "100%|##########| 5/5 [00:00<?, ?it/s]\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\n",
      "100%|##########| 5/5 [00:00<?, ?it/s]\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\n",
      "100%|##########| 5/5 [00:00<00:00, 482.11it/s]\n",
      "C:\\Users\\Owner\\Documents\\Pycharm_Projects\\OpeNTF\\src\\mdl\\ntf.py:42: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  Y_ = torch.load(f'{model_path}/f{foldidx}.{pred_set}.{epoch}pred')\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\n",
      "100%|##########| 5/5 [00:00<?, ?it/s]\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\n",
      "100%|##########| 5/5 [00:00<?, ?it/s]\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\n",
      "100%|##########| 5/5 [00:00<?, ?it/s]\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\n",
      "100%|##########| 5/5 [00:00<00:00, 431.74it/s]\n"
     ]
    }
   ],
   "source": [
    "!python -u main.py -data ../data/raw/dblp/toy.dblp.v12.json -domain dblp -model fnn --emb_model gat --emb_graph_type stm --emb_agg mean --emb_e 5 --emb_d 8 --emb_ns 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "## Setting Hyperparameters\n",
    "`OpeNTF`'s codebase offers the following hyperparameter to be set for each neural team formation methods:\n",
    "\n",
    "### `model`\n",
    "- Contains the baseline hyperparameters in the form of `'model-name' : { params }`, which allows the models to be integrated into the baseline with their unique parameters.\n",
    "- Allows the customization of which stages of the system to be executed through `cmd`.\n",
    "- Contains other training parameter for the models (e.g., temporal).\n",
    "\n",
    "### `data`\n",
    "- Contains parameters for manipulating datasets, including dataset filters (e.g., minimum team size) and bucket size for sparse matrix parallel generation.\n",
    "\n",
    "### `fair`\n",
    "- Contains parameters for the fairness metrics used in consideration during team formation.\n",
    "\n",
    "A snippet of the parameters used in `src/mdl/team2vec/params.py` is displayed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "During setting up the edge_types, if we want to include skill-skill or expert-expert connections,\n",
    "the edge_type identifier \"sm\" or \"stm\" will have to be included as \"sm.en\" or \"stm.en\" [en = enhanced]\n",
    "'''\n",
    "\n",
    "\n",
    "settings = {\n",
    "    'graph':{\n",
    "        'edge_types':                   # this is an array holding the edge_type info in the form [('edge_type1', 'edge_type1_code'), ('edge_type2', 'edge_type2_code') ... ]\n",
    "            # [('member', 'm')],\n",
    "            # [([('skill', 'to', 'member')], 'sm')],\n",
    "            # [([('skill', 'to', 'skill'), ('member', 'to', 'member'), ('skill', 'to', 'member')], 'sm')], # sm enhanced\n",
    "            # [([('skill', 'to', 'team'), ('member', 'to', 'team')], 'stm')],\n",
    "            # [([('skill', 'to', 'team'), ('member', 'to', 'team'), ('loc', 'to', 'team')], 'stml')],\n",
    "            [([('skill', 'to', 'member')], 'sm'), ([('skill', 'to', 'team'), ('member', 'to', 'team')], 'stm'), ([('skill', 'to', 'team'), ('member', 'to', 'team'), ('loc', 'to', 'team')], 'stml')],  # sm, stm, stml\n",
    "            # ([('skill', 'to', 'skill'), ('member', 'to', 'member'), ('skill', 'to', 'team'), ('member', 'to', 'team'), ('skill', 'to', 'member')], 'stm') # stm enhanced,\n",
    "            # [([('skill', 'to', 'member')], 'sm'), ([('skill', 'to', 'team'), ('member', 'to', 'team')], 'stm')],\n",
    "            # [([('skill', 'to', 'member')], 'sm'), ([('skill', 'to', 'team'), ('member', 'to', 'team'), ('location', 'to', 'team')], 'stml')],\n",
    "            # [([('skill', 'to', 'skill'), ('member', 'to', 'member'), ('skill', 'to', 'member')], 'sm.en'), ([('skill', 'to', 'skill'), ('member', 'to', 'member'), ('skill', 'to', 'team'), ('member', 'to', 'team'), ('skill','to','member')], 'stm.en')], # sm stm strongly connected\n",
    "\n",
    "        'custom_supervision' : False, # if false, it will take all the forward edge_types as supervision edges\n",
    "        # 'supervision_edge_types': [([('skill', 'to', 'skill'), ('member', 'to', 'member'), ('skill', 'to', 'member')], 'sm'), ([('skill', 'to', 'skill'), ('member', 'to', 'member'), ('skill', 'to', 'team'), ('member', 'to', 'team'), ('skill', 'to', 'member')], 'stm')], # sm stm strongly connected\n",
    "        'supervision_edge_types': [([('skill', 'to', 'member')], 'sm.en'), ([('skill', 'to', 'team'), ('member', 'to', 'team')], 'stm.en')],\n",
    "        'dir': False,\n",
    "        'dup_edge': ['add', 'mean', 'min', 'max', 'mul'],         #None: keep the duplicates, else: reduce by 'add', 'mean', 'min', 'max', 'mul'\n",
    "    },\n",
    "    'model': {\n",
    "        'd' : 8,                    # embedding dim array\n",
    "        'b' : 128,                  # batch_size for loaders\n",
    "        'e' : 100,                  # num epochs\n",
    "        'ns' : 5,                   # number of negative samples\n",
    "        'lr': 0.001,\n",
    "        'loader_shuffle': True,\n",
    "        'num_workers': 0,\n",
    "        'save_per_epoch': False,\n",
    "        'pt' : 0,                   # 1 -> use pretrained d2v skill vectors as initial node features of graph data\n",
    "\n",
    "        'gnn.gs': {\n",
    "            'e' : 100,                # number of epochs\n",
    "            'b' : 128,              # batch size\n",
    "            'd' : 8,                # embedding dimension\n",
    "            'ns' : 5,               # number of negative samples\n",
    "            'h' : 2,                # number of attention heads (if applicable)\n",
    "            'nn' : [30, 20],        # number of neighbors in each hop ([20, 10] -> 20 neighbors in first hop, 10 neighbors in second hop)\n",
    "            'graph_types' : 'stm',   # graph type used for a single run (stm -> ste -> skill-team-expert)\n",
    "            'agg' : 'mean',         # aggregation method used for merging multiple edges between the same source and destination node\n",
    "            'dir' : False,          # whether the graph is directed\n",
    "        },\n",
    "        'gnn.gin': {\n",
    "            'e': 100,\n",
    "            'b': 128,\n",
    "            'd': 8,\n",
    "            'ns' : 5,\n",
    "            'h': 2,\n",
    "            'nn': [30, 20],\n",
    "            'graph_types': 'stm',\n",
    "            'agg': 'mean',\n",
    "            'dir': False,\n",
    "        },\n",
    "        'gnn.gat': {\n",
    "            'e': 100,\n",
    "            'b': 128,\n",
    "            'd': 8,\n",
    "            'ns' : 5,\n",
    "            'h': 2,\n",
    "            'nn': [30, 20],\n",
    "            'graph_types': 'stm',\n",
    "            'agg': 'mean',\n",
    "            'dir': False,\n",
    "        },\n",
    "    },\n",
    "    'cmd' : ['graph', 'emb'],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure and Inheritance\n",
    "\n",
    "### Dataset Structure\n",
    "<p align=\"center\"><img src='dataset_hierarchy.png' width=\"500\" ></p>\n",
    "\n",
    "To integrate a new dataset into the baseline, follow the structure of the `team` class. Additional fields can be added, like its derived classes. Ideally, only the `read_data()` function should be overriden.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Structure\n",
    "<img src=\"./gnn_hierarchy.png\" height=400px />\n",
    "\n",
    "To integrate a new model into the GNN baseline, we can create the driver gnn model class and then apply that model into the `encoder` and `decoder` classes respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Resources\n",
    "- [`OpeNTF` codebase](https://github.com/fani-lab/OpeNTF/tree/main)\n",
    "- [`OpeNTF on GNN` codebase](https://github.com/fani-lab/OpeNTF/tree/gnn)\n",
    "- [`Adila` codebase](https://github.com/fani-lab/adila)\n",
    "- [`vivaFemme` codebase](https://github.com/fani-lab/OpeNTF/tree/vivaFemme)\n",
    "- [Streaming Training Strategy codebase](https://github.com/fani-lab/OpeNTF/tree/ecir24)\n",
    "- [Tutorial Website and Materials](https://fani-lab.github.io/OpeNTF/tutorial/umap24/)\n",
    "    - [`OpeNTF` paper](https://doi.org/10.1145/3511808.3557526)\n",
    "    - [`Adila` paper](https://doi.org/10.1007/978-3-031-37249-0_9)\n",
    "    - [`vivaFemme` paper](https://hosseinfani.github.io/res/papers/2024_BIAS_SIGIR_vivaFemme_Mitigating_Gender_Bias_in_Neural_Team_Recommendation_via_Female-Advocate_Loss_Regularization.pdf)\n",
    "    - [Streaming Training Strategy paper](https://link.springer.com/chapter/10.1007/978-3-031-56027-9_20)\n",
    "\n",
    "<img src=\"qr-code.png\" height=300px />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tmp_opentf",
   "language": "python",
   "name": "tmp_opentf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
