batch_size: 4
data:
  corpus_1:
    path_src: ./../output/dblp.v12.json.filtered.mt75.ts3/nmt/t99375.s29661.m14214.base_config./mdl/nmt_config.yaml/t99375.s29661.m14214.etLSTM.l128.wv128.lr0.001.b4.e1000/fold1/src-train.txt
    path_tgt: ./../output/dblp.v12.json.filtered.mt75.ts3/nmt/t99375.s29661.m14214.base_config./mdl/nmt_config.yaml/t99375.s29661.m14214.etLSTM.l128.wv128.lr0.001.b4.e1000/fold1/tgt-train.txt
  valid:
    path_src: ./../output/dblp.v12.json.filtered.mt75.ts3/nmt/t99375.s29661.m14214.base_config./mdl/nmt_config.yaml/t99375.s29661.m14214.etLSTM.l128.wv128.lr0.001.b4.e1000/fold1/src-valid.txt
    path_tgt: ./../output/dblp.v12.json.filtered.mt75.ts3/nmt/t99375.s29661.m14214.base_config./mdl/nmt_config.yaml/t99375.s29661.m14214.etLSTM.l128.wv128.lr0.001.b4.e1000/fold1/tgt-valid.txt
decay_steps: 50
decoder_type: rnn
encoder_type: rnn
gpu_ranks:
- 0
keep_checkpoint: -1
layers: 1
learning_rate: 0.001
learning_rate_decay: 0.9
optim: adam
overwrite: true
report_every: 1000
rnn_size: 128
rnn_type: LSTM
save_checkpoint_steps: 500
save_data: ./../output/dblp.v12.json.filtered.mt75.ts3/nmt/t99375.s29661.m14214.base_config./mdl/nmt_config.yaml/t99375.s29661.m14214.etLSTM.l128.wv128.lr0.001.b4.e1000/fold1/
save_model: ./../output/dblp.v12.json.filtered.mt75.ts3/nmt/t99375.s29661.m14214.base_config./mdl/nmt_config.yaml/t99375.s29661.m14214.etLSTM.l128.wv128.lr0.001.b4.e1000/fold1/model
seed: 0
src_vocab: ./../output/dblp.v12.json.filtered.mt75.ts3/nmt/t99375.s29661.m14214.base_config./mdl/nmt_config.yaml/t99375.s29661.m14214.etLSTM.l128.wv128.lr0.001.b4.e1000/fold1/vocab.src
start_decay_steps: 50
tgt_vocab: ./../output/dblp.v12.json.filtered.mt75.ts3/nmt/t99375.s29661.m14214.base_config./mdl/nmt_config.yaml/t99375.s29661.m14214.etLSTM.l128.wv128.lr0.001.b4.e1000/fold1/vocab.tgt
train_steps: 1000
valid_steps: 1000
vocab_size: 30000
word_vec_size: 128
world_size: 1
