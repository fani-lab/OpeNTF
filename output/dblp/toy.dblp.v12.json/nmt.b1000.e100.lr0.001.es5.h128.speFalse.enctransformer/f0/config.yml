save_data: ../output/dblp/toy.dblp.v12.json/nmt.b1000.e100.lr0.001.es5.h128.speFalse.enctransformer/f0/
save_model: ../output/dblp/toy.dblp.v12.json/nmt.b1000.e100.lr0.001.es5.h128.speFalse.enctransformer/f0/model
src_vocab: ../output/dblp/toy.dblp.v12.json/nmt.b1000.e100.lr0.001.es5.h128.speFalse.enctransformer/f0/vocab.src
tgt_vocab: ../output/dblp/toy.dblp.v12.json/nmt.b1000.e100.lr0.001.es5.h128.speFalse.enctransformer/f0/vocab.tgt
overwrite: true
data:
  corpus_1:
    path_src: ../output/dblp/toy.dblp.v12.json/nmt.b1000.e100.lr0.001.es5.h128.speFalse.enctransformer/f0/src-train.txt
    path_tgt: ../output/dblp/toy.dblp.v12.json/nmt.b1000.e100.lr0.001.es5.h128.speFalse.enctransformer/f0/tgt-train.txt
    weight: 1
  valid:
    path_src: ../output/dblp/toy.dblp.v12.json/nmt.b1000.e100.lr0.001.es5.h128.speFalse.enctransformer/f0/src-valid.txt
    path_tgt: ../output/dblp/toy.dblp.v12.json/nmt.b1000.e100.lr0.001.es5.h128.speFalse.enctransformer/f0/tgt-valid.txt
keep_checkpoint: -1
seed: 0
train_epochs: 100
save_checkpoint_steps: 100
valid_steps: 1
report_every: 1
early_stopping: 5
early_stopping_criteria: accuracy
dropout: 0.2
word_vec_size: 128
world_size: 1
gpu_ranks: []
num_workers: 8
batch_size: 1000
bucket_size: 832
valid_batch_size: 1000
model_dtype: fp16
optim: adam
weight_decay: 0.0001
learning_rate: 0.001
adam_beta1: 0.9
adam_beta2: 0.98
encoder_type: transformer
decoder_type: transformer
cnn_size: 128
cnn_kernel_width: 3
layers: 4
rnn_type: LSTM
rnn_size: 128
input_feed: 1
enc_layers: 4
dec_layers: 4
warmup_steps: 501
position_encoding: false
hidden_size: 128
transformer_ff: 512
attention_dropout: 0.2
heads: 8
