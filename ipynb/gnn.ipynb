{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fani-lab/OpeNTF/blob/main/ipynb/gnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QGHFtvGnTLJ"
      },
      "source": [
        "`OpeNTF-GNN` via `PyG`\n",
        "\n",
        "`OpeNTF` previously used traditional embedding methods (non-graph based) like `doc2vec` to learn skill embeddings as an input alternative to the `1-hot` encoded skills. With graph neural networks (gnn) in `PyG`, we now have integrated graph-based skill embeddings. The gnns capture the synergistic collaborative ties within our transformed graph data to provide with significantly better embeddings for skills, or even direct recommendation of experts for a team via link prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4tk12X-nTLK"
      },
      "source": [
        "**Expert (Member) Graph Structures**\n",
        "\n",
        "<p align=\"center\"><img src='https://raw.githubusercontent.com/fani-lab/OpeNTF/refs/heads/main/docs/graph_structures.png' width=\"400\" ></p>\n",
        "\n",
        "`OpeNTF` applied with gnn aims to cover as many variations in graph structures for a given set of team instances. Currently, it implemented `heterogeneous`, `directed`, `unweighted` graph structures including `[[[skill, to, member]], sm]` bipartite, `[[[skill, to, team], [member, to, team]], stm]` tripartite and `[[[skill, to, team], [member, to, team], [loc, to, team]], stml]`, as seen in the figure, and can be set like:\n",
        "\n",
        "`\"+data.embedding.model.gnn.graph.structure=[[[skill, to, team], [member, to, team], [loc, to, team]], stml]\"`\n",
        "\n",
        "(see [`src/mdl/emb/__config__.yaml`](https://github.com/fani-lab/OpeNTF/blob/main/src/mdl/emb/__config__.yaml#L27) for more details)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ysL9sdP23CI-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "0Uiz41tPnTLK"
      },
      "source": [
        "**Transfer vs. End-to-End Learning with GNN**\n",
        "\n",
        "Gnn methods on an expert graph can be used in either of following ways:\n",
        "\n",
        "<p align=\"center\"><img src='https://raw.githubusercontent.com/fani-lab/OpeNTF/refs/heads/main/docs/transfer.png' width=\"500\" ></p>\n",
        "\n",
        "1.  **Transfer-based [[WISE24](https://doi.org/10.1007/978-981-96-0567-5_15), [IJCNN23](10.1109/IJCNN54540.2023.10191717), [SIGIR21](https://doi.org/10.1145/3404835.3463105)]**: A gnn method is mainly trained to learn `skill` embeddings, overlooking the embeddings for other node types, and then fed (transfer) into an underlying multilabel classifier, e.g., non-variational feedforward neural net ([`src/mdl/fnn.py`](https://github.com/fani-lab/OpeNTF/blob/main/src/mdl/fnn.py)) or variational Bayesian ([`src/mdl/bnn.py`](https://github.com/fani-lab/OpeNTF/blob/main/src/mdl/bnn.py)). In this case, `OpeNTF` runs in embedding mode by setting `data.embedding.class_method` like\n",
        "\n",
        "    `data.embedding.class_method=mdl.emb.gnn.Gnn_n2v` for [Node2Vec](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.Node2Vec.html)\n",
        "    `data.embedding.class_method=mdl.emb.gnn.Gnn_m2v` for [MetaPath2Vec](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.MetaPath2Vec.html)\n",
        "    `data.embedding.class_method=mdl.emb.gnn.Gnn_gs` for [GraphSAGE](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.SAGEConv.html)\n",
        "\n",
        "\n",
        "    (see [`src/__config__.yaml#L44`](https://github.com/fani-lab/OpeNTF/blob/main/src/__config__.yaml#L44) for more options)\n",
        "\n",
        "\n",
        "    and the classifier model(s) is set by `models.instances` like\n",
        "\n",
        "    `\"models.instances=[mdl.fnn.Fnn, mdl.bnn.Bnn]\"`\n",
        "\n",
        "    (see [`src/__config__.yaml#L57`](https://github.com/fani-lab/OpeNTF/blob/main/src/__config__.yaml#L57) for more options)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "python main.py  \"cmd=[prep,train,test,eval]\" \\\n",
        "                \"models.instances=[mdl.fnn.Fnn, mdl.bnn.Bnn]\" \\\n",
        "                data.domain=cmn.publication.Publication \\\n",
        "                data.source=../data/dblp/toy.dblp.v12.json \\\n",
        "                data.output=../output/dblp/toy.dblp.v12.json \\\n",
        "                ~data.filter \\\n",
        "                data.embedding.class_method=mdl.emb.gnn.Gnn_gs \\\n",
        "                \"+data.embedding.model.gnn.graph.structure=[[[skill, to, team], [member, to, team], [loc, to, team]], stml]\"\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "dE7D-VfK3Fla"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"center\"><img src='https://raw.githubusercontent.com/fani-lab/OpeNTF/refs/heads/main/docs/e2e.png' width=\"500\" ></p>\n",
        "\n",
        "2.   **Graph Neural Team Recommendation (End-to-End) [[WSDM26, Under Review](https://)]**: A gnn method is used to directly predict expert-team links to recommend top-k expert members of a team, skipping the underlying multilabel classifier, as shown above. In this case, `OpeNTF` runs in embedding mode by setting `data.embedding.class_method` like in transfer-based but the classifier model is set `fixed` by `\"models.instances=[mdl.emb.gnn.Gnn]\"`\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "python main.py  \"cmd=[prep,train,test,eval]\" \\\n",
        "                \"models.instances=[mdl.emb.gnn.Gnn]\" \\\n",
        "                data.domain=cmn.publication.Publication \\\n",
        "                data.source=../data/dblp/toy.dblp.v12.json \\\n",
        "                data.output=../output/dblp/toy.dblp.v12.json \\\n",
        "                ~data.filter \\\n",
        "                data.embedding.class_method=mdl.emb.gnn.Gnn_gs \\\n",
        "                \"+data.embedding.model.gnn.graph.structure=[[[skill, to, team], [member, to, team], [loc, to, team]], stml]\"\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "c8R8Dp2_wejY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "U3bXSGQC3Lic"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        },
        "id": "93caHB2EnTLL"
      },
      "source": [
        "**Hyperparameters**\n",
        "\n",
        "`OpeNTF` leverage `[hydra](https://hydra.cc/)` to manage models hyperparameters in hierarchy:\n",
        "\n",
        "*   [`src/__config__.yaml`](https://github.com/fani-lab/OpeNTF/blob/main/src/__config__.yaml): `OpeNTF`'s main settings for the pipeline execution like `data.*`, `models.*`, `train.*`, `test.*`, `eval.*`\n",
        "    *   [`src/mdl/__config__.yaml`](https://github.com/fani-lab/OpeNTF/blob/main/src/mdl/__config__.yaml): models' training hyperparameters like `fnn.*`, `bnn.*`, `tntf.*`, `lr`, `batch_size`, ...\n",
        "        * [`src/mdl/emb/__config__.yaml`](https://github.com/fani-lab/OpeNTF/blob/main/src/mdl/emb/__config__.yaml): training hyperparameters for embedding methods including gnns like `n2v.*`, `m2v.*`, ..., `dim`, ...\n",
        "\n",
        "To set these hyperparameters,\n",
        "\n",
        "- `Override` them in the running commands (recommended), or\n",
        "- Change the defaults in the `__config__.yaml` files\n",
        "\n",
        "\n",
        "```\n",
        "python main.py  \"cmd=[prep,train,test,eval]\" \\\n",
        "                \"models.instances=[mdl.rnd.Rnd, mdl.fnn.Fnn, mdl.bnn.Bnn, mdl.emb.gnn.Gnn]\" \\\n",
        "                data.domain=cmn.publication.Publication data.source=../data/dblp/toy.dblp.v12.json data.output=../output/dblp/toy.dblp.v12.json ~data.filter \\\n",
        "                data.embedding.class_method=mdl.emb.gnn.Gnn_gatv2 \\\n",
        "                +models.batch_size=2 +models.nsd=unigram_b'\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "rEqgd5eO95eD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwEN1xLDnTLK"
      },
      "source": [
        "**Setup & Quickstart**\n",
        "\n",
        "From the [`quickstart`](https://colab.research.google.com/github/fani-lab/OpeNTF/blob/main/ipynb/quickstart.ipynb) script:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mZln-_cnnTLM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b103a2d-7a22-4ad7-c7bf-4fbefd528185"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:3 https://cli.github.com/packages stable InRelease\n",
            "Get:4 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:10 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,289 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,526 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [5,969 kB]\n",
            "Get:15 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [2,123 kB]\n",
            "Get:16 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,421 kB]\n",
            "Get:17 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,820 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,594 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [6,168 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,856 kB]\n",
            "Fetched 37.2 MB in 8s (4,799 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libpython3.8-minimal libpython3.8-stdlib python3-pkg-resources\n",
            "  python3-setuptools python3-wheel python3.8-lib2to3 python3.8-minimal\n",
            "Suggested packages:\n",
            "  python-setuptools-doc binfmt-support\n",
            "The following NEW packages will be installed:\n",
            "  libpython3.8-minimal libpython3.8-stdlib python3-pip python3-setuptools\n",
            "  python3-wheel python3.8 python3.8-distutils python3.8-lib2to3\n",
            "  python3.8-minimal python3.8-venv\n",
            "The following packages will be upgraded:\n",
            "  python3-pkg-resources\n",
            "1 upgraded, 10 newly installed, 0 to remove and 43 not upgraded.\n",
            "Need to get 10.0 MB of archives.\n",
            "After this operation, 32.6 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 python3-wheel all 0.37.1-2ubuntu0.22.04.1 [32.0 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 python3-pip all 22.0.2+dfsg-1ubuntu0.7 [1,306 kB]\n",
            "Get:3 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 libpython3.8-minimal amd64 3.8.20-1+jammy1 [796 kB]\n",
            "Get:4 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 python3.8-minimal amd64 3.8.20-1+jammy1 [2,023 kB]\n",
            "Get:5 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 libpython3.8-stdlib amd64 3.8.20-1+jammy1 [1,817 kB]\n",
            "Get:6 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy/main amd64 python3-pkg-resources all 68.1.2-2~jammy3 [216 kB]\n",
            "Get:7 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy/main amd64 python3-setuptools all 68.1.2-2~jammy3 [465 kB]\n",
            "Get:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 python3.8 amd64 3.8.20-1+jammy1 [440 kB]\n",
            "Get:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 python3.8-lib2to3 all 3.8.20-1+jammy1 [126 kB]\n",
            "Get:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 python3.8-distutils all 3.8.20-1+jammy1 [193 kB]\n",
            "Get:11 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 python3.8-venv amd64 3.8.20-1+jammy1 [2,618 kB]\n",
            "Fetched 10.0 MB in 21s (489 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 11.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libpython3.8-minimal:amd64.\n",
            "(Reading database ... 125082 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libpython3.8-minimal_3.8.20-1+jammy1_amd64.deb ...\n",
            "Unpacking libpython3.8-minimal:amd64 (3.8.20-1+jammy1) ...\n",
            "Selecting previously unselected package python3.8-minimal.\n",
            "Preparing to unpack .../01-python3.8-minimal_3.8.20-1+jammy1_amd64.deb ...\n",
            "Unpacking python3.8-minimal (3.8.20-1+jammy1) ...\n",
            "Selecting previously unselected package libpython3.8-stdlib:amd64.\n",
            "Preparing to unpack .../02-libpython3.8-stdlib_3.8.20-1+jammy1_amd64.deb ...\n",
            "Unpacking libpython3.8-stdlib:amd64 (3.8.20-1+jammy1) ...\n",
            "Preparing to unpack .../03-python3-pkg-resources_68.1.2-2~jammy3_all.deb ...\n",
            "Unpacking python3-pkg-resources (68.1.2-2~jammy3) over (59.6.0-1.2ubuntu0.22.04.3) ...\n",
            "Selecting previously unselected package python3-setuptools.\n",
            "Preparing to unpack .../04-python3-setuptools_68.1.2-2~jammy3_all.deb ...\n",
            "Unpacking python3-setuptools (68.1.2-2~jammy3) ...\n",
            "Selecting previously unselected package python3-wheel.\n",
            "Preparing to unpack .../05-python3-wheel_0.37.1-2ubuntu0.22.04.1_all.deb ...\n",
            "Unpacking python3-wheel (0.37.1-2ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package python3-pip.\n",
            "Preparing to unpack .../06-python3-pip_22.0.2+dfsg-1ubuntu0.7_all.deb ...\n",
            "Unpacking python3-pip (22.0.2+dfsg-1ubuntu0.7) ...\n",
            "Selecting previously unselected package python3.8.\n",
            "Preparing to unpack .../07-python3.8_3.8.20-1+jammy1_amd64.deb ...\n",
            "Unpacking python3.8 (3.8.20-1+jammy1) ...\n",
            "Selecting previously unselected package python3.8-lib2to3.\n",
            "Preparing to unpack .../08-python3.8-lib2to3_3.8.20-1+jammy1_all.deb ...\n",
            "Unpacking python3.8-lib2to3 (3.8.20-1+jammy1) ...\n",
            "Selecting previously unselected package python3.8-distutils.\n",
            "Preparing to unpack .../09-python3.8-distutils_3.8.20-1+jammy1_all.deb ...\n",
            "Unpacking python3.8-distutils (3.8.20-1+jammy1) ...\n",
            "Selecting previously unselected package python3.8-venv.\n",
            "Preparing to unpack .../10-python3.8-venv_3.8.20-1+jammy1_amd64.deb ...\n",
            "Unpacking python3.8-venv (3.8.20-1+jammy1) ...\n",
            "Setting up python3-pkg-resources (68.1.2-2~jammy3) ...\n",
            "Setting up libpython3.8-minimal:amd64 (3.8.20-1+jammy1) ...\n",
            "Setting up python3-setuptools (68.1.2-2~jammy3) ...\n",
            "Setting up python3-wheel (0.37.1-2ubuntu0.22.04.1) ...\n",
            "Setting up python3.8-lib2to3 (3.8.20-1+jammy1) ...\n",
            "Setting up python3-pip (22.0.2+dfsg-1ubuntu0.7) ...\n",
            "Setting up python3.8-minimal (3.8.20-1+jammy1) ...\n",
            "Setting up python3.8-distutils (3.8.20-1+jammy1) ...\n",
            "Setting up libpython3.8-stdlib:amd64 (3.8.20-1+jammy1) ...\n",
            "Setting up python3.8 (3.8.20-1+jammy1) ...\n",
            "Setting up python3.8-venv (3.8.20-1+jammy1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n",
            "update-alternatives: using /usr/bin/python3.8 to provide /usr/bin/python3 (python3) in auto mode\n",
            "Python 3.8.20\n"
          ]
        }
      ],
      "source": [
        "# set up python 3.8\n",
        "!sudo apt-get update -y\n",
        "!sudo apt-get install -y python3.8 python3.8-venv python3.8-distutils python3-pip\n",
        "!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.8 10\n",
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get OpeNTF\n",
        "!rm -R opentf/\n",
        "!git clone https://github.com/Fani-Lab/opentf\n",
        "!pip install --upgrade pip setuptools\n",
        "!pip install -r opentf/requirements.txt"
      ],
      "metadata": {
        "id": "95eDgj3AAInL",
        "outputId": "14b27a9c-63df-4e98-a1d0-0879b2ae8e48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'opentf/': No such file or directory\n",
            "Cloning into 'opentf'...\n",
            "remote: Enumerating objects: 22706, done.\u001b[K\n",
            "remote: Counting objects: 100% (123/123), done.\u001b[K\n",
            "remote: Compressing objects: 100% (96/96), done.\u001b[K\n",
            "remote: Total 22706 (delta 56), reused 50 (delta 26), pack-reused 22583 (from 3)\u001b[K\n",
            "Receiving objects: 100% (22706/22706), 1004.21 MiB | 24.56 MiB/s, done.\n",
            "Resolving deltas: 100% (11259/11259), done.\n",
            "Updating files: 100% (1595/1595), done.\n",
            "Requirement already satisfied: pip in /usr/lib/python3/dist-packages (22.0.2)\n",
            "Collecting pip\n",
            "  Downloading pip-25.0.1-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/lib/python3/dist-packages (68.1.2)\n",
            "Collecting setuptools\n",
            "  Downloading setuptools-75.3.2-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: setuptools, pip\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 68.1.2\n",
            "    Not uninstalling setuptools at /usr/lib/python3/dist-packages, outside environment /usr\n",
            "    Can't uninstall 'setuptools'. No files were found to uninstall.\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 22.0.2\n",
            "    Not uninstalling pip at /usr/lib/python3/dist-packages, outside environment /usr\n",
            "    Can't uninstall 'pip'. No files were found to uninstall.\n",
            "Successfully installed pip-25.0.1 setuptools-75.3.2\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "_distutils_hack"
                ]
              },
              "id": "5d6b6ecd1b78458d8009a7dcbea6f32d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting hydra-core==1.3.2 (from -r opentf/requirements.txt (line 3))\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting scipy==1.10.1 (from -r opentf/requirements.txt (line 4))\n",
            "  Downloading scipy-1.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
            "Collecting numpy==1.24.4 (from -r opentf/requirements.txt (line 5))\n",
            "  Downloading numpy-1.24.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Collecting omegaconf<2.4,>=2.2 (from hydra-core==1.3.2->-r opentf/requirements.txt (line 3))\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from hydra-core==1.3.2->-r opentf/requirements.txt (line 3))\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting packaging (from hydra-core==1.3.2->-r opentf/requirements.txt (line 3))\n",
            "  Downloading packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting importlib-resources (from hydra-core==1.3.2->-r opentf/requirements.txt (line 3))\n",
            "  Downloading importlib_resources-6.4.5-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting PyYAML>=5.1.0 (from omegaconf<2.4,>=2.2->hydra-core==1.3.2->-r opentf/requirements.txt (line 3))\n",
            "  Downloading PyYAML-6.0.3-cp38-cp38-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting zipp>=3.1.0 (from importlib-resources->hydra-core==1.3.2->-r opentf/requirements.txt (line 3))\n",
            "  Downloading zipp-3.20.2-py3-none-any.whl.metadata (3.7 kB)\n",
            "Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "Downloading scipy-1.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.5/34.5 MB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.24.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m134.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "Downloading importlib_resources-6.4.5-py3-none-any.whl (36 kB)\n",
            "Downloading packaging-25.0-py3-none-any.whl (66 kB)\n",
            "Downloading PyYAML-6.0.3-cp38-cp38-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (806 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m806.0/806.0 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading zipp-3.20.2-py3-none-any.whl (9.2 kB)\n",
            "Building wheels for collected packages: antlr4-python3-runtime\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144552 sha256=3bcc1d21796b586e22a3644f07c6bb17c97cf7f4d4cfe62d7dff82266c7cc28b\n",
            "  Stored in directory: /root/.cache/pip/wheels/b1/a3/c2/6df046c09459b73cc9bb6c4401b0be6c47048baf9a1617c485\n",
            "Successfully built antlr4-python3-runtime\n",
            "Installing collected packages: antlr4-python3-runtime, zipp, PyYAML, packaging, numpy, scipy, omegaconf, importlib-resources, hydra-core\n",
            "  Attempting uninstall: zipp\n",
            "    Found existing installation: zipp 1.0.0\n",
            "    Uninstalling zipp-1.0.0:\n",
            "      Successfully uninstalled zipp-1.0.0\n",
            "Successfully installed PyYAML-6.0.3 antlr4-python3-runtime-4.9.3 hydra-core-1.3.2 importlib-resources-6.4.5 numpy-1.24.4 omegaconf-2.3.0 packaging-25.0 scipy-1.10.1 zipp-3.20.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "packaging",
                  "pydevd_plugins"
                ]
              },
              "id": "cc69fd6a138f40998fece5635e8d1583"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Graph Neural Team Recommendation\n",
        "\n",
        "- Preprocessing raw data `cmd=prep` into `teamsvecs` sparse matrix each row of which is a team with its skills `teamsvecs['skill']`, members `teamsvecs['member']` as 1-hot vectors\n",
        "- `data.domain`, `data.source`, `data.output` from `toy.dblp` dataset, available at the codebase [`OpeNTF/data/dblp`](https://github.com/fani-lab/OpeNTF/tree/main/data/dblp)\n",
        "- No filtering for min team size `data.filter.min_nteam` and min number of teams per experts `data.filter.min_team_size`\n",
        "- End-to-End GraphSAGE (`mdl.emb.gnn.Gnn_gs`) for team recommendation for `train`, `test`, and `eval` steps on the `skill-team-expert-location` graph structure.  \n"
      ],
      "metadata": {
        "id": "F0R0OLHXASxY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd opentf/src/\n",
        "!python main.py \"cmd=[prep,train,test,eval]\" \"models.instances=[mdl.emb.gnn.Gnn]\" data.domain=cmn.publication.Publication data.source=../data/dblp/toy.dblp.v12.json data.output=../output/dblp/toy.dblp.v12.json ~data.filter data.embedding.class_method=mdl.emb.gnn.Gnn_gs \"+data.embedding.model.gnn.graph.structure=[[[skill, to, team], [member, to, team], [loc, to, team]], stml]\"\n"
      ],
      "metadata": {
        "id": "fDdbvZnWATUw",
        "outputId": "fcbd4a0c-3186-41b9-fdf2-8fd13155a3d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/opentf/src\n",
            "[2025-11-06 21:21:38,170][cmn.team][INFO] - Loading teamsvecs matrices from ../output/dblp/toy.dblp.v12.json/teamsvecs.pkl ...\n",
            "[2025-11-06 21:21:38,171][pkgmgr][INFO] - tqdm not found.\n",
            "[2025-11-06 21:21:38,171][pkgmgr][INFO] - Installing tqdm...\n",
            "[2025-11-06 21:21:39,239][pkgmgr][INFO] - Collecting tqdm==4.65.0\n",
            "  Downloading tqdm-4.65.0-py3-none-any.whl.metadata (56 kB)\n",
            "Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
            "Installing collected packages: tqdm\n",
            "Successfully installed tqdm-4.65.0\n",
            "\n",
            "[2025-11-06 21:21:39,244][cmn.team][INFO] - Loading indexes pickle from ../output/dblp/toy.dblp.v12.json/indexes.pkl ...\n",
            "[2025-11-06 21:21:39,244][cmn.team][INFO] - Indexes pickle is loaded.\n",
            "[2025-11-06 21:21:39,244][cmn.team][INFO] - Teamsvecs matrices and indexes for skills (31, 10), members (31, 13), and locations (31, 29) are loaded.\n",
            "[2025-11-06 21:21:39,245][__main__][INFO] - Loading splits from ../output/dblp/toy.dblp.v12.json/splits.f3.r0.85.pkl ...\n",
            "[2025-11-06 21:21:39,245][cmn.team][INFO] - Loading member-skill co-occurrence matrix (13, 10) from ../output/dblp/toy.dblp.v12.json/skillcoverage.pkl ...\n",
            "[2025-11-06 21:21:39,332][pkgmgr][INFO] - torch not found.\n",
            "[2025-11-06 21:21:39,332][pkgmgr][INFO] - Installing torch...\n",
            "[2025-11-06 21:24:10,469][pkgmgr][INFO] - Collecting torch==2.4.1\n",
            "  Downloading torch-2.4.1-cp38-cp38-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Collecting filelock (from torch==2.4.1)\n",
            "  Downloading filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting typing-extensions>=4.8.0 (from torch==2.4.1)\n",
            "  Downloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting sympy (from torch==2.4.1)\n",
            "  Downloading sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting networkx (from torch==2.4.1)\n",
            "  Downloading networkx-3.1-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting jinja2 (from torch==2.4.1)\n",
            "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting fsspec (from torch==2.4.1)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.4.1)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.4.1)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.4.1)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.4.1)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.4.1)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.4.1)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.4.1)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.4.1)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.4.1)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.4.1)\n",
            "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.4.1)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==3.0.0 (from torch==2.4.1)\n",
            "  Downloading triton-3.0.0-1-cp38-cp38-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.1)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/lib/python3/dist-packages (from jinja2->torch==2.4.1) (2.0.1)\n",
            "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch==2.4.1)\n",
            "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Downloading torch-2.4.1-cp38-cp38-manylinux1_x86_64.whl (797.1 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 797.1/797.1 MB 33.2 MB/s eta 0:00:00\n",
            "Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 410.6/410.6 MB 43.3 MB/s eta 0:00:00\n",
            "Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.1/14.1 MB 126.9 MB/s eta 0:00:00\n",
            "Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 23.7/23.7 MB 167.8 MB/s eta 0:00:00\n",
            "Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 823.6/823.6 kB 34.5 MB/s eta 0:00:00\n",
            "Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 664.8/664.8 MB 17.1 MB/s eta 0:00:00\n",
            "Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 121.6/121.6 MB 67.0 MB/s eta 0:00:00\n",
            "Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.5/56.5 MB 63.9 MB/s eta 0:00:00\n",
            "Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 124.2/124.2 MB 58.4 MB/s eta 0:00:00\n",
            "Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 196.0/196.0 MB 68.7 MB/s eta 0:00:00\n",
            "Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 176.2/176.2 MB 62.1 MB/s eta 0:00:00\n",
            "Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Downloading triton-3.0.0-1-cp38-cp38-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 209.4/209.4 MB 63.9 MB/s eta 0:00:00\n",
            "Downloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
            "Downloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
            "Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
            "Downloading networkx-3.1-py3-none-any.whl (2.1 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.1/2.1 MB 47.6 MB/s eta 0:00:00\n",
            "Downloading sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.2/6.2 MB 117.2 MB/s eta 0:00:00\n",
            "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 536.2/536.2 kB 17.1 MB/s eta 0:00:00\n",
            "Downloading nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.7 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 39.7/39.7 MB 45.2 MB/s eta 0:00:00\n",
            "Installing collected packages: mpmath, typing-extensions, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, jinja2, fsspec, filelock, triton, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
            "Successfully installed filelock-3.16.1 fsspec-2025.3.0 jinja2-3.1.6 mpmath-1.3.0 networkx-3.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.9.86 nvidia-nvtx-cu12-12.1.105 sympy-1.13.3 torch-2.4.1 triton-3.0.0 typing-extensions-4.13.2\n",
            "\n",
            "[2025-11-06 21:24:13,568][pkgmgr][INFO] - torch_geometric not found.\n",
            "[2025-11-06 21:24:13,568][pkgmgr][INFO] - Installing torch_geometric...\n",
            "[2025-11-06 21:24:29,305][pkgmgr][INFO] - Looking in links: https://data.pyg.org/whl/torch-2.4.1+cpu.html\n",
            "Collecting torch_geometric==2.6.1\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "Collecting torch_cluster==1.6.3\n",
            "  Downloading https://data.pyg.org/whl/torch-2.4.0%2Bcpu/torch_cluster-1.6.3%2Bpt24cpu-cp38-cp38-linux_x86_64.whl (788 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 788.2/788.2 kB 10.1 MB/s eta 0:00:00\n",
            "Collecting torch_sparse==0.6.18\n",
            "  Downloading https://data.pyg.org/whl/torch-2.4.0%2Bcpu/torch_sparse-0.6.18%2Bpt24cpu-cp38-cp38-linux_x86_64.whl (1.2 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 6.3 MB/s eta 0:00:00\n",
            "Collecting torch_scatter==2.1.2\n",
            "  Downloading https://data.pyg.org/whl/torch-2.4.0%2Bcpu/torch_scatter-2.1.2%2Bpt24cpu-cp38-cp38-linux_x86_64.whl (541 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 541.3/541.3 kB 3.6 MB/s eta 0:00:00\n",
            "Collecting aiohttp (from torch_geometric==2.6.1)\n",
            "  Downloading aiohttp-3.10.11-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.8/dist-packages (from torch_geometric==2.6.1) (2025.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from torch_geometric==2.6.1) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torch_geometric==2.6.1) (1.24.4)\n",
            "Collecting psutil>=5.8.0 (from torch_geometric==2.6.1)\n",
            "  Downloading psutil-7.1.3-cp36-abi3-manylinux2010_x86_64.manylinux_2_12_x86_64.manylinux_2_28_x86_64.whl.metadata (23 kB)\n",
            "Requirement already satisfied: pyparsing in /usr/lib/python3/dist-packages (from torch_geometric==2.6.1) (2.4.7)\n",
            "Collecting requests (from torch_geometric==2.6.1)\n",
            "  Downloading requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from torch_geometric==2.6.1) (4.65.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from torch_cluster==1.6.3) (1.10.1)\n",
            "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->torch_geometric==2.6.1)\n",
            "  Downloading aiohappyeyeballs-2.4.4-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting aiosignal>=1.1.2 (from aiohttp->torch_geometric==2.6.1)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting attrs>=17.3.0 (from aiohttp->torch_geometric==2.6.1)\n",
            "  Downloading attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp->torch_geometric==2.6.1)\n",
            "  Downloading frozenlist-1.5.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp->torch_geometric==2.6.1)\n",
            "  Downloading multidict-6.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
            "Collecting yarl<2.0,>=1.12.0 (from aiohttp->torch_geometric==2.6.1)\n",
            "  Downloading yarl-1.15.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (56 kB)\n",
            "Collecting async-timeout<6.0,>=4.0 (from aiohttp->torch_geometric==2.6.1)\n",
            "  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/lib/python3/dist-packages (from jinja2->torch_geometric==2.6.1) (2.0.1)\n",
            "Collecting charset_normalizer<4,>=2 (from requests->torch_geometric==2.6.1)\n",
            "  Downloading charset_normalizer-3.4.4-cp38-cp38-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (37 kB)\n",
            "Collecting idna<4,>=2.5 (from requests->torch_geometric==2.6.1)\n",
            "  Downloading idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests->torch_geometric==2.6.1)\n",
            "  Downloading urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests->torch_geometric==2.6.1)\n",
            "  Downloading certifi-2025.10.5-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.8/dist-packages (from multidict<7.0,>=4.5->aiohttp->torch_geometric==2.6.1) (4.13.2)\n",
            "Collecting propcache>=0.2.0 (from yarl<2.0,>=1.12.0->aiohttp->torch_geometric==2.6.1)\n",
            "  Downloading propcache-0.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 19.4 MB/s eta 0:00:00\n",
            "Downloading psutil-7.1.3-cp36-abi3-manylinux2010_x86_64.manylinux_2_12_x86_64.manylinux_2_28_x86_64.whl (263 kB)\n",
            "Downloading aiohttp-3.10.11-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 49.4 MB/s eta 0:00:00\n",
            "Downloading requests-2.32.4-py3-none-any.whl (64 kB)\n",
            "Downloading aiohappyeyeballs-2.4.4-py3-none-any.whl (14 kB)\n",
            "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
            "Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n",
            "Downloading certifi-2025.10.5-py3-none-any.whl (163 kB)\n",
            "Downloading charset_normalizer-3.4.4-cp38-cp38-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (147 kB)\n",
            "Downloading frozenlist-1.5.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (243 kB)\n",
            "Downloading idna-3.11-py3-none-any.whl (71 kB)\n",
            "Downloading multidict-6.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "Downloading urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
            "Downloading yarl-1.15.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (319 kB)\n",
            "Downloading propcache-0.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
            "Installing collected packages: urllib3, torch_scatter, psutil, propcache, multidict, idna, frozenlist, charset_normalizer, certifi, attrs, async-timeout, aiohappyeyeballs, yarl, torch_sparse, torch_cluster, requests, aiosignal, aiohttp, torch_geometric\n",
            "Successfully installed aiohappyeyeballs-2.4.4 aiohttp-3.10.11 aiosignal-1.3.1 async-timeout-5.0.1 attrs-25.3.0 certifi-2025.10.5 charset_normalizer-3.4.4 frozenlist-1.5.0 idna-3.11 multidict-6.1.0 propcache-0.2.0 psutil-7.1.3 requests-2.32.4 torch_cluster-1.6.3+pt24cpu torch_geometric-2.6.1 torch_scatter-2.1.2+pt24cpu torch_sparse-0.6.18+pt24cpu urllib3-2.2.3 yarl-1.15.2\n",
            "\n",
            "[2025-11-06 21:24:32,030][pkgmgr][INFO] - tensorboardX not found.\n",
            "[2025-11-06 21:24:32,030][pkgmgr][INFO] - Installing tensorboardX...\n",
            "[2025-11-06 21:24:41,275][pkgmgr][INFO] - Collecting tensorboardX==2.6.2.2\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting tensorboard==2.14.0\n",
            "  Downloading tensorboard-2.14.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting protobuf==3.20\n",
            "  Downloading protobuf-3.20.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (698 bytes)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from tensorboardX==2.6.2.2) (1.24.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorboardX==2.6.2.2) (25.0)\n",
            "Collecting absl-py>=0.4 (from tensorboard==2.14.0)\n",
            "  Downloading absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting grpcio>=1.48.2 (from tensorboard==2.14.0)\n",
            "  Downloading grpcio-1.70.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
            "Collecting google-auth<3,>=1.6.3 (from tensorboard==2.14.0)\n",
            "  Downloading google_auth-2.43.0-py2.py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard==2.14.0)\n",
            "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/lib/python3/dist-packages (from tensorboard==2.14.0) (3.3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.14.0) (2.32.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.14.0) (75.3.2)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard==2.14.0)\n",
            "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting werkzeug>=1.0.1 (from tensorboard==2.14.0)\n",
            "  Downloading werkzeug-3.0.6-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/lib/python3/dist-packages (from tensorboard==2.14.0) (0.37.1)\n",
            "Collecting cachetools<7.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard==2.14.0)\n",
            "  Downloading cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard==2.14.0)\n",
            "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard==2.14.0)\n",
            "  Downloading rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<1.1,>=0.5->tensorboard==2.14.0)\n",
            "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/lib/python3/dist-packages (from markdown>=2.6.8->tensorboard==2.14.0) (4.6.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard==2.14.0) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard==2.14.0) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard==2.14.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard==2.14.0) (2025.10.5)\n",
            "Collecting MarkupSafe>=2.1.1 (from werkzeug>=1.0.1->tensorboard==2.14.0)\n",
            "  Downloading MarkupSafe-2.1.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Collecting pyasn1<0.7.0,>=0.6.1 (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard==2.14.0)\n",
            "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard==2.14.0) (3.2.0)\n",
            "Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "Downloading tensorboard-2.14.0-py3-none-any.whl (5.5 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.5/5.5 MB 66.6 MB/s eta 0:00:00\n",
            "Downloading protobuf-3.20.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 39.9 MB/s eta 0:00:00\n",
            "Downloading absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
            "Downloading google_auth-2.43.0-py2.py3-none-any.whl (223 kB)\n",
            "Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
            "Downloading grpcio-1.70.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.0/6.0 MB 124.1 MB/s eta 0:00:00\n",
            "Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.6/6.6 MB 126.3 MB/s eta 0:00:00\n",
            "Downloading werkzeug-3.0.6-py3-none-any.whl (227 kB)\n",
            "Downloading cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
            "Downloading MarkupSafe-2.1.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26 kB)\n",
            "Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
            "Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
            "Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
            "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
            "Installing collected packages: tensorboard-data-server, pyasn1, protobuf, MarkupSafe, grpcio, cachetools, absl-py, werkzeug, tensorboardX, rsa, requests-oauthlib, pyasn1-modules, google-auth, google-auth-oauthlib, tensorboard\n",
            "  Attempting uninstall: MarkupSafe\n",
            "    Found existing installation: MarkupSafe 2.0.1\n",
            "    Uninstalling MarkupSafe-2.0.1:\n",
            "      Successfully uninstalled MarkupSafe-2.0.1\n",
            "Successfully installed MarkupSafe-2.1.5 absl-py-2.3.1 cachetools-5.5.2 google-auth-2.43.0 google-auth-oauthlib-1.0.0 grpcio-1.70.0 protobuf-3.20.0 pyasn1-0.6.1 pyasn1-modules-0.4.2 requests-oauthlib-2.0.0 rsa-4.9.1 tensorboard-2.14.0 tensorboard-data-server-0.7.2 tensorboardX-2.6.2.2 werkzeug-3.0.6\n",
            "\n",
            "[2025-11-06 21:24:41,314][mdl.emb.gnn][INFO] - Loading graph of ([['skill', 'to', 'team'], ['member', 'to', 'team'], ['loc', 'to', 'team']], 'stml') from ../output/dblp/toy.dblp.v12.json/splits.f3.r0.85/../stml.add.graph.pkl  ...\n",
            "[2025-11-06 21:24:41,321][mdl.emb.gnn][INFO] - \u001b[94mTraining gs \u001b[0m... \n",
            "[2025-11-06 21:24:41,381][mdl.emb.gnn][INFO] - Loading the model ../output/dblp/toy.dblp.v12.json/splits.f3.r0.85/gs.b1000.e100.ns5.lr0.001.es5.spe10.d128.add.stml.h128.nn30-20/f0.pt ...\n",
            "/content/opentf/src/mdl/emb/gnn.py:230: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  return self.model.load_state_dict(Gnn.torch.load(f'{self.output}/f{foldidx}.pt', map_location=self.device)['model_state_dict'])\n",
            "[2025-11-06 21:24:41,381][mdl.emb.gnn][INFO] - \u001b[93mFile not found! Training ...\u001b[0m\n",
            "/usr/local/lib/python3.8/dist-packages/torch_geometric/sampler/neighbor_sampler.py:61: UserWarning: Using 'NeighborSampler' without a 'pyg-lib' installation is deprecated and will be removed soon. Please install 'pyg-lib' for accelerated neighborhood sampling\n",
            "  warnings.warn(f\"Using '{self.__class__.__name__}' without a \"\n",
            "[2025-11-06 21:24:41,406][mdl.emb.gnn][INFO] - Fold 0/2, Epoch 0, \u001b[94mTrain Loss: 0.7920\u001b[0m\n",
            "[2025-11-06 21:24:41,411][mdl.emb.gnn][INFO] - Fold 0/2, Epoch 0, \u001b[95mValid Loss: 0.5722\u001b[0m\n",
            "[2025-11-06 21:24:41,420][mdl.emb.gnn][INFO] - gs model with d128.e100.b1000.lr0.001.es5.ns5.spe10.h[128].nn[30, 20].supervision_edge_types[['member', 'to', 'team'], ['team', 'rev_to', 'member']] saved at ../output/dblp/toy.dblp.v12.json/splits.f3.r0.85/gs.b1000.e100.ns5.lr0.001.es5.spe10.d128.add.stml.h128.nn30-20.e0.pt\n",
            "[2025-11-06 21:24:41,420][mdl.emb.gnn][INFO] - Validation loss decreased (inf --> 0.572195)\n",
            "[2025-11-06 21:24:41,427][mdl.emb.gnn][INFO] - Fold 0/2, Epoch 1, \u001b[94mTrain Loss: 0.7821\u001b[0m\n",
            "[2025-11-06 21:24:41,432][mdl.emb.gnn][INFO] - Fold 0/2, Epoch 1, \u001b[95mValid Loss: 0.5808\u001b[0m\n",
            "[2025-11-06 21:24:41,432][mdl.emb.gnn][INFO] - EarlyStopping counter: 1 out of 5\n",
            "[2025-11-06 21:24:41,438][mdl.emb.gnn][INFO] - Fold 0/2, Epoch 2, \u001b[94mTrain Loss: 0.7699\u001b[0m\n",
            "[2025-11-06 21:24:41,441][mdl.emb.gnn][INFO] - Fold 0/2, Epoch 2, \u001b[95mValid Loss: 0.5890\u001b[0m\n",
            "[2025-11-06 21:24:41,442][mdl.emb.gnn][INFO] - EarlyStopping counter: 2 out of 5\n",
            "[2025-11-06 21:24:41,449][mdl.emb.gnn][INFO] - Fold 0/2, Epoch 3, \u001b[94mTrain Loss: 0.7601\u001b[0m\n",
            "[2025-11-06 21:24:41,454][mdl.emb.gnn][INFO] - Fold 0/2, Epoch 3, \u001b[95mValid Loss: 0.5970\u001b[0m\n",
            "[2025-11-06 21:24:41,455][mdl.emb.gnn][INFO] - EarlyStopping counter: 3 out of 5\n",
            "[2025-11-06 21:24:41,460][mdl.emb.gnn][INFO] - Fold 0/2, Epoch 4, \u001b[94mTrain Loss: 0.7524\u001b[0m\n",
            "[2025-11-06 21:24:41,465][mdl.emb.gnn][INFO] - Fold 0/2, Epoch 4, \u001b[95mValid Loss: 0.6047\u001b[0m\n",
            "[2025-11-06 21:24:41,465][mdl.emb.gnn][INFO] - EarlyStopping counter: 4 out of 5\n",
            "[2025-11-06 21:24:41,471][mdl.emb.gnn][INFO] - Fold 0/2, Epoch 5, \u001b[94mTrain Loss: 0.7458\u001b[0m\n",
            "[2025-11-06 21:24:41,475][mdl.emb.gnn][INFO] - Fold 0/2, Epoch 5, \u001b[95mValid Loss: 0.6121\u001b[0m\n",
            "[2025-11-06 21:24:41,475][mdl.emb.gnn][INFO] - EarlyStopping counter: 5 out of 5\n",
            "[2025-11-06 21:24:41,475][mdl.emb.gnn][INFO] - Early stopping triggered at epoch: 5\n",
            "[2025-11-06 21:24:41,483][mdl.emb.gnn][INFO] - gs model with d128.e100.b1000.lr0.001.es5.ns5.spe10.h[128].nn[30, 20].supervision_edge_types[['member', 'to', 'team'], ['team', 'rev_to', 'member']] saved at ../output/dblp/toy.dblp.v12.json/splits.f3.r0.85/gs.b1000.e100.ns5.lr0.001.es5.spe10.d128.add.stml.h128.nn30-20.pt.\n",
            "[2025-11-06 21:24:41,491][mdl.emb.gnn][INFO] - Loading the model ../output/dblp/toy.dblp.v12.json/splits.f3.r0.85/gs.b1000.e100.ns5.lr0.001.es5.spe10.d128.add.stml.h128.nn30-20/f1.pt ...\n",
            "[2025-11-06 21:24:41,491][mdl.emb.gnn][INFO] - \u001b[93mFile not found! Training ...\u001b[0m\n",
            "[2025-11-06 21:24:41,500][mdl.emb.gnn][INFO] - Fold 1/2, Epoch 0, \u001b[94mTrain Loss: 0.7986\u001b[0m\n",
            "[2025-11-06 21:24:41,506][mdl.emb.gnn][INFO] - Fold 1/2, Epoch 0, \u001b[95mValid Loss: 0.5872\u001b[0m\n",
            "[2025-11-06 21:24:41,514][mdl.emb.gnn][INFO] - gs model with d128.e100.b1000.lr0.001.es5.ns5.spe10.h[128].nn[30, 20].supervision_edge_types[['member', 'to', 'team'], ['team', 'rev_to', 'member']] saved at ../output/dblp/toy.dblp.v12.json/splits.f3.r0.85/gs.b1000.e100.ns5.lr0.001.es5.spe10.d128.add.stml.h128.nn30-20.e0.pt\n",
            "[2025-11-06 21:24:41,514][mdl.emb.gnn][INFO] - Validation loss decreased (inf --> 0.587206)\n",
            "[2025-11-06 21:24:41,521][mdl.emb.gnn][INFO] - Fold 1/2, Epoch 1, \u001b[94mTrain Loss: 0.7854\u001b[0m\n",
            "[2025-11-06 21:24:41,527][mdl.emb.gnn][INFO] - Fold 1/2, Epoch 1, \u001b[95mValid Loss: 0.5942\u001b[0m\n",
            "[2025-11-06 21:24:41,528][mdl.emb.gnn][INFO] - EarlyStopping counter: 1 out of 5\n",
            "[2025-11-06 21:24:41,534][mdl.emb.gnn][INFO] - Fold 1/2, Epoch 2, \u001b[94mTrain Loss: 0.7756\u001b[0m\n",
            "[2025-11-06 21:24:41,538][mdl.emb.gnn][INFO] - Fold 1/2, Epoch 2, \u001b[95mValid Loss: 0.6008\u001b[0m\n",
            "[2025-11-06 21:24:41,538][mdl.emb.gnn][INFO] - EarlyStopping counter: 2 out of 5\n",
            "[2025-11-06 21:24:41,544][mdl.emb.gnn][INFO] - Fold 1/2, Epoch 3, \u001b[94mTrain Loss: 0.7658\u001b[0m\n",
            "[2025-11-06 21:24:41,548][mdl.emb.gnn][INFO] - Fold 1/2, Epoch 3, \u001b[95mValid Loss: 0.6071\u001b[0m\n",
            "[2025-11-06 21:24:41,548][mdl.emb.gnn][INFO] - EarlyStopping counter: 3 out of 5\n",
            "[2025-11-06 21:24:41,554][mdl.emb.gnn][INFO] - Fold 1/2, Epoch 4, \u001b[94mTrain Loss: 0.7546\u001b[0m\n",
            "[2025-11-06 21:24:41,558][mdl.emb.gnn][INFO] - Fold 1/2, Epoch 4, \u001b[95mValid Loss: 0.6130\u001b[0m\n",
            "[2025-11-06 21:24:41,558][mdl.emb.gnn][INFO] - EarlyStopping counter: 4 out of 5\n",
            "[2025-11-06 21:24:41,564][mdl.emb.gnn][INFO] - Fold 1/2, Epoch 5, \u001b[94mTrain Loss: 0.7469\u001b[0m\n",
            "[2025-11-06 21:24:41,567][mdl.emb.gnn][INFO] - Fold 1/2, Epoch 5, \u001b[95mValid Loss: 0.6187\u001b[0m\n",
            "[2025-11-06 21:24:41,567][mdl.emb.gnn][INFO] - EarlyStopping counter: 5 out of 5\n",
            "[2025-11-06 21:24:41,568][mdl.emb.gnn][INFO] - Early stopping triggered at epoch: 5\n",
            "[2025-11-06 21:24:41,575][mdl.emb.gnn][INFO] - gs model with d128.e100.b1000.lr0.001.es5.ns5.spe10.h[128].nn[30, 20].supervision_edge_types[['member', 'to', 'team'], ['team', 'rev_to', 'member']] saved at ../output/dblp/toy.dblp.v12.json/splits.f3.r0.85/gs.b1000.e100.ns5.lr0.001.es5.spe10.d128.add.stml.h128.nn30-20.pt.\n",
            "[2025-11-06 21:24:41,583][mdl.emb.gnn][INFO] - Loading the model ../output/dblp/toy.dblp.v12.json/splits.f3.r0.85/gs.b1000.e100.ns5.lr0.001.es5.spe10.d128.add.stml.h128.nn30-20/f2.pt ...\n",
            "[2025-11-06 21:24:41,583][mdl.emb.gnn][INFO] - \u001b[93mFile not found! Training ...\u001b[0m\n",
            "[2025-11-06 21:24:41,593][mdl.emb.gnn][INFO] - Fold 2/2, Epoch 0, \u001b[94mTrain Loss: 0.7978\u001b[0m\n",
            "[2025-11-06 21:24:41,597][mdl.emb.gnn][INFO] - Fold 2/2, Epoch 0, \u001b[95mValid Loss: 0.5905\u001b[0m\n",
            "[2025-11-06 21:24:41,604][mdl.emb.gnn][INFO] - gs model with d128.e100.b1000.lr0.001.es5.ns5.spe10.h[128].nn[30, 20].supervision_edge_types[['member', 'to', 'team'], ['team', 'rev_to', 'member']] saved at ../output/dblp/toy.dblp.v12.json/splits.f3.r0.85/gs.b1000.e100.ns5.lr0.001.es5.spe10.d128.add.stml.h128.nn30-20.e0.pt\n",
            "[2025-11-06 21:24:41,605][mdl.emb.gnn][INFO] - Validation loss decreased (inf --> 0.590475)\n",
            "[2025-11-06 21:24:41,610][mdl.emb.gnn][INFO] - Fold 2/2, Epoch 1, \u001b[94mTrain Loss: 0.7885\u001b[0m\n",
            "[2025-11-06 21:24:41,614][mdl.emb.gnn][INFO] - Fold 2/2, Epoch 1, \u001b[95mValid Loss: 0.5987\u001b[0m\n",
            "[2025-11-06 21:24:41,614][mdl.emb.gnn][INFO] - EarlyStopping counter: 1 out of 5\n",
            "[2025-11-06 21:24:41,620][mdl.emb.gnn][INFO] - Fold 2/2, Epoch 2, \u001b[94mTrain Loss: 0.7764\u001b[0m\n",
            "[2025-11-06 21:24:41,624][mdl.emb.gnn][INFO] - Fold 2/2, Epoch 2, \u001b[95mValid Loss: 0.6062\u001b[0m\n",
            "[2025-11-06 21:24:41,624][mdl.emb.gnn][INFO] - EarlyStopping counter: 2 out of 5\n",
            "[2025-11-06 21:24:41,630][mdl.emb.gnn][INFO] - Fold 2/2, Epoch 3, \u001b[94mTrain Loss: 0.7666\u001b[0m\n",
            "[2025-11-06 21:24:41,633][mdl.emb.gnn][INFO] - Fold 2/2, Epoch 3, \u001b[95mValid Loss: 0.6132\u001b[0m\n",
            "[2025-11-06 21:24:41,633][mdl.emb.gnn][INFO] - EarlyStopping counter: 3 out of 5\n",
            "[2025-11-06 21:24:41,639][mdl.emb.gnn][INFO] - Fold 2/2, Epoch 4, \u001b[94mTrain Loss: 0.7560\u001b[0m\n",
            "[2025-11-06 21:24:41,642][mdl.emb.gnn][INFO] - Fold 2/2, Epoch 4, \u001b[95mValid Loss: 0.6198\u001b[0m\n",
            "[2025-11-06 21:24:41,642][mdl.emb.gnn][INFO] - EarlyStopping counter: 4 out of 5\n",
            "[2025-11-06 21:24:41,648][mdl.emb.gnn][INFO] - Fold 2/2, Epoch 5, \u001b[94mTrain Loss: 0.7485\u001b[0m\n",
            "[2025-11-06 21:24:41,651][mdl.emb.gnn][INFO] - Fold 2/2, Epoch 5, \u001b[95mValid Loss: 0.6262\u001b[0m\n",
            "[2025-11-06 21:24:41,652][mdl.emb.gnn][INFO] - EarlyStopping counter: 5 out of 5\n",
            "[2025-11-06 21:24:41,652][mdl.emb.gnn][INFO] - Early stopping triggered at epoch: 5\n",
            "[2025-11-06 21:24:41,659][mdl.emb.gnn][INFO] - gs model with d128.e100.b1000.lr0.001.es5.ns5.spe10.h[128].nn[30, 20].supervision_edge_types[['member', 'to', 'team'], ['team', 'rev_to', 'member']] saved at ../output/dblp/toy.dblp.v12.json/splits.f3.r0.85/gs.b1000.e100.ns5.lr0.001.es5.spe10.d128.add.stml.h128.nn30-20.pt.\n",
            "[2025-11-06 21:24:41,714][__main__][INFO] - \u001b[94mTraining team recommender instance mdl.emb.gnn.Gnn ... \u001b[0m\n",
            "[2025-11-06 21:24:41,714][__main__][INFO] - \u001b[93mTraining a Gnn instance is through data.embedding.class_method! \u001b[0m\n",
            "[2025-11-06 21:24:41,714][__main__][INFO] - \u001b[92mTesting team recommender instance mdl.emb.gnn.Gnn ... \u001b[0m\n",
            "[2025-11-06 21:24:41,715][mdl.emb.gnn][INFO] - \u001b[94mTesting gs \u001b[0m... \n",
            "/content/opentf/src/mdl/emb/gnn.py:417: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  self.model.load_state_dict(Gnn.torch.load(modelfile, map_location=self.device)['model_state_dict'])\n",
            "[2025-11-06 21:24:41,728][mdl.emb.gnn][INFO] - gs model predictions for fold0.test. has saved at ../output/dblp/toy.dblp.v12.json/splits.f3.r0.85/gs.b1000.e100.ns5.lr0.001.es5.spe10.d128.add.stml.h128.nn30-20/f0.test.pred\n",
            "[2025-11-06 21:24:41,737][mdl.emb.gnn][INFO] - gs model predictions for fold0.test.e0. has saved at ../output/dblp/toy.dblp.v12.json/splits.f3.r0.85/gs.b1000.e100.ns5.lr0.001.es5.spe10.d128.add.stml.h128.nn30-20/f0.test.e0.pred\n",
            "[2025-11-06 21:24:41,746][mdl.emb.gnn][INFO] - gs model predictions for fold1.test. has saved at ../output/dblp/toy.dblp.v12.json/splits.f3.r0.85/gs.b1000.e100.ns5.lr0.001.es5.spe10.d128.add.stml.h128.nn30-20/f1.test.pred\n",
            "[2025-11-06 21:24:41,756][mdl.emb.gnn][INFO] - gs model predictions for fold1.test.e0. has saved at ../output/dblp/toy.dblp.v12.json/splits.f3.r0.85/gs.b1000.e100.ns5.lr0.001.es5.spe10.d128.add.stml.h128.nn30-20/f1.test.e0.pred\n",
            "[2025-11-06 21:24:41,767][mdl.emb.gnn][INFO] - gs model predictions for fold2.test. has saved at ../output/dblp/toy.dblp.v12.json/splits.f3.r0.85/gs.b1000.e100.ns5.lr0.001.es5.spe10.d128.add.stml.h128.nn30-20/f2.test.pred\n",
            "[2025-11-06 21:24:41,775][mdl.emb.gnn][INFO] - gs model predictions for fold2.test.e0. has saved at ../output/dblp/toy.dblp.v12.json/splits.f3.r0.85/gs.b1000.e100.ns5.lr0.001.es5.spe10.d128.add.stml.h128.nn30-20/f2.test.e0.pred\n",
            "[2025-11-06 21:24:41,775][__main__][INFO] - \u001b[95mEvaluating team recommender instance mdl.emb.gnn.Gnn ... \u001b[0m\n",
            "[2025-11-06 21:24:41,784][pkgmgr][INFO] - pandas not found.\n",
            "[2025-11-06 21:24:41,784][pkgmgr][INFO] - Installing pandas...\n",
            "[2025-11-06 21:24:50,568][pkgmgr][INFO] - Collecting pandas==2.0.0\n",
            "  Downloading pandas-2.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Collecting python-dateutil>=2.8.2 (from pandas==2.0.0)\n",
            "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting pytz>=2020.1 (from pandas==2.0.0)\n",
            "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.1 (from pandas==2.0.0)\n",
            "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.8/dist-packages (from pandas==2.0.0) (1.24.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas==2.0.0) (1.16.0)\n",
            "Downloading pandas-2.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.3/12.3 MB 102.2 MB/s eta 0:00:00\n",
            "Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
            "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
            "Installing collected packages: pytz, tzdata, python-dateutil, pandas\n",
            "Successfully installed pandas-2.0.0 python-dateutil-2.9.0.post0 pytz-2025.2 tzdata-2025.2\n",
            "\n",
            "/content/opentf/src/mdl/ntf.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  Y_ = Ntf.torch.load(predfile, map_location='cpu')['y_pred']\n",
            "[2025-11-06 21:24:50,950][mdl.ntf][INFO] - Evaluating predictions at ../output/dblp/toy.dblp.v12.json/splits.f3.r0.85/gs.b1000.e100.ns5.lr0.001.es5.spe10.d128.add.stml.h128.nn30-20/f0.test.pred ... for {'trec': ['P_2,5,10', 'recall_2,5,10', 'ndcg_cut_2,5,10', 'map_cut_2,5,10', 'success_2,5,10'], 'other': ['skill_coverage_2,5,10', 'aucroc']}\n",
            "[2025-11-06 21:24:50,951][mdl.ntf][INFO] - ['P_2,5,10', 'recall_2,5,10', 'ndcg_cut_2,5,10', 'map_cut_2,5,10', 'success_2,5,10'] ...\n",
            "[2025-11-06 21:24:50,958][pkgmgr][INFO] - pytrec_eval not found.\n",
            "[2025-11-06 21:24:50,958][pkgmgr][INFO] - Installing pytrec-eval-terrier...\n",
            "[2025-11-06 21:24:52,441][pkgmgr][INFO] - Collecting pytrec-eval-terrier==0.5.2\n",
            "  Downloading pytrec_eval_terrier-0.5.2-cp38-cp38-manylinux2010_x86_64.whl.metadata (800 bytes)\n",
            "Downloading pytrec_eval_terrier-0.5.2-cp38-cp38-manylinux2010_x86_64.whl (287 kB)\n",
            "Installing collected packages: pytrec-eval-terrier\n",
            "Successfully installed pytrec-eval-terrier-0.5.2\n",
            "\n",
            "[2025-11-06 21:24:52,443][evl.metric][INFO] - Building pytrec_eval input for 5 instances ...\n",
            "100% 5/5 [00:00<00:00, 879.35it/s]\n",
            "[2025-11-06 21:24:52,466][mdl.ntf][INFO] - ['aucroc'] ...\n",
            "[2025-11-06 21:24:52,467][pkgmgr][INFO] - sklearn.metrics not found.\n",
            "[2025-11-06 21:24:52,467][pkgmgr][INFO] - Installing scikit-learn...\n",
            "[2025-11-06 21:24:56,854][pkgmgr][INFO] - Collecting scikit-learn==1.2.2\n",
            "  Downloading scikit_learn-1.2.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from scikit-learn==1.2.2) (1.24.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.8/dist-packages (from scikit-learn==1.2.2) (1.10.1)\n",
            "Collecting joblib>=1.1.1 (from scikit-learn==1.2.2)\n",
            "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting threadpoolctl>=2.0.0 (from scikit-learn==1.2.2)\n",
            "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
            "Downloading scikit_learn-1.2.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.8 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.8/9.8 MB 93.7 MB/s eta 0:00:00\n",
            "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
            "Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
            "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
            "Successfully installed joblib-1.4.2 scikit-learn-1.2.2 threadpoolctl-3.5.0\n",
            "\n",
            "[2025-11-06 21:24:57,683][mdl.ntf][INFO] - ['skill_coverage_2,5,10'] ...\n",
            "100% 5/5 [00:00<00:00, 139.17it/s]\n",
            "[2025-11-06 21:24:57,735][mdl.ntf][INFO] - Saving file per fold as ../output/dblp/toy.dblp.v12.json/splits.f3.r0.85/gs.b1000.e100.ns5.lr0.001.es5.spe10.d128.add.stml.h128.nn30-20/f0.test.pred.eval.mean.csv\n",
            "/content/opentf/src/mdl/ntf.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  Y_ = Ntf.torch.load(predfile, map_location='cpu')['y_pred']\n",
            "[2025-11-06 21:24:57,739][mdl.ntf][INFO] - Evaluating predictions at ../output/dblp/toy.dblp.v12.json/splits.f3.r0.85/gs.b1000.e100.ns5.lr0.001.es5.spe10.d128.add.stml.h128.nn30-20/f0.test.e0.pred ... for {'trec': ['P_2,5,10', 'recall_2,5,10', 'ndcg_cut_2,5,10', 'map_cut_2,5,10', 'success_2,5,10'], 'other': ['skill_coverage_2,5,10', 'aucroc']}\n",
            "[2025-11-06 21:24:57,741][mdl.ntf][INFO] - ['P_2,5,10', 'recall_2,5,10', 'ndcg_cut_2,5,10', 'map_cut_2,5,10', 'success_2,5,10'] ...\n",
            "[2025-11-06 21:24:57,749][evl.metric][INFO] - Building pytrec_eval input for 5 instances ...\n",
            "100% 5/5 [00:00<00:00, 1943.25it/s]\n",
            "[2025-11-06 21:24:57,755][mdl.ntf][INFO] - ['aucroc'] ...\n",
            "[2025-11-06 21:24:57,759][mdl.ntf][INFO] - ['skill_coverage_2,5,10'] ...\n",
            "100% 5/5 [00:00<00:00, 251.40it/s]\n",
            "[2025-11-06 21:24:57,787][mdl.ntf][INFO] - Saving file per fold as ../output/dblp/toy.dblp.v12.json/splits.f3.r0.85/gs.b1000.e100.ns5.lr0.001.es5.spe10.d128.add.stml.h128.nn30-20/f0.test.e0.pred.eval.mean.csv\n",
            "/content/opentf/src/mdl/ntf.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  Y_ = Ntf.torch.load(predfile, map_location='cpu')['y_pred']\n",
            "[2025-11-06 21:24:57,790][mdl.ntf][INFO] - Evaluating predictions at ../output/dblp/toy.dblp.v12.json/splits.f3.r0.85/gs.b1000.e100.ns5.lr0.001.es5.spe10.d128.add.stml.h128.nn30-20/f1.test.pred ... for {'trec': ['P_2,5,10', 'recall_2,5,10', 'ndcg_cut_2,5,10', 'map_cut_2,5,10', 'success_2,5,10'], 'other': ['skill_coverage_2,5,10', 'aucroc']}\n",
            "[2025-11-06 21:24:57,791][mdl.ntf][INFO] - ['P_2,5,10', 'recall_2,5,10', 'ndcg_cut_2,5,10', 'map_cut_2,5,10', 'success_2,5,10'] ...\n",
            "[2025-11-06 21:24:57,797][evl.metric][INFO] - Building pytrec_eval input for 5 instances ...\n",
            "100% 5/5 [00:00<00:00, 3192.50it/s]\n",
            "[2025-11-06 21:24:57,801][mdl.ntf][INFO] - ['aucroc'] ...\n",
            "[2025-11-06 21:24:57,805][mdl.ntf][INFO] - ['skill_coverage_2,5,10'] ...\n",
            "100% 5/5 [00:00<00:00, 242.66it/s]\n",
            "[2025-11-06 21:24:57,834][mdl.ntf][INFO] - Saving file per fold as ../output/dblp/toy.dblp.v12.json/splits.f3.r0.85/gs.b1000.e100.ns5.lr0.001.es5.spe10.d128.add.stml.h128.nn30-20/f1.test.pred.eval.mean.csv\n",
            "/content/opentf/src/mdl/ntf.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  Y_ = Ntf.torch.load(predfile, map_location='cpu')['y_pred']\n",
            "[2025-11-06 21:24:57,837][mdl.ntf][INFO] - Evaluating predictions at ../output/dblp/toy.dblp.v12.json/splits.f3.r0.85/gs.b1000.e100.ns5.lr0.001.es5.spe10.d128.add.stml.h128.nn30-20/f1.test.e0.pred ... for {'trec': ['P_2,5,10', 'recall_2,5,10', 'ndcg_cut_2,5,10', 'map_cut_2,5,10', 'success_2,5,10'], 'other': ['skill_coverage_2,5,10', 'aucroc']}\n",
            "[2025-11-06 21:24:57,838][mdl.ntf][INFO] - ['P_2,5,10', 'recall_2,5,10', 'ndcg_cut_2,5,10', 'map_cut_2,5,10', 'success_2,5,10'] ...\n",
            "[2025-11-06 21:24:57,843][evl.metric][INFO] - Building pytrec_eval input for 5 instances ...\n",
            "100% 5/5 [00:00<00:00, 3140.39it/s]\n",
            "[2025-11-06 21:24:57,847][mdl.ntf][INFO] - ['aucroc'] ...\n",
            "[2025-11-06 21:24:57,851][mdl.ntf][INFO] - ['skill_coverage_2,5,10'] ...\n",
            "100% 5/5 [00:00<00:00, 246.69it/s]\n",
            "[2025-11-06 21:24:57,880][mdl.ntf][INFO] - Saving file per fold as ../output/dblp/toy.dblp.v12.json/splits.f3.r0.85/gs.b1000.e100.ns5.lr0.001.es5.spe10.d128.add.stml.h128.nn30-20/f1.test.e0.pred.eval.mean.csv\n",
            "/content/opentf/src/mdl/ntf.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  Y_ = Ntf.torch.load(predfile, map_location='cpu')['y_pred']\n",
            "[2025-11-06 21:24:57,883][mdl.ntf][INFO] - Evaluating predictions at ../output/dblp/toy.dblp.v12.json/splits.f3.r0.85/gs.b1000.e100.ns5.lr0.001.es5.spe10.d128.add.stml.h128.nn30-20/f2.test.pred ... for {'trec': ['P_2,5,10', 'recall_2,5,10', 'ndcg_cut_2,5,10', 'map_cut_2,5,10', 'success_2,5,10'], 'other': ['skill_coverage_2,5,10', 'aucroc']}\n",
            "[2025-11-06 21:24:57,885][mdl.ntf][INFO] - ['P_2,5,10', 'recall_2,5,10', 'ndcg_cut_2,5,10', 'map_cut_2,5,10', 'success_2,5,10'] ...\n",
            "[2025-11-06 21:24:57,890][evl.metric][INFO] - Building pytrec_eval input for 5 instances ...\n",
            "100% 5/5 [00:00<00:00, 3202.74it/s]\n",
            "[2025-11-06 21:24:57,894][mdl.ntf][INFO] - ['aucroc'] ...\n",
            "[2025-11-06 21:24:57,898][mdl.ntf][INFO] - ['skill_coverage_2,5,10'] ...\n",
            "100% 5/5 [00:00<00:00, 181.77it/s]\n",
            "[2025-11-06 21:24:57,934][mdl.ntf][INFO] - Saving file per fold as ../output/dblp/toy.dblp.v12.json/splits.f3.r0.85/gs.b1000.e100.ns5.lr0.001.es5.spe10.d128.add.stml.h128.nn30-20/f2.test.pred.eval.mean.csv\n",
            "/content/opentf/src/mdl/ntf.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  Y_ = Ntf.torch.load(predfile, map_location='cpu')['y_pred']\n",
            "[2025-11-06 21:24:57,936][mdl.ntf][INFO] - Evaluating predictions at ../output/dblp/toy.dblp.v12.json/splits.f3.r0.85/gs.b1000.e100.ns5.lr0.001.es5.spe10.d128.add.stml.h128.nn30-20/f2.test.e0.pred ... for {'trec': ['P_2,5,10', 'recall_2,5,10', 'ndcg_cut_2,5,10', 'map_cut_2,5,10', 'success_2,5,10'], 'other': ['skill_coverage_2,5,10', 'aucroc']}\n",
            "[2025-11-06 21:24:57,938][mdl.ntf][INFO] - ['P_2,5,10', 'recall_2,5,10', 'ndcg_cut_2,5,10', 'map_cut_2,5,10', 'success_2,5,10'] ...\n",
            "[2025-11-06 21:24:57,944][evl.metric][INFO] - Building pytrec_eval input for 5 instances ...\n",
            "100% 5/5 [00:00<00:00, 3067.36it/s]\n",
            "[2025-11-06 21:24:57,948][mdl.ntf][INFO] - ['aucroc'] ...\n",
            "[2025-11-06 21:24:57,952][mdl.ntf][INFO] - ['skill_coverage_2,5,10'] ...\n",
            "100% 5/5 [00:00<00:00, 247.22it/s]\n",
            "[2025-11-06 21:24:57,980][mdl.ntf][INFO] - Saving file per fold as ../output/dblp/toy.dblp.v12.json/splits.f3.r0.85/gs.b1000.e100.ns5.lr0.001.es5.spe10.d128.add.stml.h128.nn30-20/f2.test.e0.pred.eval.mean.csv\n",
            "[2025-11-06 21:24:57,985][mdl.ntf][INFO] - Saving mean evaluation file over 3 folds as ../output/dblp/toy.dblp.v12.json/splits.f3.r0.85/gs.b1000.e100.ns5.lr0.001.es5.spe10.d128.add.stml.h128.nn30-20/test.pred.eval.mean.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls ../output/dblp/toy.dblp.v12.json"
      ],
      "metadata": {
        "id": "feJf72sDElJ4",
        "outputId": "672b4795-f702-4bfc-f7f8-51c6355acde4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "indexes.pkl\t\t  splits.f3.r0.85.pkl  stml.mean.graph.pkl\n",
            "prep.train.test.eval.log  stm.add.graph.pkl    stm.mean.graph.pkl\n",
            "skillcoverage.pkl\t  stm.dup.graph.pkl    teams.pkl\n",
            "skill.docs.pkl\t\t  stml.add.graph.pkl   teamsvecs.pkl\n",
            "splits.f3.r0.85\t\t  stml.dup.graph.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls ../output/dblp/toy.dblp.v12.json/splits.f3.r0.85/gs.b1000.e100.ns5.lr0.001.es5.spe10.d128.add.stml.h128.nn30-20"
      ],
      "metadata": {
        "id": "Mu577_wqFMWq",
        "outputId": "73caebb3-ea88-447c-d385-8d6ca75d7198",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "f0.e0.pt\t\t\t       f1.test.pred.eval.mean.csv\n",
            "f0.pt\t\t\t\t       f1.test.pred.eval.per_instance.csv\n",
            "f0.test.e0.pred\t\t\t       f2.e0.pt\n",
            "f0.test.e0.pred.eval.mean.csv\t       f2.pt\n",
            "f0.test.e0.pred.eval.per_instance.csv  f2.test.e0.pred\n",
            "f0.test.pred\t\t\t       f2.test.e0.pred.eval.mean.csv\n",
            "f0.test.pred.eval.mean.csv\t       f2.test.e0.pred.eval.per_instance.csv\n",
            "f0.test.pred.eval.per_instance.csv     f2.test.pred\n",
            "f1.e0.pt\t\t\t       f2.test.pred.eval.mean.csv\n",
            "f1.pt\t\t\t\t       f2.test.pred.eval.per_instance.csv\n",
            "f1.test.e0.pred\t\t\t       logs4tboard\n",
            "f1.test.e0.pred.eval.mean.csv\t       ntf.\n",
            "f1.test.e0.pred.eval.per_instance.csv  test.pred.eval.mean.csv\n",
            "f1.test.pred\t\t\t       test.pred.eval.per_instance_mean.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "pd.read_csv('/content/opentf/output/dblp/toy.dblp.v12.json/splits.f3.r0.85/gs.b1000.e100.ns5.lr0.001.es5.spe10.d128.add.stml.h128.nn30-20/test.pred.eval.mean.csv', index_col = 0)\n"
      ],
      "metadata": {
        "id": "cnLNdZBjFiZv",
        "outputId": "a98ac764-dbbd-4206-c00c-756cf9dc1e7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                       mean       std\n",
              "P_2                0.200000  0.200000\n",
              "P_5                0.160000  0.069282\n",
              "P_10               0.180000  0.034641\n",
              "recall_2           0.166667  0.166667\n",
              "recall_5           0.333333  0.145297\n",
              "recall_10          0.811111  0.153960\n",
              "ndcg_cut_2         0.200000  0.200000\n",
              "ndcg_cut_5         0.270506  0.164425\n",
              "ndcg_cut_10        0.452995  0.163011\n",
              "map_cut_2          0.150000  0.169148\n",
              "map_cut_5          0.211111  0.175066\n",
              "map_cut_10         0.302610  0.167251\n",
              "success_2          0.266667  0.230940\n",
              "success_5          0.533333  0.230940\n",
              "success_10         0.933333  0.115470\n",
              "aucroc             0.513468  0.128697\n",
              "skill_coverage_2   0.844444  0.189541\n",
              "skill_coverage_5   0.955556  0.076980\n",
              "skill_coverage_10  1.000000  0.000000"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ede065ff-fea7-4741-9b2f-12da943827db\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mean</th>\n",
              "      <th>std</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>P_2</th>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.200000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>P_5</th>\n",
              "      <td>0.160000</td>\n",
              "      <td>0.069282</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>P_10</th>\n",
              "      <td>0.180000</td>\n",
              "      <td>0.034641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>recall_2</th>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.166667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>recall_5</th>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.145297</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>recall_10</th>\n",
              "      <td>0.811111</td>\n",
              "      <td>0.153960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ndcg_cut_2</th>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.200000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ndcg_cut_5</th>\n",
              "      <td>0.270506</td>\n",
              "      <td>0.164425</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ndcg_cut_10</th>\n",
              "      <td>0.452995</td>\n",
              "      <td>0.163011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>map_cut_2</th>\n",
              "      <td>0.150000</td>\n",
              "      <td>0.169148</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>map_cut_5</th>\n",
              "      <td>0.211111</td>\n",
              "      <td>0.175066</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>map_cut_10</th>\n",
              "      <td>0.302610</td>\n",
              "      <td>0.167251</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>success_2</th>\n",
              "      <td>0.266667</td>\n",
              "      <td>0.230940</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>success_5</th>\n",
              "      <td>0.533333</td>\n",
              "      <td>0.230940</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>success_10</th>\n",
              "      <td>0.933333</td>\n",
              "      <td>0.115470</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>aucroc</th>\n",
              "      <td>0.513468</td>\n",
              "      <td>0.128697</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>skill_coverage_2</th>\n",
              "      <td>0.844444</td>\n",
              "      <td>0.189541</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>skill_coverage_5</th>\n",
              "      <td>0.955556</td>\n",
              "      <td>0.076980</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>skill_coverage_10</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ede065ff-fea7-4741-9b2f-12da943827db')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ede065ff-fea7-4741-9b2f-12da943827db button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ede065ff-fea7-4741-9b2f-12da943827db');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-f73ac31c-205f-4f15-b312-e87a3f20e73f\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f73ac31c-205f-4f15-b312-e87a3f20e73f')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-f73ac31c-205f-4f15-b312-e87a3f20e73f button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"pd\",\n  \"rows\": 19,\n  \"fields\": [\n    {\n      \"column\": \"mean\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.3072236793933124,\n        \"min\": 0.15,\n        \"max\": 1.0,\n        \"num_unique_values\": 18,\n        \"samples\": [\n          0.2,\n          0.16,\n          0.15\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"std\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.06267396676221809,\n        \"min\": 0.0,\n        \"max\": 0.2309401076758503,\n        \"num_unique_values\": 17,\n        \"samples\": [\n          0.2,\n          0.069282032302755,\n          0.1539600717839002\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "tmp_opentf",
      "language": "python",
      "name": "tmp_opentf"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}