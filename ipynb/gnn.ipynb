{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fani-lab/OpeNTF/blob/main/ipynb/gnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QGHFtvGnTLJ"
      },
      "source": [
        "`OpeNTF-GNN` via `PyG`\n",
        "\n",
        "`OpeNTF` previously used traditional embedding methods (non-graph based) like `doc2vec` to learn skill embeddings as an input alternative to the `1-hot` encoded skills. With graph neural networks (gnn) in `PyG`, we now have integrated graph-based skill embeddings. The gnns capture the synergistic collaborative ties within our transformed graph data to provide with significantly better embeddings for skills, or even direct recommendation of experts for a team via link prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4tk12X-nTLK"
      },
      "source": [
        "**Expert (Member) Graph Structures**\n",
        "\n",
        "<p align=\"center\"><img src='https://raw.githubusercontent.com/fani-lab/OpeNTF/refs/heads/main/docs/figs/graph_structures.png' width=\"400\" ></p>\n",
        "\n",
        "`OpeNTF` applied with gnn aims to cover as many variations in graph structures for a given set of team instances. Currently, it implemented `heterogeneous`, `directed`, `unweighted` graph structures including `[[[skill, to, member]], sm]` bipartite, `[[[skill, to, team], [member, to, team]], stm]` tripartite and `[[[skill, to, team], [member, to, team], [loc, to, team]], stml]`, as seen in the figure, and can be set like:\n",
        "\n",
        "`\"+data.embedding.model.gnn.graph.structure=[[[skill, to, team], [member, to, team], [loc, to, team]], stml]\"`\n",
        "\n",
        "(see [`src/mdl/emb/__config__.yaml`](https://github.com/fani-lab/OpeNTF/blob/main/src/mdl/emb/__config__.yaml#L27) for more details)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ysL9sdP23CI-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "0Uiz41tPnTLK"
      },
      "source": [
        "**Transfer vs. End-to-End Learning with GNN**\n",
        "\n",
        "Gnn methods on an expert graph can be used in either of following ways:\n",
        "\n",
        "<p align=\"center\"><img src='https://raw.githubusercontent.com/fani-lab/OpeNTF/refs/heads/main/docs/figs/gnn_transfer.png' width=\"500\" ></p>\n",
        "\n",
        "1.  **Transfer-based [[WISE24](https://doi.org/10.1007/978-981-96-0567-5_15), [IJCNN23](10.1109/IJCNN54540.2023.10191717), [SIGIR21](https://doi.org/10.1145/3404835.3463105)]**: A gnn method is mainly trained to learn `skill` embeddings, overlooking the embeddings for other node types, and then fed (transfer) into an underlying multilabel classifier, e.g., non-variational feedforward neural net ([`src/mdl/fnn.py`](https://github.com/fani-lab/OpeNTF/blob/main/src/mdl/fnn.py)) or variational Bayesian ([`src/mdl/bnn.py`](https://github.com/fani-lab/OpeNTF/blob/main/src/mdl/bnn.py)). In this case, `OpeNTF` runs in embedding mode by setting `data.embedding.class_method` like\n",
        "\n",
        "    `data.embedding.class_method=mdl.emb.gnn.Gnn_n2v` for [Node2Vec](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.Node2Vec.html)\n",
        "    `data.embedding.class_method=mdl.emb.gnn.Gnn_m2v` for [MetaPath2Vec](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.MetaPath2Vec.html)\n",
        "    `data.embedding.class_method=mdl.emb.gnn.Gnn_gs` for [GraphSAGE](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.SAGEConv.html)\n",
        "\n",
        "\n",
        "    (see [`src/__config__.yaml#L44`](https://github.com/fani-lab/OpeNTF/blob/main/src/__config__.yaml#L44) for more options)\n",
        "\n",
        "\n",
        "    and the classifier model(s) is set by `models.instances` like\n",
        "\n",
        "    `\"models.instances=[mdl.fnn.Fnn, mdl.bnn.Bnn]\"`\n",
        "\n",
        "    (see [`src/__config__.yaml#L57`](https://github.com/fani-lab/OpeNTF/blob/main/src/__config__.yaml#L57) for more options)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "python main.py  \"cmd=[prep,train,test,eval]\" \\\n",
        "                \"models.instances=[mdl.fnn.Fnn, mdl.bnn.Bnn]\" \\\n",
        "                data.domain=cmn.publication.Publication \\\n",
        "                data.source=../data/dblp/toy.dblp.v12.json \\\n",
        "                data.output=../output/dblp/toy.dblp.v12.json \\\n",
        "                ~data.filter \\\n",
        "                data.embedding.class_method=mdl.emb.gnn.Gnn_gs \\\n",
        "                \"+data.embedding.model.gnn.graph.structure=[[[skill, to, team], [member, to, team], [loc, to, team]], stml]\"\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "dE7D-VfK3Fla"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"center\"><img src='https://raw.githubusercontent.com/fani-lab/OpeNTF/refs/heads/main/docs/figs/gnn_e2e.png' width=\"500\" ></p>\n",
        "\n",
        "2.   **Graph Neural Team Recommendation (End-to-End) [[Under Review](https://hosseinfani.github.io/res/papers/Graph_Neural_Team_Recommendation_An_Integrated_Approach.pdf)]**: A gnn method is used to directly predict expert-team links to recommend top-k expert members of a team, skipping the underlying multilabel classifier, as shown above. In this case, the graph neural network in `data.embedding.class_method` is trained to directly predict `member-team` links. So, the classifier model is set `fixed` by `\"models.instances=[mdl.emb.gnn.Gnn]\"`. During inference, given a test team with its subset of required skills yet `unseen` expert members, the graph neural network predicts links between expert nodes and the team's node. In contrast with transfer-based, the end-to-end approach avoids two disjoint learning processes, i.e., pretraining of a graph neural network and fine-tuning for an underlying multilabel classifier, also alleviates the curse of sparsity in multilabel classifier's output layer (there is no classifier).\n",
        "\n",
        "\n",
        "```\n",
        "python main.py  \"cmd=[prep,train,test,eval]\" \\\n",
        "                \"models.instances=[mdl.emb.gnn.Gnn]\" \\\n",
        "                data.domain=cmn.publication.Publication \\\n",
        "                data.source=../data/dblp/toy.dblp.v12.json \\\n",
        "                data.output=../output/dblp/toy.dblp.v12.json \\\n",
        "                ~data.filter \\\n",
        "                data.embedding.class_method=mdl.emb.gnn.Gnn_gs \\\n",
        "                \"+data.embedding.model.gnn.graph.structure=[[[skill, to, team], [member, to, team], [loc, to, team]], stml]\"\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "c8R8Dp2_wejY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "U3bXSGQC3Lic"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        },
        "id": "93caHB2EnTLL"
      },
      "source": [
        "**Hyperparameters**\n",
        "\n",
        "`OpeNTF` leverage [`hydra`](https://hydra.cc/) to manage models hyperparameters in hierarchy:\n",
        "\n",
        "*   [`src/__config__.yaml`](https://github.com/fani-lab/OpeNTF/blob/main/src/__config__.yaml): `OpeNTF`'s main settings for the pipeline execution like `data.*`, `models.*`, `train.*`, `test.*`, `eval.*`\n",
        "    *   [`src/mdl/__config__.yaml`](https://github.com/fani-lab/OpeNTF/blob/main/src/mdl/__config__.yaml): models' training hyperparameters like `fnn.*`, `bnn.*`, `tntf.*`, `lr`, `batch_size`, ...\n",
        "        * [`src/mdl/emb/__config__.yaml`](https://github.com/fani-lab/OpeNTF/blob/main/src/mdl/emb/__config__.yaml): training hyperparameters for embedding methods including gnns like `n2v.*`, `m2v.*`, ..., `dim`, ...\n",
        "\n",
        "To set these hyperparameters,\n",
        "\n",
        "- `Override` them in the running commands (recommended), or\n",
        "- Change the defaults in the `__config__.yaml` files\n",
        "\n",
        "\n",
        "```\n",
        "python main.py  \"cmd=[prep,train,test,eval]\" \\\n",
        "                \"models.instances=[mdl.emb.gnn.Gnn]\" \\\n",
        "                data.domain=cmn.publication.Publication data.source=../data/dblp/toy.dblp.v12.json data.output=../output/dblp/toy.dblp.v12.json ~data.filter \\\n",
        "                data.embedding.class_method=mdl.emb.gnn.Gnn_gs \\\n",
        "                +data.embedding.model.gnn.gs.d=64\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "rEqgd5eO95eD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwEN1xLDnTLK"
      },
      "source": [
        "**Setup & Quickstart**\n",
        "\n",
        "From the [`quickstart`](https://colab.research.google.com/github/fani-lab/OpeNTF/blob/main/ipynb/quickstart.ipynb) script:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZln-_cnnTLM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b103a2d-7a22-4ad7-c7bf-4fbefd528185"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:3 https://cli.github.com/packages stable InRelease\n",
            "Get:4 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:10 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,289 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,526 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [5,969 kB]\n",
            "Get:15 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [2,123 kB]\n",
            "Get:16 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,421 kB]\n",
            "Get:17 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,820 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,594 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [6,168 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,856 kB]\n",
            "Fetched 37.2 MB in 8s (4,799 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libpython3.8-minimal libpython3.8-stdlib python3-pkg-resources\n",
            "  python3-setuptools python3-wheel python3.8-lib2to3 python3.8-minimal\n",
            "Suggested packages:\n",
            "  python-setuptools-doc binfmt-support\n",
            "The following NEW packages will be installed:\n",
            "  libpython3.8-minimal libpython3.8-stdlib python3-pip python3-setuptools\n",
            "  python3-wheel python3.8 python3.8-distutils python3.8-lib2to3\n",
            "  python3.8-minimal python3.8-venv\n",
            "The following packages will be upgraded:\n",
            "  python3-pkg-resources\n",
            "1 upgraded, 10 newly installed, 0 to remove and 43 not upgraded.\n",
            "Need to get 10.0 MB of archives.\n",
            "After this operation, 32.6 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 python3-wheel all 0.37.1-2ubuntu0.22.04.1 [32.0 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 python3-pip all 22.0.2+dfsg-1ubuntu0.7 [1,306 kB]\n",
            "Get:3 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 libpython3.8-minimal amd64 3.8.20-1+jammy1 [796 kB]\n",
            "Get:4 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 python3.8-minimal amd64 3.8.20-1+jammy1 [2,023 kB]\n",
            "Get:5 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 libpython3.8-stdlib amd64 3.8.20-1+jammy1 [1,817 kB]\n",
            "Get:6 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy/main amd64 python3-pkg-resources all 68.1.2-2~jammy3 [216 kB]\n",
            "Get:7 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy/main amd64 python3-setuptools all 68.1.2-2~jammy3 [465 kB]\n",
            "Get:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 python3.8 amd64 3.8.20-1+jammy1 [440 kB]\n",
            "Get:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 python3.8-lib2to3 all 3.8.20-1+jammy1 [126 kB]\n",
            "Get:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 python3.8-distutils all 3.8.20-1+jammy1 [193 kB]\n",
            "Get:11 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 python3.8-venv amd64 3.8.20-1+jammy1 [2,618 kB]\n",
            "Fetched 10.0 MB in 21s (489 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 11.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libpython3.8-minimal:amd64.\n",
            "(Reading database ... 125082 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libpython3.8-minimal_3.8.20-1+jammy1_amd64.deb ...\n",
            "Unpacking libpython3.8-minimal:amd64 (3.8.20-1+jammy1) ...\n",
            "Selecting previously unselected package python3.8-minimal.\n",
            "Preparing to unpack .../01-python3.8-minimal_3.8.20-1+jammy1_amd64.deb ...\n",
            "Unpacking python3.8-minimal (3.8.20-1+jammy1) ...\n",
            "Selecting previously unselected package libpython3.8-stdlib:amd64.\n",
            "Preparing to unpack .../02-libpython3.8-stdlib_3.8.20-1+jammy1_amd64.deb ...\n",
            "Unpacking libpython3.8-stdlib:amd64 (3.8.20-1+jammy1) ...\n",
            "Preparing to unpack .../03-python3-pkg-resources_68.1.2-2~jammy3_all.deb ...\n",
            "Unpacking python3-pkg-resources (68.1.2-2~jammy3) over (59.6.0-1.2ubuntu0.22.04.3) ...\n",
            "Selecting previously unselected package python3-setuptools.\n",
            "Preparing to unpack .../04-python3-setuptools_68.1.2-2~jammy3_all.deb ...\n",
            "Unpacking python3-setuptools (68.1.2-2~jammy3) ...\n",
            "Selecting previously unselected package python3-wheel.\n",
            "Preparing to unpack .../05-python3-wheel_0.37.1-2ubuntu0.22.04.1_all.deb ...\n",
            "Unpacking python3-wheel (0.37.1-2ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package python3-pip.\n",
            "Preparing to unpack .../06-python3-pip_22.0.2+dfsg-1ubuntu0.7_all.deb ...\n",
            "Unpacking python3-pip (22.0.2+dfsg-1ubuntu0.7) ...\n",
            "Selecting previously unselected package python3.8.\n",
            "Preparing to unpack .../07-python3.8_3.8.20-1+jammy1_amd64.deb ...\n",
            "Unpacking python3.8 (3.8.20-1+jammy1) ...\n",
            "Selecting previously unselected package python3.8-lib2to3.\n",
            "Preparing to unpack .../08-python3.8-lib2to3_3.8.20-1+jammy1_all.deb ...\n",
            "Unpacking python3.8-lib2to3 (3.8.20-1+jammy1) ...\n",
            "Selecting previously unselected package python3.8-distutils.\n",
            "Preparing to unpack .../09-python3.8-distutils_3.8.20-1+jammy1_all.deb ...\n",
            "Unpacking python3.8-distutils (3.8.20-1+jammy1) ...\n",
            "Selecting previously unselected package python3.8-venv.\n",
            "Preparing to unpack .../10-python3.8-venv_3.8.20-1+jammy1_amd64.deb ...\n",
            "Unpacking python3.8-venv (3.8.20-1+jammy1) ...\n",
            "Setting up python3-pkg-resources (68.1.2-2~jammy3) ...\n",
            "Setting up libpython3.8-minimal:amd64 (3.8.20-1+jammy1) ...\n",
            "Setting up python3-setuptools (68.1.2-2~jammy3) ...\n",
            "Setting up python3-wheel (0.37.1-2ubuntu0.22.04.1) ...\n",
            "Setting up python3.8-lib2to3 (3.8.20-1+jammy1) ...\n",
            "Setting up python3-pip (22.0.2+dfsg-1ubuntu0.7) ...\n",
            "Setting up python3.8-minimal (3.8.20-1+jammy1) ...\n",
            "Setting up python3.8-distutils (3.8.20-1+jammy1) ...\n",
            "Setting up libpython3.8-stdlib:amd64 (3.8.20-1+jammy1) ...\n",
            "Setting up python3.8 (3.8.20-1+jammy1) ...\n",
            "Setting up python3.8-venv (3.8.20-1+jammy1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n",
            "update-alternatives: using /usr/bin/python3.8 to provide /usr/bin/python3 (python3) in auto mode\n",
            "Python 3.8.20\n"
          ]
        }
      ],
      "source": [
        "# set up python 3.8\n",
        "!sudo apt-get update -y\n",
        "!sudo apt-get install -y python3.8 python3.8-venv python3.8-distutils python3-pip\n",
        "!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.8 10\n",
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get OpeNTF\n",
        "!rm -R opentf/\n",
        "!git clone https://github.com/Fani-Lab/opentf\n",
        "!pip install --upgrade pip setuptools\n",
        "!pip install -r opentf/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "95eDgj3AAInL",
        "outputId": "14b27a9c-63df-4e98-a1d0-0879b2ae8e48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'opentf/': No such file or directory\n",
            "Cloning into 'opentf'...\n",
            "remote: Enumerating objects: 22706, done.\u001b[K\n",
            "remote: Counting objects: 100% (123/123), done.\u001b[K\n",
            "remote: Compressing objects: 100% (96/96), done.\u001b[K\n",
            "remote: Total 22706 (delta 56), reused 50 (delta 26), pack-reused 22583 (from 3)\u001b[K\n",
            "Receiving objects: 100% (22706/22706), 1004.21 MiB | 24.56 MiB/s, done.\n",
            "Resolving deltas: 100% (11259/11259), done.\n",
            "Updating files: 100% (1595/1595), done.\n",
            "Requirement already satisfied: pip in /usr/lib/python3/dist-packages (22.0.2)\n",
            "Collecting pip\n",
            "  Downloading pip-25.0.1-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/lib/python3/dist-packages (68.1.2)\n",
            "Collecting setuptools\n",
            "  Downloading setuptools-75.3.2-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: setuptools, pip\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 68.1.2\n",
            "    Not uninstalling setuptools at /usr/lib/python3/dist-packages, outside environment /usr\n",
            "    Can't uninstall 'setuptools'. No files were found to uninstall.\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 22.0.2\n",
            "    Not uninstalling pip at /usr/lib/python3/dist-packages, outside environment /usr\n",
            "    Can't uninstall 'pip'. No files were found to uninstall.\n",
            "Successfully installed pip-25.0.1 setuptools-75.3.2\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "_distutils_hack"
                ]
              },
              "id": "5d6b6ecd1b78458d8009a7dcbea6f32d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting hydra-core==1.3.2 (from -r opentf/requirements.txt (line 3))\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting scipy==1.10.1 (from -r opentf/requirements.txt (line 4))\n",
            "  Downloading scipy-1.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
            "Collecting numpy==1.24.4 (from -r opentf/requirements.txt (line 5))\n",
            "  Downloading numpy-1.24.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Collecting omegaconf<2.4,>=2.2 (from hydra-core==1.3.2->-r opentf/requirements.txt (line 3))\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from hydra-core==1.3.2->-r opentf/requirements.txt (line 3))\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting packaging (from hydra-core==1.3.2->-r opentf/requirements.txt (line 3))\n",
            "  Downloading packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting importlib-resources (from hydra-core==1.3.2->-r opentf/requirements.txt (line 3))\n",
            "  Downloading importlib_resources-6.4.5-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting PyYAML>=5.1.0 (from omegaconf<2.4,>=2.2->hydra-core==1.3.2->-r opentf/requirements.txt (line 3))\n",
            "  Downloading PyYAML-6.0.3-cp38-cp38-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting zipp>=3.1.0 (from importlib-resources->hydra-core==1.3.2->-r opentf/requirements.txt (line 3))\n",
            "  Downloading zipp-3.20.2-py3-none-any.whl.metadata (3.7 kB)\n",
            "Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "Downloading scipy-1.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.5/34.5 MB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.24.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m134.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "Downloading importlib_resources-6.4.5-py3-none-any.whl (36 kB)\n",
            "Downloading packaging-25.0-py3-none-any.whl (66 kB)\n",
            "Downloading PyYAML-6.0.3-cp38-cp38-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (806 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m806.0/806.0 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading zipp-3.20.2-py3-none-any.whl (9.2 kB)\n",
            "Building wheels for collected packages: antlr4-python3-runtime\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144552 sha256=3bcc1d21796b586e22a3644f07c6bb17c97cf7f4d4cfe62d7dff82266c7cc28b\n",
            "  Stored in directory: /root/.cache/pip/wheels/b1/a3/c2/6df046c09459b73cc9bb6c4401b0be6c47048baf9a1617c485\n",
            "Successfully built antlr4-python3-runtime\n",
            "Installing collected packages: antlr4-python3-runtime, zipp, PyYAML, packaging, numpy, scipy, omegaconf, importlib-resources, hydra-core\n",
            "  Attempting uninstall: zipp\n",
            "    Found existing installation: zipp 1.0.0\n",
            "    Uninstalling zipp-1.0.0:\n",
            "      Successfully uninstalled zipp-1.0.0\n",
            "Successfully installed PyYAML-6.0.3 antlr4-python3-runtime-4.9.3 hydra-core-1.3.2 importlib-resources-6.4.5 numpy-1.24.4 omegaconf-2.3.0 packaging-25.0 scipy-1.10.1 zipp-3.20.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "packaging",
                  "pydevd_plugins"
                ]
              },
              "id": "cc69fd6a138f40998fece5635e8d1583"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Graph Neural Team Recommendation\n",
        "\n",
        "- Preprocessing raw data `cmd=prep` into `teamsvecs` sparse matrix each row of which is a team with its skills `teamsvecs['skill']`, members `teamsvecs['member']` as 1-hot vectors\n",
        "- `data.domain`, `data.source`, `data.output` from `toy.dblp` dataset, available at the codebase [`OpeNTF/data/dblp`](https://github.com/fani-lab/OpeNTF/tree/main/data/dblp)\n",
        "- No filtering for min team size `data.filter.min_nteam` and min number of teams per experts `data.filter.min_team_size`\n",
        "- End-to-End GraphSAGE (`mdl.emb.gnn.Gnn_gs`) for team recommendation for `train`, `test`, and `eval` steps on the `skill-team-expert-location` graph structure.  \n",
        "- Embedding size `+data.embedding.model.gnn.gs.d=64`\n"
      ],
      "metadata": {
        "id": "F0R0OLHXASxY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd opentf/src/\n",
        "!python main.py \"cmd=[prep,train,test,eval]\" \"models.instances=[mdl.emb.gnn.Gnn]\" data.domain=cmn.publication.Publication data.source=../data/dblp/toy.dblp.v12.json data.output=../output/dblp/toy.dblp.v12.json ~data.filter data.embedding.class_method=mdl.emb.gnn.Gnn_gs +data.embedding.model.gnn.gs.d=64 \"+data.embedding.model.gnn.graph.structure=[[[skill, to, team], [member, to, team], [loc, to, team]], stml]\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDdbvZnWATUw",
        "outputId": "8959d593-17bf-4518-fadb-5bc77e715693"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'opentf/src/'\n",
            "/content/opentf/src\n",
            "[2025-11-06 21:36:30,503][cmn.team][INFO] - Loading teamsvecs matrices from ../output/dblp/toy.dblp.v12.json/teamsvecs.pkl ...\n",
            "[2025-11-06 21:36:30,533][cmn.team][INFO] - Loading indexes pickle from ../output/dblp/toy.dblp.v12.json/indexes.pkl ...\n",
            "[2025-11-06 21:36:30,534][cmn.team][INFO] - Indexes pickle is loaded.\n",
            "[2025-11-06 21:36:30,535][cmn.team][INFO] - Teamsvecs matrices and indexes for skills (31, 10), members (31, 13), and locations (31, 29) are loaded.\n",
            "[2025-11-06 21:36:30,535][__main__][INFO] - Loading splits from ../output/dblp/toy.dblp.v12.json/splits.f3.r0.85.pkl ...\n",
            "[2025-11-06 21:36:30,537][cmn.team][INFO] - Loading member-skill co-occurrence matrix (13, 10) from ../output/dblp/toy.dblp.v12.json/skillcoverage.pkl ...\n",
            "[2025-11-06 21:36:35,436][mdl.emb.gnn][INFO] - Loading graph of ([['skill', 'to', 'team'], ['member', 'to', 'team'], ['loc', 'to', 'team']], 'stml') from ../output/dblp/toy.dblp.v12.json/splits.f3.r0.85/../stml.add.graph.pkl  ...\n",
            "[2025-11-06 21:36:35,441][mdl.emb.gnn][INFO] - \u001b[94mTraining gs \u001b[0m... \n",
            "[2025-11-06 21:36:35,509][mdl.emb.gnn][INFO] - Loading the model ../output/dblp/toy.dblp.v12.json/splits.f3.r0.85/gs.b1000.e100.ns5.lr0.001.es5.spe10.d64.add.stml.h128.nn30-20/f0.pt ...\n",
            "/content/opentf/src/mdl/emb/gnn.py:230: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  return self.model.load_state_dict(Gnn.torch.load(f'{self.output}/f{foldidx}.pt', map_location=self.device)['model_state_dict'])\n",
            "[2025-11-06 21:36:35,509][mdl.emb.gnn][INFO] - \u001b[93mFile not found! Training ...\u001b[0m\n",
            "/usr/local/lib/python3.8/dist-packages/torch_geometric/sampler/neighbor_sampler.py:61: UserWarning: Using 'NeighborSampler' without a 'pyg-lib' installation is deprecated and will be removed soon. Please install 'pyg-lib' for accelerated neighborhood sampling\n",
            "  warnings.warn(f\"Using '{self.__class__.__name__}' without a \"\n",
            "[2025-11-06 21:36:35,533][mdl.emb.gnn][INFO] - Fold 0/2, Epoch 0, \u001b[94mTrain Loss: 0.9145\u001b[0m\n",
            "[2025-11-06 21:36:35,538][mdl.emb.gnn][INFO] - Fold 0/2, Epoch 0, \u001b[95mValid Loss: 0.4813\u001b[0m\n",
            "[2025-11-06 21:36:35,547][mdl.emb.gnn][INFO] - gs model with d64.e100.b1000.lr0.001.es5.ns5.spe10.h[128].nn[30, 20].supervision_edge_types[['member', 'to', 'team'], ['team', 'rev_to', 'member']] saved at ../output/dblp/toy.dblp.v12.json/splits.f3.r0.85/gs.b1000.e100.ns5.lr0.001.es5.spe10.d64.add.stml.h128.nn30-20.e0.pt\n",
            "[2025-11-06 21:36:35,547][mdl.emb.gnn][INFO] - Validation loss decreased (inf --> 0.481272)\n",
            "[2025-11-06 21:36:35,555][mdl.emb.gnn][INFO] - Fold 0/2, Epoch 1, \u001b[94mTrain Loss: 0.9000\u001b[0m\n",
            "[2025-11-06 21:36:35,559][mdl.emb.gnn][INFO] - Fold 0/2, Epoch 1, \u001b[95mValid Loss: 0.4896\u001b[0m\n",
            "[2025-11-06 21:36:35,559][mdl.emb.gnn][INFO] - EarlyStopping counter: 1 out of 5\n",
            "[2025-11-06 21:36:35,565][mdl.emb.gnn][INFO] - Fold 0/2, Epoch 2, \u001b[94mTrain Loss: 0.8850\u001b[0m\n",
            "[2025-11-06 21:36:35,568][mdl.emb.gnn][INFO] - Fold 0/2, Epoch 2, \u001b[95mValid Loss: 0.4977\u001b[0m\n",
            "[2025-11-06 21:36:35,569][mdl.emb.gnn][INFO] - EarlyStopping counter: 2 out of 5\n",
            "[2025-11-06 21:36:35,574][mdl.emb.gnn][INFO] - Fold 0/2, Epoch 3, \u001b[94mTrain Loss: 0.8719\u001b[0m\n",
            "[2025-11-06 21:36:35,578][mdl.emb.gnn][INFO] - Fold 0/2, Epoch 3, \u001b[95mValid Loss: 0.5056\u001b[0m\n",
            "[2025-11-06 21:36:35,578][mdl.emb.gnn][INFO] - EarlyStopping counter: 3 out of 5\n",
            "[2025-11-06 21:36:35,583][mdl.emb.gnn][INFO] - Fold 0/2, Epoch 4, \u001b[94mTrain Loss: 0.8593\u001b[0m\n",
            "[2025-11-06 21:36:35,587][mdl.emb.gnn][INFO] - Fold 0/2, Epoch 4, \u001b[95mValid Loss: 0.5132\u001b[0m\n",
            "[2025-11-06 21:36:35,587][mdl.emb.gnn][INFO] - EarlyStopping counter: 4 out of 5\n",
            "[2025-11-06 21:36:35,594][mdl.emb.gnn][INFO] - Fold 0/2, Epoch 5, \u001b[94mTrain Loss: 0.8465\u001b[0m\n",
            "[2025-11-06 21:36:35,598][mdl.emb.gnn][INFO] - Fold 0/2, Epoch 5, \u001b[95mValid Loss: 0.5208\u001b[0m\n",
            "[2025-11-06 21:36:35,599][mdl.emb.gnn][INFO] - EarlyStopping counter: 5 out of 5\n",
            "[2025-11-06 21:36:35,599][mdl.emb.gnn][INFO] - Early stopping triggered at epoch: 5\n",
            "[2025-11-06 21:36:35,606][mdl.emb.gnn][INFO] - gs model with d64.e100.b1000.lr0.001.es5.ns5.spe10.h[128].nn[30, 20].supervision_edge_types[['member', 'to', 'team'], ['team', 'rev_to', 'member']] saved at ../output/dblp/toy.dblp.v12.json/splits.f3.r0.85/gs.b1000.e100.ns5.lr0.001.es5.spe10.d64.add.stml.h128.nn30-20.pt.\n",
            "[2025-11-06 21:36:35,613][mdl.emb.gnn][INFO] - Loading the model ../output/dblp/toy.dblp.v12.json/splits.f3.r0.85/gs.b1000.e100.ns5.lr0.001.es5.spe10.d64.add.stml.h128.nn30-20/f1.pt ...\n",
            "[2025-11-06 21:36:35,614][mdl.emb.gnn][INFO] - \u001b[93mFile not found! Training ...\u001b[0m\n",
            "[2025-11-06 21:36:35,623][mdl.emb.gnn][INFO] - Fold 1/2, Epoch 0, \u001b[94mTrain Loss: 0.8848\u001b[0m\n",
            "[2025-11-06 21:36:35,627][mdl.emb.gnn][INFO] - Fold 1/2, Epoch 0, \u001b[95mValid Loss: 0.4856\u001b[0m\n",
            "[2025-11-06 21:36:35,634][mdl.emb.gnn][INFO] - gs model with d64.e100.b1000.lr0.001.es5.ns5.spe10.h[128].nn[30, 20].supervision_edge_types[['member', 'to', 'team'], ['team', 'rev_to', 'member']] saved at ../output/dblp/toy.dblp.v12.json/splits.f3.r0.85/gs.b1000.e100.ns5.lr0.001.es5.spe10.d64.add.stml.h128.nn30-20.e0.pt\n",
            "[2025-11-06 21:36:35,634][mdl.emb.gnn][INFO] - Validation loss decreased (inf --> 0.485626)\n",
            "[2025-11-06 21:36:35,641][mdl.emb.gnn][INFO] - Fold 1/2, Epoch 1, \u001b[94mTrain Loss: 0.8734\u001b[0m\n",
            "[2025-11-06 21:36:35,644][mdl.emb.gnn][INFO] - Fold 1/2, Epoch 1, \u001b[95mValid Loss: 0.4926\u001b[0m\n",
            "[2025-11-06 21:36:35,645][mdl.emb.gnn][INFO] - EarlyStopping counter: 1 out of 5\n",
            "[2025-11-06 21:36:35,651][mdl.emb.gnn][INFO] - Fold 1/2, Epoch 2, \u001b[94mTrain Loss: 0.8634\u001b[0m\n",
            "[2025-11-06 21:36:35,655][mdl.emb.gnn][INFO] - Fold 1/2, Epoch 2, \u001b[95mValid Loss: 0.4994\u001b[0m\n",
            "[2025-11-06 21:36:35,655][mdl.emb.gnn][INFO] - EarlyStopping counter: 2 out of 5\n",
            "[2025-11-06 21:36:35,660][mdl.emb.gnn][INFO] - Fold 1/2, Epoch 3, \u001b[94mTrain Loss: 0.8488\u001b[0m\n",
            "[2025-11-06 21:36:35,664][mdl.emb.gnn][INFO] - Fold 1/2, Epoch 3, \u001b[95mValid Loss: 0.5062\u001b[0m\n",
            "[2025-11-06 21:36:35,664][mdl.emb.gnn][INFO] - EarlyStopping counter: 3 out of 5\n",
            "[2025-11-06 21:36:35,669][mdl.emb.gnn][INFO] - Fold 1/2, Epoch 4, \u001b[94mTrain Loss: 0.8377\u001b[0m\n",
            "[2025-11-06 21:36:35,673][mdl.emb.gnn][INFO] - Fold 1/2, Epoch 4, \u001b[95mValid Loss: 0.5128\u001b[0m\n",
            "[2025-11-06 21:36:35,673][mdl.emb.gnn][INFO] - EarlyStopping counter: 4 out of 5\n",
            "[2025-11-06 21:36:35,678][mdl.emb.gnn][INFO] - Fold 1/2, Epoch 5, \u001b[94mTrain Loss: 0.8267\u001b[0m\n",
            "[2025-11-06 21:36:35,682][mdl.emb.gnn][INFO] - Fold 1/2, Epoch 5, \u001b[95mValid Loss: 0.5193\u001b[0m\n",
            "[2025-11-06 21:36:35,682][mdl.emb.gnn][INFO] - EarlyStopping counter: 5 out of 5\n",
            "[2025-11-06 21:36:35,682][mdl.emb.gnn][INFO] - Early stopping triggered at epoch: 5\n",
            "[2025-11-06 21:36:35,690][mdl.emb.gnn][INFO] - gs model with d64.e100.b1000.lr0.001.es5.ns5.spe10.h[128].nn[30, 20].supervision_edge_types[['member', 'to', 'team'], ['team', 'rev_to', 'member']] saved at ../output/dblp/toy.dblp.v12.json/splits.f3.r0.85/gs.b1000.e100.ns5.lr0.001.es5.spe10.d64.add.stml.h128.nn30-20.pt.\n",
            "[2025-11-06 21:36:35,697][mdl.emb.gnn][INFO] - Loading the model ../output/dblp/toy.dblp.v12.json/splits.f3.r0.85/gs.b1000.e100.ns5.lr0.001.es5.spe10.d64.add.stml.h128.nn30-20/f2.pt ...\n",
            "[2025-11-06 21:36:35,697][mdl.emb.gnn][INFO] - \u001b[93mFile not found! Training ...\u001b[0m\n",
            "[2025-11-06 21:36:35,707][mdl.emb.gnn][INFO] - Fold 2/2, Epoch 0, \u001b[94mTrain Loss: 0.8532\u001b[0m\n",
            "[2025-11-06 21:36:35,710][mdl.emb.gnn][INFO] - Fold 2/2, Epoch 0, \u001b[95mValid Loss: 0.5446\u001b[0m\n",
            "[2025-11-06 21:36:35,718][mdl.emb.gnn][INFO] - gs model with d64.e100.b1000.lr0.001.es5.ns5.spe10.h[128].nn[30, 20].supervision_edge_types[['member', 'to', 'team'], ['team', 'rev_to', 'member']] saved at ../output/dblp/toy.dblp.v12.json/splits.f3.r0.85/gs.b1000.e100.ns5.lr0.001.es5.spe10.d64.add.stml.h128.nn30-20.e0.pt\n",
            "[2025-11-06 21:36:35,719][mdl.emb.gnn][INFO] - Validation loss decreased (inf --> 0.544593)\n",
            "[2025-11-06 21:36:35,724][mdl.emb.gnn][INFO] - Fold 2/2, Epoch 1, \u001b[94mTrain Loss: 0.8447\u001b[0m\n",
            "[2025-11-06 21:36:35,729][mdl.emb.gnn][INFO] - Fold 2/2, Epoch 1, \u001b[95mValid Loss: 0.5520\u001b[0m\n",
            "[2025-11-06 21:36:35,730][mdl.emb.gnn][INFO] - EarlyStopping counter: 1 out of 5\n",
            "[2025-11-06 21:36:35,736][mdl.emb.gnn][INFO] - Fold 2/2, Epoch 2, \u001b[94mTrain Loss: 0.8316\u001b[0m\n",
            "[2025-11-06 21:36:35,740][mdl.emb.gnn][INFO] - Fold 2/2, Epoch 2, \u001b[95mValid Loss: 0.5591\u001b[0m\n",
            "[2025-11-06 21:36:35,741][mdl.emb.gnn][INFO] - EarlyStopping counter: 2 out of 5\n",
            "[2025-11-06 21:36:35,746][mdl.emb.gnn][INFO] - Fold 2/2, Epoch 3, \u001b[94mTrain Loss: 0.8208\u001b[0m\n",
            "[2025-11-06 21:36:35,750][mdl.emb.gnn][INFO] - Fold 2/2, Epoch 3, \u001b[95mValid Loss: 0.5660\u001b[0m\n",
            "[2025-11-06 21:36:35,750][mdl.emb.gnn][INFO] - EarlyStopping counter: 3 out of 5\n",
            "[2025-11-06 21:36:35,756][mdl.emb.gnn][INFO] - Fold 2/2, Epoch 4, \u001b[94mTrain Loss: 0.8116\u001b[0m\n",
            "[2025-11-06 21:36:35,759][mdl.emb.gnn][INFO] - Fold 2/2, Epoch 4, \u001b[95mValid Loss: 0.5726\u001b[0m\n",
            "[2025-11-06 21:36:35,760][mdl.emb.gnn][INFO] - EarlyStopping counter: 4 out of 5\n",
            "[2025-11-06 21:36:35,765][mdl.emb.gnn][INFO] - Fold 2/2, Epoch 5, \u001b[94mTrain Loss: 0.8030\u001b[0m\n",
            "[2025-11-06 21:36:35,768][mdl.emb.gnn][INFO] - Fold 2/2, Epoch 5, \u001b[95mValid Loss: 0.5790\u001b[0m\n",
            "[2025-11-06 21:36:35,769][mdl.emb.gnn][INFO] - EarlyStopping counter: 5 out of 5\n",
            "[2025-11-06 21:36:35,769][mdl.emb.gnn][INFO] - Early stopping triggered at epoch: 5\n",
            "[2025-11-06 21:36:35,776][mdl.emb.gnn][INFO] - gs model with d64.e100.b1000.lr0.001.es5.ns5.spe10.h[128].nn[30, 20].supervision_edge_types[['member', 'to', 'team'], ['team', 'rev_to', 'member']] saved at ../output/dblp/toy.dblp.v12.json/splits.f3.r0.85/gs.b1000.e100.ns5.lr0.001.es5.spe10.d64.add.stml.h128.nn30-20.pt.\n",
            "[2025-11-06 21:36:35,808][__main__][INFO] - \u001b[94mTraining team recommender instance mdl.emb.gnn.Gnn ... \u001b[0m\n",
            "[2025-11-06 21:36:35,808][__main__][INFO] - \u001b[93mTraining a Gnn instance is through data.embedding.class_method! \u001b[0m\n",
            "[2025-11-06 21:36:35,808][__main__][INFO] - \u001b[92mTesting team recommender instance mdl.emb.gnn.Gnn ... \u001b[0m\n",
            "[2025-11-06 21:36:35,808][mdl.emb.gnn][INFO] - \u001b[94mTesting gs \u001b[0m... \n",
            "/content/opentf/src/mdl/emb/gnn.py:417: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  self.model.load_state_dict(Gnn.torch.load(modelfile, map_location=self.device)['model_state_dict'])\n",
            "[2025-11-06 21:36:35,820][mdl.emb.gnn][INFO] - gs model predictions for fold0.test. has saved at ../output/dblp/toy.dblp.v12.json/splits.f3.r0.85/gs.b1000.e100.ns5.lr0.001.es5.spe10.d64.add.stml.h128.nn30-20/f0.test.pred\n",
            "[2025-11-06 21:36:35,829][mdl.emb.gnn][INFO] - gs model predictions for fold0.test.e0. has saved at ../output/dblp/toy.dblp.v12.json/splits.f3.r0.85/gs.b1000.e100.ns5.lr0.001.es5.spe10.d64.add.stml.h128.nn30-20/f0.test.e0.pred\n",
            "[2025-11-06 21:36:35,839][mdl.emb.gnn][INFO] - gs model predictions for fold1.test. has saved at ../output/dblp/toy.dblp.v12.json/splits.f3.r0.85/gs.b1000.e100.ns5.lr0.001.es5.spe10.d64.add.stml.h128.nn30-20/f1.test.pred\n",
            "[2025-11-06 21:36:35,848][mdl.emb.gnn][INFO] - gs model predictions for fold1.test.e0. has saved at ../output/dblp/toy.dblp.v12.json/splits.f3.r0.85/gs.b1000.e100.ns5.lr0.001.es5.spe10.d64.add.stml.h128.nn30-20/f1.test.e0.pred\n",
            "[2025-11-06 21:36:35,858][mdl.emb.gnn][INFO] - gs model predictions for fold2.test. has saved at ../output/dblp/toy.dblp.v12.json/splits.f3.r0.85/gs.b1000.e100.ns5.lr0.001.es5.spe10.d64.add.stml.h128.nn30-20/f2.test.pred\n",
            "[2025-11-06 21:36:35,868][mdl.emb.gnn][INFO] - gs model predictions for fold2.test.e0. has saved at ../output/dblp/toy.dblp.v12.json/splits.f3.r0.85/gs.b1000.e100.ns5.lr0.001.es5.spe10.d64.add.stml.h128.nn30-20/f2.test.e0.pred\n",
            "[2025-11-06 21:36:35,868][__main__][INFO] - \u001b[95mEvaluating team recommender instance mdl.emb.gnn.Gnn ... \u001b[0m\n",
            "/content/opentf/src/mdl/ntf.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  Y_ = Ntf.torch.load(predfile, map_location='cpu')['y_pred']\n",
            "[2025-11-06 21:36:36,166][mdl.ntf][INFO] - Evaluating predictions at ../output/dblp/toy.dblp.v12.json/splits.f3.r0.85/gs.b1000.e100.ns5.lr0.001.es5.spe10.d64.add.stml.h128.nn30-20/f0.test.pred ... for {'trec': ['P_2,5,10', 'recall_2,5,10', 'ndcg_cut_2,5,10', 'map_cut_2,5,10', 'success_2,5,10'], 'other': ['skill_coverage_2,5,10', 'aucroc']}\n",
            "[2025-11-06 21:36:36,167][mdl.ntf][INFO] - ['P_2,5,10', 'recall_2,5,10', 'ndcg_cut_2,5,10', 'map_cut_2,5,10', 'success_2,5,10'] ...\n",
            "[2025-11-06 21:36:36,175][evl.metric][INFO] - Building pytrec_eval input for 5 instances ...\n",
            "100% 5/5 [00:00<00:00, 2606.45it/s]\n",
            "[2025-11-06 21:36:36,187][mdl.ntf][INFO] - ['aucroc'] ...\n",
            "[2025-11-06 21:36:36,717][mdl.ntf][INFO] - ['skill_coverage_2,5,10'] ...\n",
            "100% 5/5 [00:00<00:00, 239.17it/s]\n",
            "[2025-11-06 21:36:36,749][mdl.ntf][INFO] - Saving file per fold as ../output/dblp/toy.dblp.v12.json/splits.f3.r0.85/gs.b1000.e100.ns5.lr0.001.es5.spe10.d64.add.stml.h128.nn30-20/f0.test.pred.eval.mean.csv\n",
            "/content/opentf/src/mdl/ntf.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  Y_ = Ntf.torch.load(predfile, map_location='cpu')['y_pred']\n",
            "[2025-11-06 21:36:36,752][mdl.ntf][INFO] - Evaluating predictions at ../output/dblp/toy.dblp.v12.json/splits.f3.r0.85/gs.b1000.e100.ns5.lr0.001.es5.spe10.d64.add.stml.h128.nn30-20/f0.test.e0.pred ... for {'trec': ['P_2,5,10', 'recall_2,5,10', 'ndcg_cut_2,5,10', 'map_cut_2,5,10', 'success_2,5,10'], 'other': ['skill_coverage_2,5,10', 'aucroc']}\n",
            "[2025-11-06 21:36:36,753][mdl.ntf][INFO] - ['P_2,5,10', 'recall_2,5,10', 'ndcg_cut_2,5,10', 'map_cut_2,5,10', 'success_2,5,10'] ...\n",
            "[2025-11-06 21:36:36,759][evl.metric][INFO] - Building pytrec_eval input for 5 instances ...\n",
            "100% 5/5 [00:00<00:00, 3312.51it/s]\n",
            "[2025-11-06 21:36:36,763][mdl.ntf][INFO] - ['aucroc'] ...\n",
            "[2025-11-06 21:36:36,767][mdl.ntf][INFO] - ['skill_coverage_2,5,10'] ...\n",
            "100% 5/5 [00:00<00:00, 241.92it/s]\n",
            "[2025-11-06 21:36:36,796][mdl.ntf][INFO] - Saving file per fold as ../output/dblp/toy.dblp.v12.json/splits.f3.r0.85/gs.b1000.e100.ns5.lr0.001.es5.spe10.d64.add.stml.h128.nn30-20/f0.test.e0.pred.eval.mean.csv\n",
            "/content/opentf/src/mdl/ntf.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  Y_ = Ntf.torch.load(predfile, map_location='cpu')['y_pred']\n",
            "[2025-11-06 21:36:36,800][mdl.ntf][INFO] - Evaluating predictions at ../output/dblp/toy.dblp.v12.json/splits.f3.r0.85/gs.b1000.e100.ns5.lr0.001.es5.spe10.d64.add.stml.h128.nn30-20/f1.test.pred ... for {'trec': ['P_2,5,10', 'recall_2,5,10', 'ndcg_cut_2,5,10', 'map_cut_2,5,10', 'success_2,5,10'], 'other': ['skill_coverage_2,5,10', 'aucroc']}\n",
            "[2025-11-06 21:36:36,801][mdl.ntf][INFO] - ['P_2,5,10', 'recall_2,5,10', 'ndcg_cut_2,5,10', 'map_cut_2,5,10', 'success_2,5,10'] ...\n",
            "[2025-11-06 21:36:36,806][evl.metric][INFO] - Building pytrec_eval input for 5 instances ...\n",
            "100% 5/5 [00:00<00:00, 3029.25it/s]\n",
            "[2025-11-06 21:36:36,810][mdl.ntf][INFO] - ['aucroc'] ...\n",
            "[2025-11-06 21:36:36,814][mdl.ntf][INFO] - ['skill_coverage_2,5,10'] ...\n",
            "100% 5/5 [00:00<00:00, 248.77it/s]\n",
            "[2025-11-06 21:36:36,843][mdl.ntf][INFO] - Saving file per fold as ../output/dblp/toy.dblp.v12.json/splits.f3.r0.85/gs.b1000.e100.ns5.lr0.001.es5.spe10.d64.add.stml.h128.nn30-20/f1.test.pred.eval.mean.csv\n",
            "/content/opentf/src/mdl/ntf.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  Y_ = Ntf.torch.load(predfile, map_location='cpu')['y_pred']\n",
            "[2025-11-06 21:36:36,846][mdl.ntf][INFO] - Evaluating predictions at ../output/dblp/toy.dblp.v12.json/splits.f3.r0.85/gs.b1000.e100.ns5.lr0.001.es5.spe10.d64.add.stml.h128.nn30-20/f1.test.e0.pred ... for {'trec': ['P_2,5,10', 'recall_2,5,10', 'ndcg_cut_2,5,10', 'map_cut_2,5,10', 'success_2,5,10'], 'other': ['skill_coverage_2,5,10', 'aucroc']}\n",
            "[2025-11-06 21:36:36,847][mdl.ntf][INFO] - ['P_2,5,10', 'recall_2,5,10', 'ndcg_cut_2,5,10', 'map_cut_2,5,10', 'success_2,5,10'] ...\n",
            "[2025-11-06 21:36:36,852][evl.metric][INFO] - Building pytrec_eval input for 5 instances ...\n",
            "100% 5/5 [00:00<00:00, 3172.70it/s]\n",
            "[2025-11-06 21:36:36,857][mdl.ntf][INFO] - ['aucroc'] ...\n",
            "[2025-11-06 21:36:36,860][mdl.ntf][INFO] - ['skill_coverage_2,5,10'] ...\n",
            "100% 5/5 [00:00<00:00, 244.26it/s]\n",
            "[2025-11-06 21:36:36,889][mdl.ntf][INFO] - Saving file per fold as ../output/dblp/toy.dblp.v12.json/splits.f3.r0.85/gs.b1000.e100.ns5.lr0.001.es5.spe10.d64.add.stml.h128.nn30-20/f1.test.e0.pred.eval.mean.csv\n",
            "/content/opentf/src/mdl/ntf.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  Y_ = Ntf.torch.load(predfile, map_location='cpu')['y_pred']\n",
            "[2025-11-06 21:36:36,892][mdl.ntf][INFO] - Evaluating predictions at ../output/dblp/toy.dblp.v12.json/splits.f3.r0.85/gs.b1000.e100.ns5.lr0.001.es5.spe10.d64.add.stml.h128.nn30-20/f2.test.pred ... for {'trec': ['P_2,5,10', 'recall_2,5,10', 'ndcg_cut_2,5,10', 'map_cut_2,5,10', 'success_2,5,10'], 'other': ['skill_coverage_2,5,10', 'aucroc']}\n",
            "[2025-11-06 21:36:36,894][mdl.ntf][INFO] - ['P_2,5,10', 'recall_2,5,10', 'ndcg_cut_2,5,10', 'map_cut_2,5,10', 'success_2,5,10'] ...\n",
            "[2025-11-06 21:36:36,899][evl.metric][INFO] - Building pytrec_eval input for 5 instances ...\n",
            "100% 5/5 [00:00<00:00, 3075.90it/s]\n",
            "[2025-11-06 21:36:36,903][mdl.ntf][INFO] - ['aucroc'] ...\n",
            "[2025-11-06 21:36:36,908][mdl.ntf][INFO] - ['skill_coverage_2,5,10'] ...\n",
            "100% 5/5 [00:00<00:00, 216.92it/s]\n",
            "[2025-11-06 21:36:36,941][mdl.ntf][INFO] - Saving file per fold as ../output/dblp/toy.dblp.v12.json/splits.f3.r0.85/gs.b1000.e100.ns5.lr0.001.es5.spe10.d64.add.stml.h128.nn30-20/f2.test.pred.eval.mean.csv\n",
            "/content/opentf/src/mdl/ntf.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  Y_ = Ntf.torch.load(predfile, map_location='cpu')['y_pred']\n",
            "[2025-11-06 21:36:36,944][mdl.ntf][INFO] - Evaluating predictions at ../output/dblp/toy.dblp.v12.json/splits.f3.r0.85/gs.b1000.e100.ns5.lr0.001.es5.spe10.d64.add.stml.h128.nn30-20/f2.test.e0.pred ... for {'trec': ['P_2,5,10', 'recall_2,5,10', 'ndcg_cut_2,5,10', 'map_cut_2,5,10', 'success_2,5,10'], 'other': ['skill_coverage_2,5,10', 'aucroc']}\n",
            "[2025-11-06 21:36:36,945][mdl.ntf][INFO] - ['P_2,5,10', 'recall_2,5,10', 'ndcg_cut_2,5,10', 'map_cut_2,5,10', 'success_2,5,10'] ...\n",
            "[2025-11-06 21:36:36,951][evl.metric][INFO] - Building pytrec_eval input for 5 instances ...\n",
            "100% 5/5 [00:00<00:00, 3039.79it/s]\n",
            "[2025-11-06 21:36:36,955][mdl.ntf][INFO] - ['aucroc'] ...\n",
            "[2025-11-06 21:36:36,959][mdl.ntf][INFO] - ['skill_coverage_2,5,10'] ...\n",
            "100% 5/5 [00:00<00:00, 249.26it/s]\n",
            "[2025-11-06 21:36:36,987][mdl.ntf][INFO] - Saving file per fold as ../output/dblp/toy.dblp.v12.json/splits.f3.r0.85/gs.b1000.e100.ns5.lr0.001.es5.spe10.d64.add.stml.h128.nn30-20/f2.test.e0.pred.eval.mean.csv\n",
            "[2025-11-06 21:36:36,992][mdl.ntf][INFO] - Saving mean evaluation file over 3 folds as ../output/dblp/toy.dblp.v12.json/splits.f3.r0.85/gs.b1000.e100.ns5.lr0.001.es5.spe10.d64.add.stml.h128.nn30-20/test.pred.eval.mean.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls ../output/dblp/toy.dblp.v12.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "feJf72sDElJ4",
        "outputId": "672b4795-f702-4bfc-f7f8-51c6355acde4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "indexes.pkl\t\t  splits.f3.r0.85.pkl  stml.mean.graph.pkl\n",
            "prep.train.test.eval.log  stm.add.graph.pkl    stm.mean.graph.pkl\n",
            "skillcoverage.pkl\t  stm.dup.graph.pkl    teams.pkl\n",
            "skill.docs.pkl\t\t  stml.add.graph.pkl   teamsvecs.pkl\n",
            "splits.f3.r0.85\t\t  stml.dup.graph.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls ../output/dblp/toy.dblp.v12.json/splits.f3.r0.85/gs.b1000.e100.ns5.lr0.001.es5.spe10.d64.add.stml.h128.nn30-20"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mu577_wqFMWq",
        "outputId": "28866498-1c53-456c-92da-ab04f812b3e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "f0.e0.pt\t\t\t       f1.test.pred.eval.mean.csv\n",
            "f0.pt\t\t\t\t       f1.test.pred.eval.per_instance.csv\n",
            "f0.test.e0.pred\t\t\t       f2.e0.pt\n",
            "f0.test.e0.pred.eval.mean.csv\t       f2.pt\n",
            "f0.test.e0.pred.eval.per_instance.csv  f2.test.e0.pred\n",
            "f0.test.pred\t\t\t       f2.test.e0.pred.eval.mean.csv\n",
            "f0.test.pred.eval.mean.csv\t       f2.test.e0.pred.eval.per_instance.csv\n",
            "f0.test.pred.eval.per_instance.csv     f2.test.pred\n",
            "f1.e0.pt\t\t\t       f2.test.pred.eval.mean.csv\n",
            "f1.pt\t\t\t\t       f2.test.pred.eval.per_instance.csv\n",
            "f1.test.e0.pred\t\t\t       logs4tboard\n",
            "f1.test.e0.pred.eval.mean.csv\t       ntf.\n",
            "f1.test.e0.pred.eval.per_instance.csv  test.pred.eval.mean.csv\n",
            "f1.test.pred\t\t\t       test.pred.eval.per_instance_mean.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "pd.read_csv('/content/opentf/output/dblp/toy.dblp.v12.json/splits.f3.r0.85/gs.b1000.e100.ns5.lr0.001.es5.spe10.d64.add.stml.h128.nn30-20/test.pred.eval.mean.csv', index_col = 0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "cnLNdZBjFiZv",
        "outputId": "6eba8749-e60d-4bc4-f9d9-9a9b09ececc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                       mean       std\n",
              "P_2                0.166667  0.057735\n",
              "P_5                0.213333  0.083267\n",
              "P_10               0.186667  0.011547\n",
              "recall_2           0.155556  0.050918\n",
              "recall_5           0.488889  0.203670\n",
              "recall_10          0.844444  0.050918\n",
              "ndcg_cut_2         0.159124  0.070800\n",
              "ndcg_cut_5         0.346042  0.085861\n",
              "ndcg_cut_10        0.490005  0.026670\n",
              "map_cut_2          0.111111  0.053576\n",
              "map_cut_5          0.234630  0.056809\n",
              "map_cut_10         0.315079  0.032069\n",
              "success_2          0.333333  0.115470\n",
              "success_5          0.800000  0.200000\n",
              "success_10         1.000000  0.000000\n",
              "aucroc             0.569585  0.070874\n",
              "skill_coverage_2   0.900000  0.000000\n",
              "skill_coverage_5   1.000000  0.000000\n",
              "skill_coverage_10  1.000000  0.000000"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-af59401a-a688-4de6-9c8d-131d42288ac3\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mean</th>\n",
              "      <th>std</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>P_2</th>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.057735</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>P_5</th>\n",
              "      <td>0.213333</td>\n",
              "      <td>0.083267</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>P_10</th>\n",
              "      <td>0.186667</td>\n",
              "      <td>0.011547</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>recall_2</th>\n",
              "      <td>0.155556</td>\n",
              "      <td>0.050918</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>recall_5</th>\n",
              "      <td>0.488889</td>\n",
              "      <td>0.203670</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>recall_10</th>\n",
              "      <td>0.844444</td>\n",
              "      <td>0.050918</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ndcg_cut_2</th>\n",
              "      <td>0.159124</td>\n",
              "      <td>0.070800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ndcg_cut_5</th>\n",
              "      <td>0.346042</td>\n",
              "      <td>0.085861</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ndcg_cut_10</th>\n",
              "      <td>0.490005</td>\n",
              "      <td>0.026670</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>map_cut_2</th>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.053576</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>map_cut_5</th>\n",
              "      <td>0.234630</td>\n",
              "      <td>0.056809</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>map_cut_10</th>\n",
              "      <td>0.315079</td>\n",
              "      <td>0.032069</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>success_2</th>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.115470</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>success_5</th>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.200000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>success_10</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>aucroc</th>\n",
              "      <td>0.569585</td>\n",
              "      <td>0.070874</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>skill_coverage_2</th>\n",
              "      <td>0.900000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>skill_coverage_5</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>skill_coverage_10</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-af59401a-a688-4de6-9c8d-131d42288ac3')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-af59401a-a688-4de6-9c8d-131d42288ac3 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-af59401a-a688-4de6-9c8d-131d42288ac3');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-229a8ca9-ddf0-467e-871c-dadc07d0c5c1\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-229a8ca9-ddf0-467e-871c-dadc07d0c5c1')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-229a8ca9-ddf0-467e-871c-dadc07d0c5c1 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"pd\",\n  \"rows\": 19,\n  \"fields\": [\n    {\n      \"column\": \"mean\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.3296069594775831,\n        \"min\": 0.1111111111111111,\n        \"max\": 1.0,\n        \"num_unique_values\": 17,\n        \"samples\": [\n          0.1666666666666666,\n          0.2133333333333333,\n          0.8444444444444444\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"std\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.059430333094005,\n        \"min\": 0.0,\n        \"max\": 0.2036700308869262,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          0.0568090078639435,\n          0.1154700538379251,\n          0.0577350269189625\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "tmp_opentf",
      "language": "python",
      "name": "tmp_opentf"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}